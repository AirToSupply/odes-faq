## 安装部署


1.解压安装包

```
sudo tar -zxvf ./datax.tar.gz -C 指定目录
```

检查安装组件权限，非zhongtai用户，执行

```
sudo chown -R zhongtai:zhongtai datax/
```

2.配置环境变量

vim ~/.bashrc

```
export DATAX_HOME=/workspace/datax
export PATH=$PATH:$DATAX_HOME/bin
```

生效配置：

```
source ~/.bashrc
```



3、更改RDBMS(支持所有关系型数据库) 读写配置

进入以下目录： `datax/plugin/reader/rdbmsreader`  路径和 `datax/plugin/writer/rdbmswriter`路径

在plugin.json配置文件内更改为：

```shell
{
    "name": "rdbmsreader",
    "class": "com.alibaba.datax.plugin.reader.rdbmsreader.RdbmsReader",
    "description": "useScene: prod. mechanism: Jdbc connection using the database, execute select sql, retrieve data from the ResultSet. warn: The more you know about the database, the less problems you encounter.",
    "developer": "alibaba",
    "drivers":["dm.jdbc.driver.DmDriver", "com.sybase.jdbc3.jdbc.SybDriver", "com.edb.Driver", "com.kingbase8.Driver", "com.pivotal.jdbc.GreenplumDriver"]
}

```

在 `libs`路径下添加以下四个jar包：

```shell
Dm8JdbcDriver18-8.1.1.49.jar
gbase-connector-java-8.3.81.53-build-54.5.1-bin.jar
greenplum-jdbc-6.0.0.000181.jar
kingbase8-8.6.0.jar
```



4、分发到另一台

```
sudo scp -r datax/ root@172.16.10.207:/workspace
```

修改权限

```
sudo chown -R zhongtai:zhongtai datax/
```

配置同2步骤



## 验证与实战

### （1）运行自检脚本

```shell
$DATAX_HOME/bin/datax.py $DATAX_HOME/job/job.json
```

有如下提示代表成功：

```shell
2022-05-11 10:29:46.170 [job-0] INFO  JobContainer - PerfTrace not enable!
2022-05-11 10:29:46.171 [job-0] INFO  StandAloneJobContainerCommunicator - Total 100000 records, 2600000 bytes | Speed 253.91KB/s, 10000 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.024s |  All Task WaitReaderTime 0.036s | Percentage 100.00%
2022-05-11 10:29:46.173 [job-0] INFO  JobContainer -
任务启动时刻                    : 2022-05-11 10:29:36
任务结束时刻                    : 2022-05-11 10:29:46
任务总计耗时                    :                 10s
任务平均流量                    :          253.91KB/s
记录写入速度                    :          10000rec/s
读出记录总数                    :              100000
读写失败总数                    :                   0
```

### （2）抽取mysql数据库至postgres

postgres建表语句如下：

```sql
CREATE TABLE "public"."xmw_organization" (
  "org_id" varchar(20) NOT NULL,
  "org_name" varchar(32),
  "org_code" varchar(32),
  "org_type" varchar(10),
  "leader" varchar(20),
  "remark" varchar(200),
  "create_time" timestamp,
  "update_last_time" timestamp,
  "founder" varchar(20),
  "parent_id" varchar(20),
  "status" varchar(10),
  "sort" int8 NOT NULL,
  PRIMARY KEY ("org_id"));
```

同步配置如下：

```json
{
  "job": {
    "content": [{
      "reader": {
        "name": "mysqlreader",
        "parameter": {
          "password": "123456",
          "username": "root",
          "connection": [{
            "jdbcUrl": ["jdbc:mysql://172.16.2.120:3306/caif_xmw"],
            "querySql": ["SELECT * FROM xmw_organization where org_type='1'"]
          }]
        }
      },
      "writer": {
        "name": "postgresqlwriter",
        "parameter": {
          "preSql": ["delete from xmw_organization"],
          "column": ["*"],
          "password": "postgres",
          "username": "postgres",
          "connection": [{
            "table": ["xmw_organization"],
            "jdbcUrl": "jdbc:postgresql://172.16.2.120:5432/test"
          }]
        }
      }
    }],
    "setting": {
      "speed": {
        "channel": 1
      },
      "errorLimit": {
        "record": 0,
        "percentage": 0.02
      }
    }
  }
}
```

执行命令如下：

```shell
$DATAX_HOME/bin/datax.py $DATAX_HOME/tmp/mysql2postgres.json
```

### （3）抽取mysql导入hdfs orc文件

同步配置如下：

```json
{
        "job": {
                "setting": {
                        "speed": {
                                "channel": 1
                        },
                        "errorLimit": {
                                "record": 0,
                                "percentage": 0.02
                        }
                },
                "content": [
                        {
                                "reader": {
                                        "name": "mysqlreader",
                                        "parameter": {
                                                "username": "usr_etl_nanjing",
                                                "password": "EfMYPAefxganOHL2",
                                                "column": [
                                                        "id",
                                                        "cardId",
                                                        "signCardId",
                                                        "signStatus",
                                                        "clientName",
                                                        "clientNo",
                                                        "certificateNumber",
                                                        "vehicleLicense",
                                                        "cardStatus",
                                                        "cardBalance",
                                                        "cardAccountBalance",
                                                        "cardPrice",
                                                        "obuId",
                                                        "bindStatus",
                                                        "operateType",
                                                        "tradeTime",
                                                        "billPoSt",
                                                        "liquidateTime",
                                                        "liquidateNumber",
                                                        "cardLiquidateTime",
                                                        "liquidateStatus",
                                                        "stationId",
                                                        "payType",
                                                        "favorRate",
                                                        "operatorId",
                                                        "verifyCode",
                                                        "transferMark",
                                                        "createBy",
                                                        "createTime",
                                                        "updateBy",
                                                        "updateTime",
                                                        "remove",
                                                        "spare",
                                                        "spare1",
                                                        "spare2",
                                                        "clientType",
                                                        "cardType",
                                                        "brand",
                                                        "model"
                                                ],
                                                "connection": [
                                                        {
                                                                "jdbcUrl": [
                                                                        "jdbc:mysql://10.232.2.156:3306/etc_project?useSSL=false"
                                                                ],
                                                                "table": [
                                                                        "etccardinfo"
                                                                ]
                                                        }
                                                ]
                                        }
                                },
                                "writer": {
                                        "name": "hdfswriter",
                                        "parameter": {
                                                "path": "/hudi/rdbms/orc/etc_project/etccardinfo",
                                                "defaultFS": "hdfs://ns",
                                                "fileType": "orc",
                                                "fileName": "data",
                                                "hadoopConfig": {
                                                        "dfs.ha.namenodes.ns": "nn1,nn2",
                                                        "fs.defaultFS": "hdfs://ns",
                                                        "dfs.namenode.rpc-address.ns.nn2": "10.232.3.218:9000",
                                                        "dfs.client.failover.proxy.provider.ns": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
                                                        "dfs.namenode.rpc-address.ns.nn1": "10.232.3.217:9000",
                                                        "dfs.nameservices": "ns",
                                                        "fs.hdfs.impl.disable.cache": "true",
                                                        "fs.hdfs.impl": "org.apache.hadoop.hdfs.DistributedFileSystem"
                                                },
                                                "column": [
                                                        {
                                                                "name": "id",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "cardId",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "signCardId",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "signStatus",
                                                                "type": "char"
                                                        },
                                                        {
                                                                "name": "clientName",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "clientNo",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "certificateNumber",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "vehicleLicense",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "cardStatus",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "cardBalance",
                                                                "type": "double"
                                                        },
                                                        {
                                                                "name": "cardAccountBalance",
                                                                "type": "double"
                                                        },
                                                        {
                                                                "name": "cardPrice",
                                                                "type": "double"
                                                        },
                                                        {
                                                                "name": "obuId",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "bindStatus",
                                                                "type": "char"
                                                        },
                                                        {
                                                                "name": "operateType",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "tradeTime",
                                                                "type": "timestamp"
                                                        },
                                                        {
                                                                "name": "billPoSt",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "liquidateTime",
                                                                "type": "timestamp"
                                                        },
                                                        {
                                                                "name": "liquidateNumber",
                                                                "type": "double"
                                                        },
                                                        {
                                                                "name": "cardLiquidateTime",
                                                                "type": "timestamp"
                                                        },
                                                        {
                                                                "name": "liquidateStatus",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "stationId",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "payType",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "favorRate",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "operatorId",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "verifyCode",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "transferMark",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "createBy",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "createTime",
                                                                "type": "timestamp"
                                                        },
                                                        {
                                                                "name": "updateBy",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "updateTime",
                                                                "type": "timestamp"
                                                        },
                                                        {
                                                                "name": "remove",
                                                                "type": "char"
                                                        },
                                                        {
                                                                "name": "spare",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "spare1",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "spare2",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "clientType",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "cardType",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "brand",
                                                                "type": "varchar"
                                                        },
                                                        {
                                                                "name": "model",
                                                                "type": "varchar"
                                                        }
                                                ],
                                                "writeMode": "append",
                                                "fieldDelimiter": "\t",
                                                "compress": "NONE"
                                        }
                                }
                        }
                ]
        }
}
```

### （4）抽取达梦导入greenplum

```json
{
    "job": {
        "setting": {
            "speed": {
                "channel": 1,
                "byte": 104857600
            }
        },
        "content": [
            {
                "reader": {
                    "name": "rdbmsreader",
                    "parameter": {
                        "username": "SYSDBA",
                        "password": "SYSDBA",
                        "column": [
                            "*"
                        ],
                        "connection": [
                            {
                                "querySql": [
                                    "SELECT * from DMHR.CITY"
                                ],
                                "jdbcUrl": [
                                    "jdbc:dm://172.16.10.171:5236/DMHR"
                                ]
                            }
                        ],
                        "fetchSize": 1024
                    }
                },
                "writer": {
                    "name": "rdbmswriter",
                    "parameter": {
                        "username": "gpadmin",
                        "password": "gpadmin",
                        "column": [
                            "*"
                        ],
                        "preSql": [
                            "delete from ods.city"
                        ],
                        "connection": [
                            {
                                "jdbcUrl": "jdbc:pivotal:greenplum://172.16.2.123:5432;DatabaseName=test",
                                "table": [
                                    "ods.city"
                                ]
                            }
                        ]
                    }
                }
            }
        ]
    }
}
```

rdbmswriter如何增加新的数据库支持:

进入rdbmsreader对应目录，这里${DATAX_HOME}为DataX主目录，即: ${DATAX_HOME}/plugin/reader/rdbmsreader
在rdbmsreader插件目录下有plugin.json配置文件，在此文件中注册您具体的数据库驱动，具体放在drivers数组中。rdbmsreader插件在任务执行时会动态选择合适的数据库驱动连接数据库。
```json
{
    "name": "rdbmsreader",
    "class": "com.alibaba.datax.plugin.reader.rdbmsreader.RdbmsReader",
    "description": "useScene: prod. mechanism: Jdbc connection using the database, execute select sql, retrieve data from the ResultSet. warn: The more you know about the database, the less problems you encounter.",
    "developer": "alibaba",
    "drivers": [
        "dm.jdbc.driver.DmDriver",
        "com.ibm.db2.jcc.DB2Driver",
        "com.sybase.jdbc3.jdbc.SybDriver",
        "com.edb.Driver"
    ]
}
```
在rdbmsreader插件目录下有libs子目录，您需要将您具体的数据库驱动放到libs目录下。
```shell
$tree
.
|-- libs
|   |-- Dm7JdbcDriver16.jar
|   |-- commons-collections-3.0.jar
|   |-- commons-io-2.4.jar
|   |-- commons-lang3-3.3.2.jar
|   |-- commons-math3-3.1.1.jar
|   |-- datax-common-0.0.1-SNAPSHOT.jar
|   |-- datax-service-face-1.0.23-20160120.024328-1.jar
|   |-- db2jcc4.jar
|   |-- druid-1.0.15.jar
|   |-- edb-jdbc16.jar
|   |-- fastjson-1.1.46.sec01.jar
|   |-- guava-r05.jar
|   |-- hamcrest-core-1.3.jar
|   |-- jconn3-1.0.0-SNAPSHOT.jar
|   |-- logback-classic-1.0.13.jar
|   |-- logback-core-1.0.13.jar
|   |-- plugin-rdbms-util-0.0.1-SNAPSHOT.jar
|   `-- slf4j-api-1.7.10.jar
|-- plugin.json
|-- plugin_job_template.json
`-- rdbmsreader-0.0.1-SNAPSHOT.jar
```

### （5）抽取金仓导入greenplum

```json
{
    "job": {
        "setting": {
            "speed": {
                "channel": 1,
                "byte": 104857600
            }
        },
        "content": [
            {
                "reader": {
                    "name": "rdbmsreader",
                    "parameter": {
                        "username": "system",
                        "password": "system",
                        "column": [
                            "*"
                        ],
                        "connection": [
                            {
                                "querySql": [
                                    "SELECT * from city"
                                ],
                                "jdbcUrl": [
                                    "jdbc:kingbase8://172.16.10.12:54321/test"
                                ]
                            }
                        ],
                        "fetchSize": 1024
                    }
                },
                "writer": {
                    "name": "rdbmswriter",
                    "parameter": {
                        "username": "gpadmin",
                        "password": "gpadmin",
                        "column": [
                            "*"
                        ],
                        "preSql": [
                            "delete from ods.city"
                        ],
                        "connection": [
                            {
                                "jdbcUrl": "jdbc:pivotal:greenplum://172.16.2.123:5432;DatabaseName=test",
                                "table": [
                                    "ods.city"
                                ]
                            }
                        ]
                    }
                }
            }
        ]
    }
}
```


### （6）HDFSWrite支持HDFS HA (高可用)配置


#### 官方文档地址

- [DataX HdfsReader 插件文档](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Falibaba%2FDataX%2Fblob%2Fmaster%2Fhdfsreader%2Fdoc%2Fhdfsreader.md%23datax-hdfsreader-%E6%8F%92%E4%BB%B6%E6%96%87%E6%A1%A3)
- [DataX HdfsWriter 插件文档](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Falibaba%2FDataX%2Fblob%2Fmaster%2Fhdfswriter%2Fdoc%2Fhdfswriter.md%23datax-hdfswriter-%E6%8F%92%E4%BB%B6%E6%96%87%E6%A1%A3)

#### Reader插件文档明确说明：

我们暂时不能做到：

1. 单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。
2. 目前还不支持hdfs HA;

#### 解决方法：

把 hdfs-site.xml、core-site.xml、hive-site.xml三个文件放入hdfsreader-0.0.1-SNAPSHOT.jar 与 hdfswriter-0.0.1-SNAPSHOT.jar 文件内。

#### 具体操作：

1. 下载对应三个文件

2. 备份datax安装路径下的datax/plugin/reader/hdfsreader/hdfsreader-0.0.1-SNAPSHOT.jar  与

   datax/plugin/writer/hdfswriter/hdfswriter-0.0.1-SNAPSHOT.jar

3. 用压缩工具打开hdfsreader-0.0.1-SNAPSHOT.jar,进入压缩文件预览模式，（如360压缩，右键用360打开，非解压），将上面三个文件直接拖入jar包内即可。如果是拷贝hdfsreader-0.0.1-SNAPSHOT.jar到其他路径下操作的，将操作完的jar包替换掉原来datax对应hdfsreader路径下的hdfsreader-0.0.1-SNAPSHOT.jar

注意：hdfs-site.xml、core-site.xml、hive-site.xml文件内一些需要主机IP的配置可能配的是hostname，那么最好在有Hadoop  HA (高可用)模式的主机上使用。

从 postgres 到 hive 的自定义json：

```
{
    "setting": {},
    "job": {
        "setting": {
            "speed": {
                "channel": 2
            }
        },
        "content": [
            {
                "reader": {
                    "name": "postgresqlreader",
                    "parameter": {
                        "username": "postgres",
                        "password": "postgres",
                        "column": [
                            "id",
                            "name",
                            "tel",
                            "total",
                            "create_time",
                            "operate_time"
                        ],
                        "connection": [
                            {
                                "table": [
                                    "test_hmt"
                                ],
                                "jdbcUrl": [
                                    "jdbc:postgresql://172.16.2.120:5432/bakbak"
                                ]
                            }
                        ]
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://ns",
                        "fileType": "text",
                        "path": "/user/hive/warehouse/excel_test_1123",
                        "fileName": "excel_test_1123",
                        "column": [
                            {
                                "name": "id",
                                "type": "BIGINT"
                            },
                            {
                                "name": "name",
                                "type": "STRING"
                            },
                            {
                                "name": "tel",
                                "type": "STRING"
                            },
                            {
                                "name": "total",
                                "type": "INT"
                            },
                            {
                                "name": "create_time",
                                "type": "TIMESTAMP"
                            },
                            {
                                "name": "operate_time",
                                "type": "TIMESTAMP"
                            }
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": ","
                    }
                }
            }
        ]
    }
}
```

参考文档：[DataX Hdfs HA(高可用)配置支持](https://www.jianshu.com/p/e00bf5c89bfe)


### （7）HDFSWrite在写入时检测目录，若目录不存在则自动创建


#### 问题所在与原因分析：

将数据库中的数据同步到HIVE分区表时，写入目录为HIVE表分区为dt=XXXX,如果不提前创建该分区，会报目录不存在的错误， 这个错误是由于DataX不支持在HDF上创建目录导致的。

#### 解决方法：

二次开发DataX，在写入时检测目录，若目录不存在自动创建此分区目录。

#### 具体操作：

​		1.从GitHub下载datax源码[ 链接](https://github.com/alibaba/DataX)

​		 2.修改hdfswriter目录下的HdfsWriter.java源码，重写里面的prepare方法：

```
    @Override
    public void prepare() {
        //若路径已经存在，检查path是否是目录
        if (hdfsHelper.isPathexists(path)) {
            if (!hdfsHelper.isPathDir(path)) {
                throw DataXException.asDataXException(HdfsWriterErrorCode.ILLEGAL_VALUE,
                        String.format("您配置的path: [%s] 不是一个合法的目录, 请您注意文件重名, 不合法目录名等情况.",
                                path));
            }
            //根据writeMode对目录下文件进行处理
            Path[] existFilePaths = hdfsHelper.hdfsDirList(path, fileName);
            boolean isExistFile = false;
            if (existFilePaths.length > 0) {
                isExistFile = true;
            }
            if ("append".equalsIgnoreCase(writeMode)) {
                LOG.info(String.format("由于您配置了writeMode append, 写入前不做清理工作, [%s] 目录下写入相应文件名前缀  [%s] 的文件",
                        path, fileName));
            } else if ("nonconflict".equalsIgnoreCase(writeMode) && isExistFile) {
                LOG.info(String.format("由于您配置了writeMode nonConflict, 开始检查 [%s] 下面的内容", path));
                List<String> allFiles = new ArrayList<String>();
                for (Path eachFile : existFilePaths) {
                    allFiles.add(eachFile.toString());
                }
                LOG.error(String.format("冲突文件列表为: [%s]", StringUtils.join(allFiles, ",")));
                throw DataXException.asDataXException(HdfsWriterErrorCode.ILLEGAL_VALUE,
                        String.format("由于您配置了writeMode nonConflict,但您配置的path: [%s] 目录不为空, 下面存在其他文件或文件夹.", path));
            } else if ("truncate".equalsIgnoreCase(writeMode) && isExistFile) {
                LOG.info(String.format("由于您配置了writeMode truncate,  [%s] 下面的内容将被覆盖重写", path));
                hdfsHelper.deleteFiles(existFilePaths);
            }
        } else {
            LOG.info(String.format("您配置的路径: [%s] 不存在,自动为您创建此路径", path));
            hdfsHelper.createPath(path);
        }
    }
```

​		 3.修改hdfswriter目录下的HdfsHelper.java源码，在里面添加createPath方法：

```
    public boolean createPath(String filePath) {
        Path path = new Path(filePath);
        boolean exist = false;
        try {
            if (fileSystem.exists(path)) {
                String message = String.format("文件路径[%s]已存在，无需创建！",
                        "message:filePath =" + filePath);
                LOG.info(message);
                exist = true;
            } else {
                exist = fileSystem.mkdirs(path);
            }
        } catch (IOException e) {
            String message = String.format("创建文件路径[%s]时发生网络IO异常,请检查您的网络是否正常！",
                    "message:filePath =" + filePath);
            LOG.error(message);
            throw DataXException.asDataXException(HdfsWriterErrorCode.CONNECT_HDFS_IO_ERROR, e);
        }
        return exist;
    }
```

4.打包修改后的源码，并替换掉集群的datax安装目录datax/plugin/writer/hdfswriter/hdfswriter-0.0.1-SNAPSHOT.jar即可。（在DataX-master根目录下的pom.xml文件核心组件、公共组件、reader/writer插件都以module的方式组装到一起了，把不需要的注释掉可加快打包速度）

5.打包命令：mvn clean package -pl hdfswriter -am -Dmaven.test.skip=true

参考文档：[二次开发DataX以支持HIVE分区表](https://blog.csdn.net/MaxineSgr/article/details/127266958)


## FAQ

### （1）运行自检脚本报错

```shell
Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.


2022-05-11 09:58:19.572 [main] WARN  ConfigParser - 插件[streamreader,streamwriter]加载失败，1s后重试... Exception:Code:[Common-00], Describe:[您提供的配置文件存在错误信息，请检查您的作业配置 .] - 配置信息错误，您提供的配置文件[/workspace/datax/plugin/reader/._txtfilereader/plugin.json]不存在. 请检查您的配置文件.
2022-05-11 09:58:20.580 [main] ERROR Engine -

经DataX智能分析,该任务最可能的错误原因是:
com.alibaba.datax.common.exception.DataXException: Code:[Common-00], Describe:[您提供的配置文件存在错误信息，请检查您的作业配置 .] - 配置信息错误，您提供的配置文件[/workspace/datax/plugin/reader/._txtfilereader/plugin.json]不存在. 请检查您的配置文件.
        at com.alibaba.datax.common.exception.DataXException.asDataXException(DataXException.java:26)
        at com.alibaba.datax.common.util.Configuration.from(Configuration.java:95)
        at com.alibaba.datax.core.util.ConfigParser.parseOnePluginConfig(ConfigParser.java:153)
        at com.alibaba.datax.core.util.ConfigParser.parsePluginConfig(ConfigParser.java:125)
        at com.alibaba.datax.core.util.ConfigParser.parse(ConfigParser.java:63)
        at com.alibaba.datax.core.Engine.entry(Engine.java:137)
        at com.alibaba.datax.core.Engine.main(Engine.java:204)
```

解决方案：

```shell
rm /workspace/datax/plugin/reader/._*
rm /workspace/datax/plugin/writer/._*
```




【参考文章】：https://yebd1h.smartapps.cn/pages/blog/index?_swebFromHost=baiduboxapp&blogId=120952339&_swebfr=1
【项目地址】：https://github.com/alibaba/DataX
【下载地址】：http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz