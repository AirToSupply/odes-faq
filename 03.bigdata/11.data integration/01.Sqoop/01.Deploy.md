## 安装与部署

下载链接如下：

```
http://archive.apache.org/dist/sqoop/1.4.7/
```

解压：

```shell
tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /workspace/
```

修改相关配置：

```shell
cd /workspace/sqoop-1.4.7.bin__hadoop-2.6.0/conf/
cp sqoop-env-template.sh sqoop-env.sh
vim sqoop-env.sh
```

修改内容如下：

```shell
export HADOOP_COMMON_HOME=/workspace/hadoop-2.10.1
export HADOOP_MAPRED_HOME=/workspace/hadoop-2.10.1/share/hadoop/mapreduce
export HIVE_HOME=/workspace/hive-2.3.8-bin
```

添加环境变量：

```shell
vim ~/.bashrc
```

```shell
export SQOOP_HOME=/workspace/sqoop-1.4.7.bin__hadoop-2.6.0
export PATH=$SQOOP_HOME/bin:$PATH
```

```shell
source ~/.bashrc
```

## 测试与验证

### （1）测试MySQL连通性

添加驱动至lib目录：

```shell
cp mysql-connector-java-5.1.34.jar /workspace/sqoop-1.4.7.bin__hadoop-2.6.0/lib/
```

测试脚本如下：

```shell
sqoop list-databases --connect jdbc:mysql://172.16.2.120:3306/caif_xmw --username root --password 123456
```

若没有异常则代表可以连通MySQL。

### （2）抽取MySQL表至Hive

```shell
sqoop import \
-D mapred.job.name=SQOOP_MYSQL_TO_HIVE \
-D org.apache.sqoop.splitter.allow_text_splitter=true \
-m 2 \
--connect "jdbc:mysql://172.16.2.120:3306/caif_xmw?allowLoadLocalInfile=false&autoDeserialize=false&allowLocalInfile=false&allowUrlInLocalInfile=false" \
--username root \
--password "123456" \
--table xmw_permission \
--hive-import \
--hive-database ds \
--hive-table ods_caif_xmw_xmw_permission \
--create-hive-table \
--hive-overwrite \
--delete-target-dir \
--target-dir /tmp/ods/caif_xmw/xmw_permission
```

## FAQ

### （1）缺少hive依赖

```shell
22/05/11 00:15:49 INFO mapreduce.ImportJobBase: Transferred 7.9072 KB in 20.8163 seconds (388.9733 bytes/sec)
22/05/11 00:15:49 INFO mapreduce.ImportJobBase: Retrieved 8 records.
22/05/11 00:15:49 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table xmw_news
22/05/11 00:15:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `xmw_news` AS t LIMIT 1
22/05/11 00:15:49 WARN hive.TableDefWriter: Column create_time had to be cast to a less precise type in Hive
22/05/11 00:15:49 WARN hive.TableDefWriter: Column update_last_time had to be cast to a less precise type in Hive
22/05/11 00:15:49 INFO hive.HiveImport: Loading uploaded data into Hive
22/05/11 00:15:49 ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.
22/05/11 00:15:49 ERROR tool.ImportTool: Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf
        at org.apache.sqoop.hive.HiveConfig.getHiveConf(HiveConfig.java:50)
        at org.apache.sqoop.hive.HiveImport.getHiveArgs(HiveImport.java:392)
        at org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:379)
        at org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:337)
        at org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:241)
        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:537)
        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
        at org.apache.sqoop.Sqoop.main(Sqoop.java:252)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.sqoop.hive.HiveConfig.getHiveConf(HiveConfig.java:44)
        ... 12 more

```

解决方式：

```shell
cp /workspace/hive-2.3.8-bin/lib/hive-common-2.3.8.jar /workspace/sqoop-1.4.7.bin__hadoop-2.6.0/lib/
```

### （2）缺少mapreduce依赖

```shell
[INFO] 2022-05-11 01:09:58.586 [taskAppId=TASK-20220511-5474043487584_5-2209-2346] TaskLogLogger-class org.apache.dolphinscheduler.plugin.task.sqoop.SqoopTask:[63] -  -> 注: /tmp/sqoop-zhongtai/compile/5115e9f4ccac0210b348f9444289ab9b/xmw_permission.java使用或覆盖了已过时的 API。
  注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
  22/05/11 01:09:57 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-zhongtai/compile/5115e9f4ccac0210b348f9444289ab9b/xmw_permission.jar
  Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/InputFormat
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
```

解决方式：

```shell
cp /workspace/hadoop-2.10.1/share/hadoop/mapreduce/*.jar /workspace/sqoop-1.4.7.bin__hadoop-2.6.0/lib/
```

【参考文章】 https://www.cnblogs.com/zggpdd/articles/14704341.html