# 一.什么是Spark Thrift Server？

​		Spark Thrift Server可以理解为是Spark SQL服务化的一种方式，类似Hive Thrift Server2。使用场景如下：

​		（1）上层应用需要通过JDBC协议并且通过Spark SQL引擎进行交互，例如：自研的Spring Boot微服务，Apache Superset可视化组件等。

​		（2）基于该服务进行二次开发和集成。

​		（3）和类似Sentry等权限系统进行集成。

# 二.基本使用

​		在$SPARK_HOME/sbin/目录下的start-thriftserver.sh提供了Spark Thrift Server的入口，具体细节可以参考：[Distributed SQL Engine](https://spark.apache.org/docs/3.1.1/sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server)。

​		该服务本质就是启动一个常驻的Spark应用程序来接受Spark SQL指令。下面这个例子是一个最小化的例子：

```shell
$SPARK_HOME/sbin/start-thriftserver.sh \
--master yarn \
--deploy-mode client \
--executor-memory 1g \
--driver-memory 2g \
--num-executors 2 \
--executor-cores 2 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
--conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=CORRECTED \
--conf spark.sql.legacy.avro.datetimeRebaseModeInWrite=CORRECTED \
--conf spark.sql.hive.convertMetastoreParquet=false 
```

​		开启服务之后通过Spark提供的客户端工具进行查询：

```shell
> $SPARK_HOME/bin/beeline

Beeline version 2.3.7 by Apache Hive

beeline> !connect jdbc:hive2://2-123:10000

Connecting to jdbc:hive2://2-123:10000
Enter username for jdbc:hive2://2-123:10000: hive
Enter password for jdbc:hive2://2-123:10000: ****
22/11/04 11:36:33 INFO jdbc.Utils: Supplied authorities: 2-123:10000
22/11/04 11:36:33 INFO jdbc.Utils: Resolved authority: 2-123:10000
Connected to: Spark SQL (version 3.1.1)
Driver: Hive JDBC (version 2.3.7)
Transaction isolation: TRANSACTION_REPEATABLE_READ

0: jdbc:hive2://2-123:10000> select 1;
+----+
| 1  |
+----+
| 1  |
+----+
1 row selected (3.32 seconds)
```



# 三.服务管理

​		我们可以通过shell脚本来方便的管理Spark Thrift Server服务状态。脚本模板如下：

```shell
#!/bin/bash

# spark thrift server host. Default value: 0.0.0.0
SPARK_THRIFT_SERVER2_HOST=
# spark thrift server port. Default value: 10000
SPARK_THRIFT_SERVER2_PORT=10001
# spark thrift server2 log dir
SPARK_THRIFT_SERVER2_LOG_DIR=/data/spark-thriftserver-3.2.3
# hive warehouse dir
HIVE_WAREHOUSE_DIR=

# ####################################################################
# Spark Thrift Server Configuration
# ####################################################################
if [ "x$SPARK_HOME" = "x" ]; then
  echo "Please specify to env SPARK_HOME"
  exit
fi

if [ "x$SPARK_THRIFT_SERVER2_HOST" = "x" ]; then
  SPARK_THRIFT_SERVER2_HOST=0.0.0.0
fi

if [ "x$SPARK_THRIFT_SERVER2_PORT" = "x" ]; then
  SPARK_THRIFT_SERVER2_PORT=10000
fi

if [ "x$HIVE_WAREHOUSE_DIR" = "x" ]; then
  HIVE_WAREHOUSE_DIR=/user/hive/warehouse/
fi

if [ "x$SPARK_THRIFT_SERVER2_LOG_DIR" = "x" ]; then
  SPARK_THRIFT_SERVER2_LOG_DIR=/tmp/spark-thriftserver
fi

if [ ! -d $SPARK_THRIFT_SERVER2_LOG_DIR ]; then
  mkdir -p $SPARK_THRIFT_SERVER2_LOG_DIR
fi

mkdir -p $SPARK_THRIFT_SERVER2_LOG_DIR/{logs,pid}

# ####################################################################
# Generate Sspark Beeline Cli
# ####################################################################
cat > $SPARK_HOME/bin/beeline-cli << EOF
$SPARK_HOME/bin/beeline -u "jdbc:hive2://$(hostname):$SPARK_THRIFT_SERVER2_PORT/default" \
-n hive \
-p hive \
--color=true \
--verbose=true \
--showDbInPrompt=true \
--incremental=true
EOF

chmod u+x $SPARK_HOME/bin/beeline-cli

# ####################################################################
# Spark Thrift Server State Machine Function
# ####################################################################
function check_process()
{
  pid=$(ps -ef 2>/dev/null | grep -i "org.apache.spark.deploy.SparkSubmit" | grep -i "Thrift JDBC/ODBC Server" | grep -v grep | awk '{print $2}')
  echo $pid
  [ "$pid" ] && return 0 || return 1
}

function server_start()
{
  metapid=$(check_process)
  if [ -n "$metapid" ]; then
    echo "Spark Thrift Server2 [pid: $metapid] has already exists!"
    return
  fi

  nohup $SPARK_HOME/sbin/start-thriftserver.sh \
  --hiveconf hive.server2.thrift.bind.host=${SPARK_THRIFT_SERVER2_HOST} \
  --hiveconf hive.server2.thrift.port=${SPARK_THRIFT_SERVER2_PORT} \
  --hiveconf hive.server2.enable.doAs=true \
  --master yarn \
  --deploy-mode client \
  --executor-memory 1G \
  --driver-memory 2G \
  --num-executors 2 \
  --executor-cores 2 \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=CORRECTED \
  --conf spark.sql.legacy.avro.datetimeRebaseModeInWrite=CORRECTED \
  --conf spark.sql.hive.convertMetastoreParquet=false \
  --conf spark.sql.warehouse.dir=${HIVE_WAREHOUSE_DIR} \
  --conf spark.shuffle.service.enabled=true \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.initialExecutors=2 \
  --conf spark.dynamicAllocation.minExecutors=2 \
  --conf spark.dynamicAllocation.maxExecutors=12 \
  --conf spark.dynamicAllocation.executorIdleTimeout=300s \
  --conf spark.dynamicAllocation.schedulerBacklogTimeout=30s \
  --conf spark.dynamicAllocation.cachedExecutorIdleTimeout=1800s > $SPARK_THRIFT_SERVER2_LOG_DIR/logs/spark-thrift-server-$(date '+%Y-%m-%d_%H-%M-%S').log 2>&1 &

  sleep 10

  metapid=$(check_process)
  echo $metapid > $SPARK_THRIFT_SERVER2_LOG_DIR/pid/spark-thrift-server2.pid && echo "Spark Thrift Server2 Startup!"
}

function server_stop()
{
  metapid=$(check_process)
  [ "$metapid" ] && $SPARK_HOME/sbin/stop-thriftserver.sh || echo "Spark Thrift Server2 Shutdown!"
}

# ####################################################################
# Spark Thrift Server State Machine
# ####################################################################
case $1 in

"start")
  server_start
  ;;

"stop")
  server_stop
  ;;

"restart")
  server_stop
  sleep 20
  server_start
  ;;

"status")
  check_process >/dev/null && echo "Spark Thrift Server2 [Active]" || echo "Spark Thrift Server2 [Inactive]"
  ;;

*)
  echo Invalid Args!
  echo 'Usage: '$(basename $0)' start|stop|restart|status'
  ;;

esac
```

​		脚本参数如下：

| 参数名称                     | 参数说明         | 默认值                  |
| ---------------------------- | ---------------- | ----------------------- |
| SPARK_THRIFT_SERVER2_HOST    | 服务IP           | 0.0.0.0                 |
| SPARK_THRIFT_SERVER2_PORT    | 服务端口         | 10000                   |
| SPARK_THRIFT_SERVER2_LOG_DIR | 服务日志路径     | /tmp/spark-thriftserver |
| HIVE_WAREHOUSE_DIR           | Hive数据宿主路径 | /user/hive/warehouse/   |

​		使用方式：

```shell
spark-thriftserver-deamon.sh start|stop|restart|status
```



# FAQ

（1）启动Spark Thrift Server时和Hive Thrift Server2服务端口冲突。

​		异常堆栈如下：

```shell
22/11/04 11:19:10 ERROR service.CompositeService: Error starting services HiveServer2
org.apache.hive.service.ServiceException: Error initializing ThriftBinaryCLIService
        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.initializeServer(ThriftBinaryCLIService.java:113)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.start(ThriftCLIService.java:177)
        at org.apache.hive.service.CompositeService.start(CompositeService.java:70)
        at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:105)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.start(HiveThriftServer2.scala:154)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.startWithContext(HiveThriftServer2.scala:64)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:104)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1030)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1039)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address /172.16.2.123:10000.
        at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:109)
        at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:91)
        at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:87)
        at org.apache.hadoop.hive.common.auth.HiveAuthUtils.getServerSocket(HiveAuthUtils.java:87)
        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.initializeServer(ThriftBinaryCLIService.java:75)
        ... 19 more
Caused by: java.net.BindException: 地址已在使用 (Bind failed)
        at java.net.PlainSocketImpl.socketBind(Native Method)
        at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)
        at java.net.ServerSocket.bind(ServerSocket.java:375)
        at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:106)
        ... 23 more
22/11/04 11:19:10 INFO service.AbstractService: Service:ThriftBinaryCLIService is stopped.
22/11/04 11:19:10 INFO service.AbstractService: Service:OperationManager is stopped.
22/11/04 11:19:10 INFO service.AbstractService: Service:SessionManager is stopped.
22/11/04 11:19:10 INFO service.AbstractService: Service:CLIService is stopped.
22/11/04 11:19:10 ERROR thriftserver.HiveThriftServer2: Error starting HiveThriftServer2
org.apache.hive.service.ServiceException: Failed to Start HiveServer2
        at org.apache.hive.service.CompositeService.start(CompositeService.java:80)
        at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:105)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.start(HiveThriftServer2.scala:154)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.startWithContext(HiveThriftServer2.scala:64)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:104)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1030)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1039)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hive.service.ServiceException: Error initializing ThriftBinaryCLIService
        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.initializeServer(ThriftBinaryCLIService.java:113)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.start(ThriftCLIService.java:177)
        at org.apache.hive.service.CompositeService.start(CompositeService.java:70)
        ... 17 more
Caused by: org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address /172.16.2.123:10000.
        at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:109)
        at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:91)
        at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:87)
        at org.apache.hadoop.hive.common.auth.HiveAuthUtils.getServerSocket(HiveAuthUtils.java:87)
        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.initializeServer(ThriftBinaryCLIService.java:75)
        ... 19 more
Caused by: java.net.BindException: 地址已在使用 (Bind failed)
        at java.net.PlainSocketImpl.socketBind(Native Method)
        at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)
        at java.net.ServerSocket.bind(ServerSocket.java:375)
        at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:106)
        ... 23 more
```

​		这种情况大多是因为Spark Thrift Server服务和Hive Thrift Server服务部署在同一台节点上，并且绑定的端口号冲突，有如下两种方式：

​		**方式一**：在$SPARK_HOME/conf/hive-site.xml文件中配置如下参数：

```xml
<property>
    <name>hive.server2.thrift.port</name>
    <value>10001</value>
</property>
<property>
    <name>hive.server2.thrift.bind.host</name>
    <value>???</value>
</property>
<!-- 最小工作线程数，默认为500-->
<property>
    <name>hive.server2.thrift.min.worker.threads</name>
    <value>???</value>
</property>
<!-- 最大工作线程数，默认为10000-->
<property>
    <name>hive.server2.thrift.max.worker.threads</name>
    <value>???</value>
</property>
```

​		**方式二**：在启动Spark Thrift Server任务通过如下参数指定：

```shell
./sbin/start-thriftserver.sh \
--hiveconf hive.server2.thrift.port=<listening-port> \
--hiveconf hive.server2.thrift.bind.host=<listening-host> \
--master <master-uri>
...
```

​		注意，上述两种方式不可以同时配置，重启服务即可。



（2）通过该服务读写数据时，引发Filesystem句柄关闭异常。

​		异常堆栈如下：

```shell
0: jdbc:hive2://2-123:10001> insert into table csv_table select * from csv_view;

Error: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Filesystem closed0 
		at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
        at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Filesystem closed
        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:112)
        at org.apache.spark.sql.hive.HiveExternalCatalog.loadTable(HiveExternalCatalog.scala:877)
        at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.loadTable(ExternalCatalogWithListener.scala:167)
        at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:343)
        at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:104)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
        at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
        at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
        at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
        at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
        at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
        ... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Filesystem closed
        at org.apache.hadoop.hive.ql.metadata.Hive.needToCopy(Hive.java:3245)
        at org.apache.hadoop.hive.ql.metadata.Hive.copyFiles(Hive.java:2914)
        at org.apache.hadoop.hive.ql.metadata.Hive.copyFiles(Hive.java:3297)
        at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:2022)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at 
```

​		需要在$SPARK_HOME/conf/core-site.xml文件中配置如下参数：

```xml
<property>
    <name>fs.hdfs.impl.disable.cache</name>
    <value>true</value>
</property>
```

​		重启服务即可。



（3）该服务如何集成数据湖？

```shell
$SPARK_HOME/sbin/start-thriftserver.sh \
--master yarn \
--deploy-mode client \
--executor-memory 1g \
--driver-memory 2g \
--num-executors 2 \
--executor-cores 2 \
--jars hdfs://ns/test_jars/hudi-spark3.1-bundle_2.12-0.11.0.jar \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
--conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension \
--conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=CORRECTED \
--conf spark.sql.legacy.avro.datetimeRebaseModeInWrite=CORRECTED \
--conf spark.sql.hive.convertMetastoreParquet=false
```



（4）通过该服务创建Hive表，查看该表的location属性指向的是本地路径，并没有指向Hive的自有数据路径。

​		在启动该服务时需要配置如下参数，指向Hive实际指向到HDFS的路径：

```shell
--conf spark.sql.warehouse.dir=/user/hive/warehouse/
```



# 参考资料

[Hive的hiveserver2和beeline的使用以及spark thritfserver的启动](https://blog.csdn.net/a18792721831/article/details/123123282?ops_request_misc=&request_id=&biz_id=102&utm_term=spark%20thriftserver%E4%B8%8Ehiveservice&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~sobaiduweb~default-1-123123282.nonecase&spm=1018.2226.3001.4450)
