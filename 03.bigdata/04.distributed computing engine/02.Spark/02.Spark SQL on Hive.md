【**步骤一**】在$SPARK_HOME/conf目录下创建hive-site.xml配置文件。

​		这里采用最小化配置，必须配置的参数如下：

| hive-site.xml参数            | 参数说明                                                     |
| ---------------------------- | ------------------------------------------------------------ |
| hive.metastore.uris          | hive metastore服务thrift连接地址                             |
| hive.metastore.warehouse.dir | hive在分布式文件系统的工作目录，如果不配置后续通过spark sql创建的hive表路劲默认指向本地的spark_warehouse |

​	【注意】你也可以将$HIVE_HOME/conf/hive-site.xml拷贝（或者创建软连接）到$SPARK_HOME/conf目录下的hive-site.xml文件，但不建议这么做，因为其参数可能会对后续Spark行为产生影响。



【**步骤二**】将hive metastore服务对应的元数据库连接驱动包拷至$SPARK_HOME/jars目录下。



【**步骤三**】启动spark-sql交互终端进行相关验证，下面提供简单的测试用例：

```sql
> $SPARK_HOME/bin/spark-sql

drop database if exists test;
create database if not exists test;
drop table if exists test.tb;
create table if not exists test.tb(col string) stored as parquet;
insert into tb values("1"),("2");

-- drop table if exists test.tb;
-- drop database if exists test;
```

​		观察Hive和Spark两侧表数据是否完备，其路径指向是否正常。