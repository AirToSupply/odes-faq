# FLINK 环境配置

### 1、解压 Flink 安装包

```shell
tar -xvf flink-1.15.3-bin-scala_2.12.tgz -C /workspace/opt
```



### 2、配置 Flink 环境变量

```shell
vim ~/.bashrc
```

​     配置内容如下：

```shell
# FLINK
export FLINK_HOME=/workspace/opt/flink-1.15.3
export PATH=$PATH:$FLINK_HOME/bin
```

​     生效配置：

```shell
source ~/.bashrc
```



### 3、配置flink-conf.yaml文件

在${FLINK_HOME}/conf 路径下配置flink-conf.yaml文件，配置项如下：

```shell
jobmanager.rpc.address: 2-120

jobmanager.rpc.port: 6123

jobmanager.bind-host: 0.0.0.0

jobmanager.memory.process.size: 2g

taskmanager.bind-host: 0.0.0.0

taskmanager.host: 0.0.0.0

taskmanager.memory.process.size: 4g

taskmanager.numberOfTaskSlots: 4

parallelism.default: 4

# JVM Metaspace Memory --- Task Manager JVM的元空间内存大小，默认 256m
taskmanager.memory.jvm-metaspace.size: 512m

taskmanager.memory.jvm-overhead.fraction: 0.2
task.cancellation.timeout: 0

execution.target: yarn-per-job

state.backend: rocksdb

state.checkpoints.dir: hdfs://ns/flink_1.15/checkpoints

state.savepoints.dir: hdfs://ns/flink_1.15/savepoints

# 最多保留已完成的检查点实例的数量
state.checkpoints.num-retained: 100
state.backend.incremental: true

akka.ask.timeout: 100s
web.timeout: 300000

taskmanager.debug.memory.log: true
taskmanager.debug.memory.log-interval: 10000
taskmanager.network.netty.transport: "epoll"

jobmanager.execution.failover-strategy: region

rest.address: 0.0.0.0

rest.bind-port: 50100-50200

rest.bind-address: 0.0.0.0

historyserver.web.address: 2-120
historyserver.archive.clean-expired-jobs: true

# JobManager归档的作业信息所存放的目录
jobmanager.archive.fs.dir: hdfs://ns/flink_1.15/completed-jobs/

# 指定History Server扫描哪些归档目录
historyserver.archive.fs.dir: hdfs://ns/flink_1.15/completed-jobs/

# 指定History Server间隔多少毫秒扫描一次归档目录
historyserver.archive.fs.refresh-interval: 60000
```



### 4、配置 masters 文件

在${FLINK_HOME}/conf 路径下配置 masters 文件，配置项如下：

```shell
2-120:8281
2-124:8281
```



### 5、配置 workers文件

在${FLINK_HOME}/conf 路径下配置 workers文件，配置项如下：

```shell
2-120
2-123
2-124
```



### 6、Flink 改log、pid、jvm的配置

在${FLINK_HOME}/bin/ 路径下配置 config.sh 脚本，配置项如下：

```shell
DEFAULT_FLINK_LOG_DIR=/data/flink-1.15.3/log
DEFAULT_ENV_PID_DIR="/data/flink-1.15.3/pid" 
DEFAULT_ENV_JAVA_OPTS="-Xmx4096m"
```



### 7、将下面jar包放入至${FLINK_HOME}/lib下

在${FLINK_HOME}/lib 路径下补充以下 jar 包，jar 包名称如下：

```shell
flink-avro-1.15.3.jar
flink-connector-hive_2.12-1.15.3.jar
flink-connector-jdbc-1.15.3.jar
flink-sql-connector-hive-3.1.2_2.12-1.15.3.jar
flink-sql-connector-kafka-1.15.3.jar
flink-sql-connector-mysql-cdc-2.3.0.jar
flink-sql-connector-postgres-cdc-2.3.0.jar
guava-27.0-jre.jar
hadoop-mapreduce-client-core-3.2.4.jar
hive-exec-3.1.2.jar
mysql-connector-java-8.0.16.jar
taos-jdbcdriver-3.0.0.jar
```



### 8、将Flink 跟目录分发至各服务器节点

```shell
scp -r /workspace/opt/flink-1.15.3  zhongtai@2-123:/workspace/opt/
scp -r /workspace/opt/flink-1.15.3  zhongtai@2-124:/workspace/opt/
scp ~/.bashrc zhongtai@2-123:~/
scp ~/.bashrc zhongtai@2-124:~/
```



### 9、启动服务

```shell
bin/start-cluster.sh
```



## 运维命令

（1）启停Flink JobManager进程：

```shell
${FLINK_HOME}/bin/jobmanager.sh stop
${FLINK_HOME}/bin/jobmanager.sh start
```

（2）启停止Flink TaskManager进程：

```shell
${FLINK_HOME}/bin/taskmanager.sh stop
${FLINK_HOME}/bin/taskmanager.sh start
```

（3）停止Flink集群：

```shell
${FLINK_HOME}/bin/stop-cluster.sh
${FLINK_HOME}/bin/start-cluster.sh
```

（4）停止Flink历史服务器：

```shell
${FLINK_HOME}/bin/historyserver.sh start
${FLINK_HOME}/bin/historyserver.sh stop
```

（5）查看Flink集群日志：

```shelll
tail -1000f ${FLINK_HOME}/log/flink-xxx-standalonesession-xxx-xxx.log

例如：
tail -1000f /data/flink-1.15.3/log/flink-zhongtai-standalonesession-0-hcdb-nn1.log
```







# FLINK ON Yarn 环境配置

### 1、配置HADOOP_CLASSPATH

在${FLINK_HOME}/bin 路径下配置 flink 脚本，添加配置项如下

```
export HADOOP_CLASSPATH=`hadoop classpath`
```



### 2、配置完成进行测试

测试代码：

```
bin/flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 ./examples/batch/WordCount.jar
```





# FLINK ON HIVE 环境配置



### 1、将下面jar包放入至${FLINK_HOME}/lib下

```
flink-connector-hive_2.12-1.15.3.jar
flink-sql-connector-hive-3.1.2_2.12-1.15.3.jar
hive-exec-3.1.2.jar
hadoop-mapreduce-client-core-3.2.4.jar
```

注：如果在flinksql中使用hive方言，则删除${FLINK_HOME}/lib中 **flink-table-planner-loader-1.15.3.jar**，添加 **flink-table-planner_2.12-1.15.3.jar**



### 2、在${FLINK_HOME}/conf目录下添加hive-site.xml

```
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
      <name>hive.metastore.warehouse.dir</name>
      <value>/user/hive/warehouse</value>
    </property>
    <property>
      <name>hive.metastore.uris</name>
      <value>thrift://172.16.2.120:9083</value>
    </property>
</configuration>
```



### 3、在${FLINK_HOME}/conf目录下添加sql-catalogs.sql

```
CREATE CATALOG hive_catalog WITH (
    'type' = 'hive',
    'default-database' = 'default',
    'hive-conf-dir' = '/workspace/opt/flink-1.15.3/conf'
);
```



### 4、启动sql-client

```
${FLINK_HOME}/bin/sql-client.sh -i ${FLINK_HOME}/conf/sql-catalogs.sql
```

```
Flink SQL> SET sql-client.execution.result-mode=tableau;
[INFO] Session property has been set.

Flink SQL> show catalogs;
+-----------------+
|    catalog name |
+-----------------+
| default_catalog |
|    hive_catalog |
+-----------------+
2 rows in set

Flink SQL> use catalog hive_catalog;
[INFO] Execute statement succeed.

Flink SQL> show databases;
+---------------+
| database name |
+---------------+
|       default |
|          test |
+---------------+
2 rows in set

Flink SQL> use test;
[INFO] Execute statement succeed.

Flink SQL> show tables;
Empty set

Flink SQL> set table.sql-dialect=hive;
[INFO] Session property has been set.

Flink SQL> create table if not exists test.tb(col string) stored as parquet;Hive Session ID = 48c3bc74-735d-4324-a2aa-118b8fdcce1d
create table if not exists test.tb(col string) stored as parquet;
[INFO] Execute statement succeed.

Flink SQL> insert into tb values("1"),("2");Hive Session ID = 1f11cc7a-e61b-4cb4-b3b2-947f966da529
insert into tb values("1"),("2");
[INFO] Submitting SQL update statement to the cluster...
2023-02-02 14:46:20,206 WARN  org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The configuration directory ('/opt/flink-1.15.3/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.
2023-02-02 14:46:20,470 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2023-02-02 14:46:20,482 WARN  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Job Clusters are deprecated since Flink 1.15. Please use an Application Cluster/Application Mode instead.
2023-02-02 14:46:20,608 INFO  org.apache.hadoop.conf.Configuration                         [] - resource-types.xml not found
2023-02-02 14:46:20,608 INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils           [] - Unable to find 'resource-types.xml'.
2023-02-02 14:46:20,669 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cluster specification: ClusterSpecification{masterMemoryMB=4096, taskManagerMemoryMB=8192, slotsPerTaskManager=4}
2023-02-02 14:46:23,503 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Submitting application master application_1673835230648_0039
2023-02-02 14:46:23,752 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted application application_1673835230648_0039
2023-02-02 14:46:23,752 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Waiting for the cluster to be allocated
2023-02-02 14:46:23,754 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deploying cluster, current state ACCEPTED
2023-02-02 14:46:28,543 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - YARN application has been deployed successfully.
2023-02-02 14:46:28,545 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface hcdb-data5:50100 of application 'application_1673835230648_0039'.
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 96921a032c98618afae4697872fef5bb


Flink SQL> select * from tb;Hive Session ID = 5f2873ad-00c3-42a4-9dc2-cac541ec17ad

2023-02-02 14:47:40,776 INFO  org.apache.hadoop.mapred.FileInputFormat                     [] - Total input files to process : 1
2023-02-02 14:47:41,024 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2023-02-02 14:47:41,025 WARN  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Job Clusters are deprecated since Flink 1.15. Please use an Application Cluster/Application Mode instead.
2023-02-02 14:47:41,037 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cluster specification: ClusterSpecification{masterMemoryMB=4096, taskManagerMemoryMB=8192, slotsPerTaskManager=4}
2023-02-02 14:47:42,909 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Submitting application master application_1673835230648_0040
2023-02-02 14:47:43,123 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted application application_1673835230648_0040
2023-02-02 14:47:43,124 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Waiting for the cluster to be allocated
2023-02-02 14:47:43,126 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deploying cluster, current state ACCEPTED
2023-02-02 14:47:48,415 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - YARN application has been deployed successfully.
2023-02-02 14:47:48,416 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface hcdb-data2:50102 of application 'application_1673835230648_0040'.
+----+--------------------------------+
| op |                            col |
+----+--------------------------------+
| +I |                              1 |
| +I |                              2 |
+----+--------------------------------+
Received a total of 2 rows
```



#### 基础配置参数

1.设置执行模式

批模式

```
SET sql-client.execution.mode=batch;
```

流模式

```
SET sql-client.execution.mode=streaming;
```

2.显示查询结果

**表格模式**（table mode）在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用：

```
SET sql-client.execution.result-mode = table；
```

**变更日志模式**（changelog mode）不会实体化和可视化结果，而是由插入（`+`）和撤销（`-`）组成的持续查询产生结果流。

```
SET sql-client.execution.result-mode = changelog;
```

**Tableau模式**（tableau mode）更接近传统的数据库，会将执行的结果以制表的形式直接打在屏幕之上。具体显示的内容会取决于作业 执行模式的不同(`execution.type`)：

```
SET sql-client.execution.result-mode = tableau;
```

3.使用hive方言

```
set table.sql-dialect=hive;
```







## FAQ:Flink On Yarn提交任务和注意事项

​		参考：[Flink on Yarn提交任务注意事项]()





































