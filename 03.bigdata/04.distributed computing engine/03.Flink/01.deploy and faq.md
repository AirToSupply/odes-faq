## 一.节点规划

|                               | StandaloneSessionClusterEntrypoint | TaskManagerRunner |
| ----------------------------- | ---------------------------------- | ----------------- |
| 10.232.3.206（zzcywhadoop11） |                                    | √                 |
| 10.232.3.207（zzcywhadoop12） |                                    | √                 |
| 10.232.3.217 （bigdata01）    | √                                  |                   |
| 10.232.3.218 （bigdata02）    | √                                  |                   |
| 10.232.3.219（bigdata03）     |                                    | √                 |



## 二.安装步骤

​		（1）解压安装包。

```shell
cd /workspace/opt
tar -zxvf /tmp/modules/flink-1.13.1-bin-scala_2.12.tgz -C .
```

​		（2）配置flink集群参数

​                ${FLINK_HOME}/conf/flink-conf.yaml：

```shell
jobmanager.rpc.address: bigdata02

jobmanager.rpc.port: 6223

rest.port: 8281

jobmanager.memory.process.size: 4g
jobmanager.archive.fs.dir: hdfs://ns/flink_1.13/completed-jobs/

#Total Process Memory --- flink task manage 总内存
#taskmanager.memory.process.size: 24g

#Total Flink Memory --- flink task executor 总共内存，不建议与 Total Process Memory 同时配置
taskmanager.memory.flink.size : 12g

#Framework Heap --- task executor 自身保留内存, 一般无需配置
#taskmanager.memory.framework.heap.size : 128m

# Task Heap --- 每一个task 运行所需内存, 默认为 Total Flink Memory – Framework Heap – Task off-heap memory – Managed Memory – Network Memory
#taskmanager.memory.task.heap.size: 4g

# Managed Memory --- 由flink 直接管理的 off-heap 内存，主要用于 排序，哈希表，中间结果缓存，RocksDB 的 backend。
# 其默认是通过 taskmanager.memory.managed.fraction配置的因子（默认0.4）来设置，即为 Total FLink Memory 的 40%
#taskmanager.memory.managed.size

#Framework Off-heap Memory --- Task Executor保留的off-heap memory，不会分配给任何slot, 默认为 128M
#taskmanager.memory.framework.off-heap.size: 128m

# Task Off-heap Memory --- Task Executor执行的Task所使用的堆外内存。如果在Flink应用的代码中调用了Native的方法，需要用到off-heap内存，这些内存会分配到Off-heap堆外内存中，默认为 0
# taskmanager.memory.task.off-heap.size

# Network Memory --- Network Memory使用的是Directory memory，在Task与Task之间进行数据交换时（shuffle），需要将数据缓存下来，缓存能够使用的内存大小就是这个Network Memory。它由是三个参数决定
# taskmanager.memory.network.min: 64m
# taskmanager.memory.network.max: 1g
# 占Total Flink Memory的百分比，默认为Total Flink Meory的10%
# taskmanager.memory.network.fraction: 0.1

# JVM Metaspace Memory --- Task Manager JVM的元空间内存大小，默认 256m
taskmanager.memory.jvm-metaspace.size: 512m

# JVM Overhead 保留给JVM其他的内存开销。例如：Thread Stack、code cache、GC回收空间等等。和Network Memory的配置方法类似。它也由三个配置决定
# taskmanager.memory.jvm-overhead.min: 192m
# taskmanager.memory.jvm-overhead.max: 1g
taskmanager.memory.jvm-overhead.fraction: 0.2
task.cancellation.timeout: 0
taskmanager.numberOfTaskSlots: 4
parallelism.default: 4

#==============================================================================
# Fault tolerance and checkpointing
#==============================================================================
state.backend: rocksdb
state.checkpoints.dir: hdfs://ns/flink_1.13/checkpoints
state.savepoints.dir: hdfs://ns/flink_1.13/checkpoints
# 最多保留已完成的检查点实例的数量
state.checkpoints.num-retained: 100
state.backend.incremental: true


akka.ask.timeout: 100s
web.timeout: 300000

taskmanager.debug.memory.log: true
taskmanager.debug.memory.log-interval: 10000
taskmanager.network.netty.transport: "epoll"

jobmanager.execution.failover-strategy: region

# fix hudi 0.9 改变类加载机制
classloader.resolve-order: parent-first
classloader.check-leaked-classloader: false

# 设置ssh端口
env.ssh.opts: -p 43215

# flink历史服务器配置
historyserver.web.address: bigdata02
historyserver.archive.clean-expired-jobs: true
# 指定History Server所监听的端口号
historyserver.web.port: 7082
# 指定History Server间隔多少毫秒扫描一次归档目录
historyserver.archive.fs.refresh-interval: 60000
# JobManager归档的作业信息所存放的目录
jobmanager.archive.fs.dir: hdfs://ns/flink_1.13/completed-jobs/
# 指定History Server扫描哪些归档目录
historyserver.archive.fs.dir: hdfs://ns/flink_1.13/completed-jobs/


# flink ha配置
high-availability: zookeeper
high-availability.zookeeper.quorum: bigdata01:2181,bigdata07:2181,bigdata04:2181
high-availability.zookeeper.path.root: /flink_ha_1.13
high-availability.cluster-id: /flink_1.13
high-availability.storageDir: hdfs://ns/flink_1.13/ha/recovery
```

​		  ${FLINK_HOME}/conf/masters：

```
bigdata01:8281
bigdata02:8281
```

​	      ${FLINK_HOME}/conf/workers：

```shell
zzcywhadoop11
zzcywhadoop12
bigdata03
```

​		（3）将需要的lib库文件拷贝到flink的lib目录下：

```shell
[zhongtai@bigdata02 lib]$ ll
total 268520
-rw-r--r-- 1 zhongtai zhongtai    128261 Aug 24 20:03 flink-avro-1.13.1.jar
-rw-r--r-- 1 zhongtai zhongtai    248980 Aug 24 20:03 flink-connector-jdbc_2.12-1.13.1.jar
-rw-r--r-- 1 zhongtai zhongtai    351089 Aug 24 20:03 flink-connector-kafka_2.12-1.13.1.jar
-rw-r--r-- 1 zhongtai zhongtai     92311 Aug 24 20:03 flink-csv-1.13.1.jar
-rw-r--r-- 1 zhongtai zhongtai 106648120 Aug 24 20:03 flink-dist_2.12-1.13.1.jar
-rw-r--r-- 1 zhongtai zhongtai     19175 Nov 12 14:22 flink-format-changelog-json-2.0.2.jar
-rw-r--r-- 1 zhongtai zhongtai    148131 Aug 24 20:03 flink-json-1.13.1.jar
-rw-rw-r-- 1 zhongtai zhongtai   7709740 Aug 24 20:03 flink-shaded-zookeeper-3.4.14.jar
-rw-r--r-- 1 zhongtai zhongtai  30089752 Sep 27 16:11 flink-sql-connector-mysql-cdc-2.0.2.jar
-rw-r--r-- 1 zhongtai zhongtai  35015429 Aug 24 20:03 flink-table_2.12-1.13.1.jar
-rw-r--r-- 1 zhongtai zhongtai  38533349 Aug 24 20:03 flink-table-blink_2.12-1.13.1.jar
-rw-r--r-- 1 zhongtai zhongtai  51002589 Aug 24 20:03 hudi-flink-bundle_2.12-0.10.0_824.jar
-rw-r--r-- 1 zhongtai zhongtai   1893108 Aug 24 20:03 kafka-clients-2.1.1.jar
-rw-rw-r-- 1 zhongtai zhongtai     67114 Aug 24 20:03 log4j-1.2-api-2.12.1.jar
-rw-rw-r-- 1 zhongtai zhongtai    276771 Aug 24 20:03 log4j-api-2.12.1.jar
-rw-rw-r-- 1 zhongtai zhongtai   1674433 Aug 24 20:03 log4j-core-2.12.1.jar
-rw-rw-r-- 1 zhongtai zhongtai     23518 Aug 24 20:03 log4j-slf4j-impl-2.12.1.jar
-rw-r--r-- 1 zhongtai zhongtai   1004719 Aug 24 20:03 postgresql-42.2.18.jar
```

​      （4）将flink跟目录分发至对应节点。

​	 （5）启动服务。

```shell
bin/start-cluster.sh
```



## 三.运维命令

（1）启停Flink JobManager进程：

```shell
${FLINK_HOME}/bin/jobmanager.sh stop
${FLINK_HOME}/bin/jobmanager.sh start
```

（2）启停止Flink TaskManager进程：

```shell
${FLINK_HOME}/bin/taskmanager.sh stop
${FLINK_HOME}/bin/taskmanager.sh start
```

（3）停止Flink集群：

```shell
${FLINK_HOME}/bin/stop-cluster.sh
${FLINK_HOME}/bin/start-cluster.sh
```

（4）停止Flink历史服务器：

```shell
${FLINK_HOME}/bin/historyserver.sh start
${FLINK_HOME}/bin/historyserver.sh stop
```

（5）查看Flink集群日志：

```shelll
tail -1000f ${FLINK_HOME}/log/flink-xxx-standalonesession-xxx-xxx.log
```



## 四.问题指南

（1）Standalone模式下任务运行一段时间taskmanager挂掉,报错如下：

```shell
Task ‘Source: Custom Source -> Map -> Map -> to: Row -> Map -> Sink: Unnamed (1/3)’ did not react to cancelling signal for 30 seconds Task did not exit gracefully within 180 + seconds. 
```

​		修改配置文件flink-conf.yaml，然后添加配置task.cancellation.timeout: 0。这个配置的含义是,超时（以毫秒为单位），在此之后任务取消超时并导致致命的TaskManager错误，设置值为0将禁用watch dog。

（2）Flink本地消费kafka的时候，报错如下：

```shell
Unable to retrieve any partitions with KafkaTopicsDescriptor: Fixed Topics ([jason_flink])
```

​        这个报错其实是Kafka已经挂了，查看Kafka的进程还在，但是连接不上。这是因为Kafka出现假死了，具体假死的原因需要Kafka侧排查，或者重启一下Kafka集群就可以消费了。

（3）出现如下异常：

```shell
Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'hdfs'. The scheme is not directly supported by Flink and no Hadoop file system to support this scheme could be loaded.
	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:403)
	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:318)
	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298)
	at org.apache.flink.runtime.state.filesystem.FsCheckpointStorage.<init>(FsCheckpointStorage.java:58)
	at org.apache.flink.runtime.state.filesystem.FsStateBackend.createCheckpointStorage(FsStateBackend.java:450)
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createCheckpointStorage(RocksDBStateBackend.java:458)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:249)
	... 19 more
Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Hadoop is not in the classpath/dependencies.
	at org.apache.flink.core.fs.UnsupportedSchemeFactory.create(UnsupportedSchemeFactory.java:64)
	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:399)
	... 25 more
```

​			 原因是Flink缺少hadoop的依赖包，在官网下载对应版本的hadoop依赖包，放到Flink的lib目即可。

​             更多问题可以参考：https://www.freesion.com/article/9802218715/。

（4）flink提交yarn任务，报错如下：

```
Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
```

​			 解决方法：

​			 修改：flink/conf 的 flink-conf.yaml文件

​			 添加：

​			 classloader.check-leaked-classloader: false

​			 注意：false前需要加上一个空格

（5）执行flinklsql，报错如下：

```
java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
```

​			 查看hadoop安装目录下share/hadoop/common/lib内guava.jar版本和hive安装目录下lib内guava.jar的版本，两者版本是否一致，
​			 不一致将低版本改为高版本，另将jar放到flink安装目录下lib内
