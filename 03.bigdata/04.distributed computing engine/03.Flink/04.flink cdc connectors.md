#### MySQL to Kafka

1.将 flink-sql-connector-mysql-cdc-2.3.0.jar、flink-sql-connector-kafka-1.15.3.jar放至${FLINK_HOME}/lib下

```sql
set execution.checkpointing.interval=3000;

CREATE TABLE stu (
  id INT,
  name STRING
) WITH (
    'connector' = 'mysql-cdc',
    'scan.startup.mode' = 'initial',
    'hostname' = '172.16.3.111',
    'port' = '3307',
    'username' = 'root',
    'password' = 'Mysql123',
    'database-name' = 'test',
    'table-name' = 'stu',
    'scan.incremental.snapshot.enabled'='false'
);

CREATE TABLE kafka_topic_table_test (
  id int,
  name string
) WITH (
 'connector' = 'kafka',
 'topic' = 't1',
 'properties.bootstrap.servers' = 'hcdb-data4:9092,hcdb-data5:9092,hcdb-data6:9092',
 'scan.startup.mode' = 'earliest-offset',
 'format' = 'debezium-json'
);

insert into kafka_topic_table_test select * from stu;
```

#### Postgres to Kafka

1.将 flink-sql-connector-postgres-cdc-2.3.0.jar、flink-sql-connector-kafka-1.15.3.jar放至${FLINK_HOME}/lib下

```sql
set execution.checkpointing.interval=3000;

CREATE TABLE test_stu (
  id INT,
  name STRING,
  age INT
) WITH (
  'connector' = 'postgres-cdc',
  'hostname' = '172.16.10.72',
  'port' = '5432',
  'username' = 'postgres',
  'password' = 'postgres',
  'database-name' = 'data_share_repository',
  'schema-name' = 'public',
  'table-name' = 'test_stu',
  'decoding.plugin.name' = 'pgoutput',
  'debezium.slot.name' = 'cdc_pg_source'
);

CREATE TABLE kafka_topic_table_test (
  id INT,
  name STRING,
  age INT
) WITH (
 'connector' = 'kafka',
 'topic' = 'test',
 'properties.bootstrap.servers' = '2-120:9092,2-123:9092,2-124:9092',
 'scan.startup.mode' = 'earliest-offset',
 'format' = 'changelog-json'
);

insert into kafka_topic_table_test select id,name,age from test_stu;
```

#### kafka to Hive
1.将 flink-sql-connector-kafka-1.15.3.jar放至${FLINK_HOME}/lib下

```sql
set execution.checkpointing.interval=3000;

CREATE TABLE kafka_topic_table_ceshi (
`event_time` TIMESTAMP(3) METADATA FROM 'timestamp',
`topic` STRING METADATA VIRTUAL,
`partition` BIGINT METADATA VIRTUAL,
`offset` BIGINT METADATA VIRTUAL,
`content` STRING
) WITH (
 'connector' = 'kafka',
 'topic' = 'ceshi',
 'properties.bootstrap.servers' = '172.16.2.120:9092,172.16.2.123:9092,172.16.2.124:9092',
 'scan.startup.mode' = 'latest-offset',
 'format' = 'raw'
);
 
 insert into `hive_catalog`.`ceshi`.`kafka1` select `content` as _content,`topic` as `_topic` ,`offset` as `_offset` ,
 `event_time` as `_time` from kafka_topic_table_test1234;
```

#### SQL Server to Hudi
1.将 flink-sql-connector-sqlserver-cdc-2.3.0.jar、hudi-flink1.15-bundle-0.12.3.jar放至${FLINK_HOME}/lib下

2.进入客户端
```shell
${FLINK_HOME}/bin/sql-client.sh -j ${FLINK_HOME}/lib/hudi-flink1.15-bundle-0.12.3.jar shell
```

```sql
set execution.checkpointing.interval=3000;

CREATE TABLE Demo3 (
  Id BIGINT,
  name STRING,
  ts TIMESTAMP(6),
  ts1 TIMESTAMP(6),
  ts2 DATE,
  PRIMARY KEY(Id) NOT ENFORCED
) WITH (
    'connector' = 'sqlserver-cdc',
    'hostname' = '172.16.2.124',
    'port' = '1433',
    'username' = 'sa',
    'password' = 'msql@123',
    'database-name' = 'test0504',
    'schema-name' = 'dbo',
    'table-name' = 'Demo3'
);

CREATE TABLE test_hudi_flink1 (
  id INTEGER PRIMARY KEY,
  accoutnum INTEGER,
  accountname STRING,
  balance FLOAT,
  createtime TIMESTAMP(3),
  updatetime TIMESTAMP(3)
)
PARTITIONED BY (updatetime)
WITH (
  'connector' = 'hudi',
  'path' = '/tmp/hudi/test_hudi_demo',
  'table.type' = 'MERGE_ON_READ',
  'hoodie.datasource.write.recordkey.field' = 'id',
  'hoodie.datasource.write.partitionpath.field' = 'updatetime',
  'write.precombine.field' = 'id',
  'hive_sync.enable' = 'true',
  'hive_sync.mode' = 'jdbc',
  'hive_sync.metastore.uris' = 'thrift://172.16.2.120:9083',
  'hive_sync.jdbc_url' = 'jdbc:hive2://172.16.2.123:10000',
  'hive_sync.table' = 'test_hudi_demo',
  'hive_sync.db' = 'ds',
  'hive_sync.partition_fields' = 'ts2',
  'hive_sync.username' = 'hive',
  'hive_sync.password' = 'hive'
);

insert into test_hudi_flink1 select id,accoutnum,accountname,balance,createtime,updatetime from Demo;
```

#### Oracle to Hudi
1.将 flink-sql-connector-oracle-cdc-2.3.0.jar、hudi-flink1.15-bundle-0.12.3.jar放至${FLINK_HOME}/lib下

2.进入客户端
```shell
${FLINK_HOME}/bin/sql-client.sh -j ${FLINK_HOME}/lib/hudi-flink1.15-bundle-0.12.3.jar shell
```

```sql
CREATE TABLE Demo (
  Id BIGINT,
  name STRING,
  ts TIMESTAMP(3),
  ts1 TIMESTAMP(3),
  ts2 TIMESTAMP(3),
  PRIMARY KEY(Id) NOT ENFORCED
) WITH (
    'connector' = 'oracle-cdc',
    'scan.startup.mode' = 'initial',
    'hostname' = '172.16.10.72',
    'port' = '1521',
    'username' = 'ROOT',
    'password' = 'ROOT',
    'database-name' = 'ORCL',
    'schema-name' = 'ROOT',
    'table-name' = 'Demo',
    'debezium.log.mining.strategy' = 'online_catalog',
    'debezium.log.mining.continuous.mine' = 'true'
);

CREATE TABLE test_hudi_flink1 (
  Id BIGINT,
  name STRING,
  ts TIMESTAMP(3),
  ts1 TIMESTAMP(3),
  ts2 TIMESTAMP(3)
)
PARTITIONED BY (ts2)
WITH (
  'connector' = 'hudi',
  'path' = '/tmp/hudi/test_hudi_flink',
  'table.type' = 'MERGE_ON_READ',
  'hoodie.datasource.write.recordkey.field' = 'Id',
  'hoodie.datasource.write.partitionpath.field' = 'ts2',
  'write.precombine.field' = 'Id',
  'hive_sync.enable' = 'true',
  'hive_sync.mode' = 'jdbc',
  'hive_sync.metastore.uris' = 'thrift://172.16.2.120:9083',
  'hive_sync.jdbc_url' = 'jdbc:hive2://172.16.2.123:10000',
  'hive_sync.table' = 'test_hudi_demo',
  'hive_sync.db' = 'ds',
  'hive_sync.partition_fields' = 'ts2',
  'hive_sync.username' = 'hive',
  'hive_sync.password' = 'hive'
);

insert into test_hudi_flink1 select Id,name,ts,ts1,ts2 from Demo;
```