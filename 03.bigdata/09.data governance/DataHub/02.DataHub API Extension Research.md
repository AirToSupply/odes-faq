# DataHub API 扩展调研

# 支持的语言

DataHub提供了多种API 应用程序接口，包括：Python ，Java，GraphQL API ， OpenAPI，Rest.li API。

最推荐的用于扩展和自定义数据中心实例行为的工具是 Python 和 Java 中的 SDK。

参考链接：[Which DataHub API is for me? | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/datahub-apis/)



## 数据中心接口比较

数据中心支持多个 API，每个 API 都有自己独特的用法和格式。 下面概述了每个 API 可以执行的操作。

> Last Updated : Apr 8 2023

| Feature                                                 | GraphQL                                                      | Python SDK                                                   | OpenAPI |
| ------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------- |
| Create a dataset                                        | 🚫                                                            | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/datasets) | ✅       |
| Delete a dataset (Soft delete)                          | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/datasets#delete-dataset) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/datasets#delete-dataset) | ✅       |
| Delete a dataset (Hard delele)                          | 🚫                                                            | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/datasets#delete-dataset) | ✅       |
| Search a dataset                                        | ✅                                                            | ✅                                                            | ✅       |
| Create a tag                                            | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅       |
| Read a tag                                              | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅       |
| Add tags to a dataset                                   | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅       |
| Add tags to a column of a dataset                       | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅       |
| Remove tags from a dataset                              | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/tags#add-tags) | ✅       |
| Create glossary terms                                   | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/terms) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/terms) | ✅       |
| Read terms from a dataset                               | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/terms) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/terms) | ✅       |
| Add terms to a column of a dataset                      | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/terms) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/terms) | ✅       |
| Add terms to a dataset                                  | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/terms) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/terms) | ✅       |
| Create domains                                          | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/domains) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/domains) | ✅       |
| Read domains                                            | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/domains) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/domains) | ✅       |
| Add domains to a dataset                                | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/domains) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/domains) | ✅       |
| Remove domains from a dataset                           | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/domains) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/domains) | ✅       |
| Crate users and groups                                  | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/owners) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/owners) | ✅       |
| Read owners of a dataset                                | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/owners) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/owners) | ✅       |
| Add owner to a dataset                                  | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/owners) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/owners) | ✅       |
| Remove owner from a dataset                             | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/owners) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/owners) | ✅       |
| Add lineage                                             | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/lineage) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/lineage) | ✅       |
| Add column level(Fine Grained) lineage                  | 🚫                                                            | ✅                                                            | ✅       |
| Add documentation(description) to a column of a dataset | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/descriptions#add-description-on-column) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/descriptions#add-description-on-column) | ✅       |
| Add documentation(description) to a dataset             | 🚫                                                            | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/descriptions#add-description-on-dataset) | ✅       |
| Add / Remove / Replace custom properties on a dataset   | 🚫 [[Guide\]](https://datahubproject.io/docs/api/tutorials/custom-properties) | ✅ [[Guide\]](https://datahubproject.io/docs/api/tutorials/custom-properties) | ✅       |



## Java Emitter

在某些情况下，你可能想直接构建元数据事件，并使用编程方式将元数据发射到DataHub。用例通常是基于推送的，包括从CI/CD管道、自定义协调器等发射元数据事件。

io.acryl:datahub-client Java包提供了REST发射器API-s，可以很容易地用于从你的基于JVM的系统发射元数据。例如，Spark lineage集成使用Java发射器，从Spark作业中发射元数据事件。

### 

基于 Java 的元数据摄取系统提供了 REST emitter 、 Kafka emitter 和 File emitter，很容易进行代码集成

**RESTEmitter**: 基于requests 模块进行了一层精简包装，它支持元数据的非阻塞排放，并处理元数据在线路上的JSON序列化细节。

构建一个REST Emitter遵循一个基于lambda的流畅构建器模式。配置参数大部分都反映了Python发射器的配置。此外，你也可以通过向HttpClient构建器传递自定义参数来定制在引擎下构建的HttpClient。

REST emitter code：https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/datahub-client/src/main/java/datahub/client/rest/RestEmitter.java



**Kafka Emitter**: 基于confluent- kaka的SerializingProducer类之上的一个包装，提供了一个非阻塞接口用于向DataHub发送元数据事件。
当你想通过利用Kafka作为一个高可用的消息总线，使你的元数据生产者与你的datahub元数据服务器的正常运行时间脱钩时，可以使用它。

例如，如果你的DataHub元数据服务由于计划内或计划外的中断而宕机，你仍然可以通过将其发送到Kafka来继续从你的关键任务系统中收集元数据。当元数据排放的吞吐量比元数据被持久化到DataHub的后端存储的确认更重要时，也可以使用这个发射器。
当将元数据发送的吞吐量比确认元数据被持久化到DataHub的后端存储更重要时，请使用此方法.
注意:
Kafka发射器使用Avro将Metadata事件序列化到Kafka。改变序列化器将导致无法处理的事件，因为DataHub目前希望通过Kafka的元数据事件能在Avro中序列化。

Kafka emitter code：https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/datahub-client/src/main/java/datahub/client/kafka/KafkaEmitter.java



**File Emitter**: 将元数据变化建议事件（MCP）写入一个JSON文件，随后可以交给Python File源进行接收。这与Python中的File sink的工作原理类似。当产生元数据事件的系统没有直接连接到DataHub的REST服务器或Kafka经纪人时，可以使用这种机制。生成的JSON文件可以稍后传输，然后使用文件源摄取到DataHub。

File emitter code：https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/datahub-client/src/main/java/datahub/client/file/FileEmitter.java





## Python Emitter

Python包提供了REST和Kafka发射器的API，可以很容易地从代码中导入和调用。

注意：API指南中，有使用Python API SDK的例子。请注意教程中的标签。

**RESTEmitter**: 基于requests 模块进行了精简包装，提供了一个通过HTTP发送元数据事件的阻塞接口，主要用于如下2种场景:
当元数据被持久化到DataHub的元数据存储的简单性和确认比元数据排放的吞吐量更重要时，可以使用这个。当存在先读后写的情况时，也可以使用它，例如，写完元数据后立即读回它。

其他例子：

- [lineage_emitter_mcpw_rest.py](https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/examples/library/lineage_emitter_mcpw_rest.py) - 通过 REST 作为 MetadataChangeProposalWrapper 发出简单的 bigquery 表到表（数据集到数据集）沿袭。

REST emitter code：https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/emitter/rest_emitter.py



**Kafka Emitter**: SerializingProducer类上面的精简包装器，它提供了一个非阻塞的接口来发送元数据事件到DataHub。当你想通过利用Kafka作为一个高可用的消息总线，使你的元数据生产者与你的datahub元数据服务器的正常运行时间脱钩时，可以使用它。例如，如果你的DataHub元数据服务由于计划内或计划外的中断而宕机，你仍然可以通过将其发送到Kafka来继续从你的关键任务系统中收集元数据。当元数据排放的吞吐量比元数据被持久化到DataHub的后端存储的确认更重要时，也可以使用这个发射器。

注意：Kafka 发射器使用 Avro 将元数据事件序列化为 Kafka。更改序列化程序将导致无法处理的事件，因为 DataHub 当前期望在 Avro 中序列化 Kafka 上的元数据事件。

Kafka emitter code：https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/emitter/kafka_emitter.py



**Python Emitter 开发参考**：[Builder | DataHub (datahubproject.io)](https://datahubproject.io/docs/python-sdk/builder)







# API and SDK Guides

### 1）数据集

数据集实体是元数据模型中最重要的实体之一。它们表示通常表示为数据库中的表或视图的数据集合（例如BigQuery，Snowflake，Redshift等），流处理环境中的流（Kafka，Pulsar等），在数据湖系统中作为文件或文件夹找到的数据包（S3，ADLS等）。 有关数据集的详细信息，请参阅[数据集](https://datahubproject.io/docs/generated/metamodel/entities/dataset)。

本指南将向您展示目标如下：

- 创建：创建包含三列的数据集。
- 删除：删除数据集。

官网参考链接：[数据集 |数据中心 (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/datasets)



### 2）血缘

血缘是是用来捕捉组织内的数据依赖关系的。它允许你跟踪一个数据资产的输入，以及下游依赖它的数据资产。关于血缘的更多信息， 请参阅[About DataHub Lineage](https://datahubproject.io/docs/lineage/lineage-feature-guide).

本指南将向您展示目标如下：

- 在数据集之间添加血缘。
- 在数据集之间添加列级血缘。

官网参考链接：[Dataset | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/lineage)



### 3）标签

标签是非正式的、松散控制的标签，有助于搜索和发现。它们可以添加到数据集、数据集架构或容器中，以便轻松标记或分类实体，而无需将它们与更广泛的业务词汇表或词汇表相关联。 有关标记的详细信息，请参阅[About DataHub Tags](https://datahubproject.io/docs/tags) .

本指南将向您展示目标如下：

- 创建：创建标签。
- 读取：读取附加到数据集的标记。
- 添加：为数据集的列或数据集本身添加标签。
- 删除：从数据集中删除标记。

官网参考链接：[Dataset | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/tags)





### 4）Terms

DataHub中的商业术语（Term）功能可以帮助你在组织内使用一个共享的词汇，它提供了一个框架来定义一套标准化的数据概念，然后将它们与你的数据生态系统中存在的物理资产联系起来。

关于术语的更多信息，请参考《关于DataHub商业词汇》。请参阅 [About DataHub Business Glossary](https://datahubproject.io/docs/glossary/business-glossary) .

本指南将向您展示目标如下：

- 创建：创建术语。
- 读取：读取附加到数据集的术语。
- 添加：在数据集的列或数据集本身中添加术语。
- 删除：从数据集中删除术语。

官网参考链接：[Dataset | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/terms)





### 5）用户和组

用户和组对于管理数据的所有权至关重要。通过创建或更新用户账户并将其分配给适当的组，管理员可以确保正确的人可以访问他们工作所需的数据。这有助于避免在谁负责特定数据集方面的混乱或冲突，并能提高整体效率。 有关数据集的详细信息，请参阅[数据集](https://datahubproject.io/docs/generated/metamodel/entities/dataset) 。

本指南将向您展示目标如下：

- 创建：创建或更新用户和组。
- 读取：读取附加到数据集的所有者。
- 添加：将用户组作为所有者添加到数据集。
- 删除：从数据集中删除所有者。

官网参考链接：[Ownership | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/owners)





### 6）域名

域是经过策划的顶级文件夹或类别，相关的资产可以被明确地分组。域的管理可以是集中式的，也可以分配给域的所有者 目前，一个资产一次只能属于一个域。关于域的更多信息，请参考[About DataHub Domains](https://datahubproject.io/docs/domains) .

本指南将向您展示目标如下：

- 创建域。
- 读取附加到数据集的属性域。
- 将数据集添加到域
- 从数据集中移除域。

官网参考链接：[Domains | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/domains)





### 7）弃用数据集

数据中心上的弃用功能指示实体的状态。对于数据集，使弃用状态保持最新对于通知用户和下游系统数据集的可用性或可靠性的更改非常重要。通过更新状态，可以主动传达更改、防止出现问题并确保用户始终使用高度受信任的数据资产。

本指南将介绍如何读取或更新数据集的弃用状态。

官网参考链接：[Domains | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/deprecation)





### 8）描述

向数据集添加说明和相关链接可以提供有关数据的重要信息，例如其来源、收集方法和潜在用途。这可以帮助其他人了解数据的背景以及它如何与他们自己的工作或研究相关。包括相关链接还可以提供对其他资源或相关数据集的访问，进一步丰富用户可用的信息。

本指南将向您展示目标如下：

- 读取数据集说明：读取数据集说明。
- 读取列说明：读取数据集列的说明。
- 添加数据集说明：添加数据集说明和链接。
- 添加列说明：向数据集的列添加说明。

官网参考链接：[Description | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/descriptions)





### 9）自定义属性

数据集的自定义属性有助于提供有关标准元数据字段中不容易获得的数据的其他信息。自定义属性可用于描述数据的特定属性，例如使用的度量单位、涵盖的日期范围或数据所属的地理区域。这在处理大型复杂数据集时特别有用，其中其他上下文有助于确保正确有效地使用数据。

数据中心将数据集的自定义属性建模为字符串的键值对映射。

自定义属性还可用于启用高级搜索和发现功能，方法是允许用户根据特定属性对数据集进行筛选和排序。这可以帮助用户快速查找和访问所需的数据，而无需手动查看大量数据集。

本指南将向您展示如何在数据集上添加、删除或替换自定义属性。以下是每个操作的含义：`fct_users_deleted`

- 添加：向数据集添加自定义属性，而不影响现有属性
- 删除：从数据集中删除特定属性，而不影响其他属性
- 替换：完全替换整个属性映射，而不会影响位于同一方面的其他字段。例如 方面包含以及其他字段，如和.`DatasetProperties``customProperties``name``description`

官网参考链接：[Description | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/custom-properties)



### 10）机器学习

机器学习系统已成为现代数据堆栈中的关键功能。 但是，机器学习系统的不同组件（如特征、模型和特征表）之间的关系可能很复杂。 因此，必须发现这些系统，以便于组织其他成员轻松访问和使用。

有关 ML 实体的更多信息，请参阅以下文档：

- [MlFeature](https://datahubproject.io/docs/generated/metamodel/entities/mlfeature)
- [MlFeatureTable](https://datahubproject.io/docs/generated/metamodel/entities/mlfeaturetable)
- [MlModel](https://datahubproject.io/docs/generated/metamodel/entities/mlmodel)
- [MlModelGroup](https://datahubproject.io/docs/generated/metamodel/entities/mlmodelgroup)

本指南将向您展示目标如下：

- 创建 ML 实体：MlFeature、MlFeatureTable、MlModel、MlModelGroup
- 读取 ML 实体：MlFeature、MlFeatureTable、MlModel、MlModelGroup
- 将 MlFeatureTable 或 MlModel 附加到 MlFeature

官网参考链接：[ML System | DataHub (datahubproject.io)](https://datahubproject.io/docs/api/tutorials/ml)



# 本地开发

**datahub 开发指南**

前置要求：

- [Java 11 SDK](https://openjdk.org/projects/jdk/11/)
- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- Docker engine with at least 8GB of memory to run tests.

官网链接：[datahub-web-react | DataHub (datahubproject.io)](https://datahubproject.io/docs/developers)

datahub 如何搭建本地开发环境： https://blog.csdn.net/m0_54252387/article/details/125757670



# 在元数据引入上进行开发

如果只想使用元数据引入，请查看[以用户为中心的](https://datahubproject.io/docs/metadata-ingestion) 指南。 本文档适用于想要开发元数据引入框架并可能参与元数据引入框架的开发人员。

另请查看[添加源](https://datahubproject.io/docs/metadata-ingestion/adding-source) 的指南。

前置要求：

1. 必须在主机环境中安装 Python 3.7+。
2. Java8（gradle 不适用于较新版本）
3. 在 MacOS 上：`brew install librdkafka`
4. 在 Debian/Ubuntu 上：`sudo apt install librdkafka-dev python3-dev python3-venv`
5. 在 Fedora 上（如果使用 LDAP 源代码集成）：`sudo yum install openldap-devel`

官网链接：https://datahubproject.io/docs/metadata-ingestion/developing





# Developing on DataHub - Modules

### 1）datahub-web-react

此模块包含一个用作数据中心 UI 的 React 应用程序。

该应用程序的最初里程碑是实现与以前的 Ember 应用程序的功能对等。这意味着支持

- 数据集配置文件、搜索、浏览体验
- 用户配置文件， 搜索
- LDAP 身份验证流程

官网链接：





### 2）DataHub Frontend

DataHub前端是一个用Java编写的[Play](https://www.playframework.com/) 服务。它作为中层提供 在作为后端服务的[DataHub GMS](https://github.com/datahub-project/datahub/blob/master/metadata-service) 和 [DataHub Web](https://datahubproject.io/docs/datahub-web-react) 之间。

**先决条件**

- 您需要在计算机上安装 [JDK11](https://openjdk.org/projects/jdk/11/)  才能构建 。`DataHub Frontend`
- 您需要拥有 [Chrome](https://www.google.com/chrome/)  网络浏览器 安装以便能够生成，因为 UI 测试依赖于 。`Google Chrome`

官网链接：https://datahubproject.io/docs/datahub-frontend





### 3）DataHub GraphQL Core

DataHub GraphQL API 是一个共享库模块，在 GMS 服务层之上包含一个 GraphQL API。它公开了基于图形的表示形式 允许对元数据图上的实体和方面进行读取和写入，包括数据集、CorpUsers 等。

此模块中包含的是

1. **GMS** 架构：基于 GMS 模型的 GQL 架构，位于[资源](https://github.com/datahub-project/datahub/tree/master/datahub-graphql-core/src/main/resources) 文件夹下。
2. **GMS Data Fetchers**（解析器）：GraphQL 引擎用来解析 GQL 架构中各个字段的组件。
3. **GMS 数据加载器**：GraphQL 引擎用来有效地从下游源获取数据的组件（通过批处理）。
4. **GraphQLEngine**：由 提供的默认对象之上的包装器。提供了一种使用简单的 .`GraphQL``graphql-java``Builder API`
5. **GMSGraphQLEngine**：一个能够使用上述数据获取器+加载器解析GMS模式的引擎（不需要额外的配置）。

选择将这些组件放在库模块中，以便 GraphQL 服务器可以在多种“模式”下部署：

1. **独立**：GraphQL 外观，主要用于从非 Java 环境以编程方式访问 GMS 图
2. **嵌入式**：可在另一个 Java 服务器中利用以显示扩展的 GraphQL 模式。例如，我们使用它来扩展 GMS GraphQL 模式`datahub-frontend`

官网链接：[datahub-graphql-core | DataHub (datahubproject.io)](https://datahubproject.io/docs/datahub-graphql-core)





### 4）Metadata Service

DataHub Metadata Service 是一种用 Java 编写的服务，由多个 servlet 组成：

1. 一个公共的 GraphQL API，用于获取和改变元数据图上的对象。
2. 用于引入构成元数据图的基础存储模型的通用 Rest.li API。

**先决条件**

- 需要在计算机上安装 [JDK8](https://www.oracle.com/java/technologies/jdk8-downloads.html) 才能构建 。`DataHub Metadata Service`

官网链接：[metadata-service | DataHub (datahubproject.io)](https://datahubproject.io/docs/metadata-service)





### 5）metadata-jobs:mae-consumer-job

- 元数据审计事件消费者是一个 Spring 作业，可以单独部署，也可以作为元数据服务的一部分进行部署。

  它的主要功能是侦听由于对元数据图所做的更改而发出的更改日志事件，将元数据模型中的更改转换为更新 反对二级搜索和图形索引（以及其他内容）

  今天，这项工作消耗了两个重要的卡夫卡主题：

    1. `MetadataChangeLog_Versioned_v1`
    2. `MetadataChangeLog_Timeseries_v1`

  > **元数据审核事件**的名称从何而来？好吧，历史。以前，此作业消耗 已弃用并从关键路径中删除的单个主题。因此，这个名字！`MetadataAuditEvent`

官网链接：[metadata-jobs:mae-consumer-job | DataHub (datahubproject.io)](https://datahubproject.io/docs/metadata-jobs/mae-consumer-job)



### 6）mce-consumer-job

- 元数据更改事件使用者是一个 Spring 作业，可以单独部署，也可以作为元数据服务的一部分进行部署。

  它的主要功能是侦听 DataHub 客户端发出的更改建议事件，这些事件请求对元数据图进行更改。然后它应用了 这些针对 DataHub 存储层的请求：元数据服务。

  今天，这项工作消耗了两个主题：

    1. `MetadataChangeProposal_v1`
    2. （已弃用）`MetadataChangeEvent_v4`

  并产生以下主题

    1. `FailedMetadataChangeProposal_v1`
    2. （已弃用）`FailedMetadataChangeEvent_v4`

  > 误导性名称**元数据更改事件**从何而来？好吧，历史。以前，此作业消耗 已弃用并替换为每个方面的元数据更改建议的单个主题。因此，这个名字！`MetadataChangeEvent`

官网链接：[mce-consumer-job | DataHub (datahubproject.io)](https://datahubproject.io/docs/metadata-jobs/mce-consumer-job)





# 插件指南

插件是以自定义方式增强基本数据中心功能的方法。

目前，DataHub正式支持2种类型的插件：

- [认证](https://datahubproject.io/docs/plugins#authentication)
- [授权](https://datahubproject.io/docs/plugins#authorization)

官网链接：[插件指南 |数据中心 (datahubproject.io)](https://datahubproject.io/docs/plugins)
