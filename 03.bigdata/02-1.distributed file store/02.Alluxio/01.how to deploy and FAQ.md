## 一.节点规划

​		本次安装选用的是Alluxio 2.1.2版本，目前部署在视博阿里云上，具体规划如下：

|                  | 172.31.41.30（bigdata01） | 172.31.41.31（bigdata02） | 172.31.41.32（bigdata03） |
| ---------------- | ------------------------- | ------------------------- | ------------------------- |
| AlluxioMaster    | √                         |                           |                           |
| AlluxioJobMaster | √                         |                           |                           |
| AlluxioWorker    |                           | √                         | √                         |
| AlluxioJobWorker |                           | √                         | √                         |
| AlluxioProxy     | √                         | √                         | √                         |



## 二.安装部署

（1）下载

​		下载地址：https://www.alluxio.io/download/，这里选用2.1.2版本。

（2）解压

```shell
tar -zxvf alluxio-2.1.2-bin.tar.gz
```

（3）配置环境变量

```shell
vim /etc/profile
```

​		添加如下环境变量：

```shell
export ALLUXIO_HOME=/home/zhongtai/opt/alluxio-2.1.2
export PATH=$PATH:$ALLUXIO_HOME/bin
```

​		使环境变量生效：

```shell
source /etc/profile
```

（4）配置组件参数

​		进入$ALLUXIO_HOME/conf 目录下，拷贝alluxio-site.properties.template配置文件一份。

```shell
cp alluxio-site.properties.template alluxio-site.properties
```

​		修改该文件中的配置参数，如下：

```properties
# Commmon properties
# 设置主节点主机名
alluxio.master.hostname=bigdata01
# 设置底层挂载的存储系统，这里挂载的HDFS（注意这个目录必须要提前创建）
alluxio.master.mount.table.root.ufs=hdfs://bigdata01:9000/alluxio

# Worker properties
# 配置worker节点的内存大小
alluxio.worker.memory.size=8GB
```

​		在$ALLUXIO_HOME/conf/masters文件中设置主节点：

```shell
bigdata01
```

​		在$ALLUXIO_HOME/conf/workers文件中设置从节点：

```shell
bigdata02
bigdata03
```

​		在完成基础配置之后，需要分发到其他节点。

（5）部署

​	①由于alluxio是内存存储系统，因此需要将其挂到到本地磁盘下。

```shell
bin/alluxio-mount.sh Mount workers
```

​		如果挂载权限，需要执行如下命令。

```shell
bin/alluxio-mount.sh SudoMount workers
```

​		当有All tasks finished字样代表挂载成功！

​	②检查alluxio运行环境。

​		在主节点上执行如下命令：

```shell
alluxio validateEnv master
```

​		在从节点上执行如下命令：

```shell
alluxio validateEnv worker
```

​		上述两部都提示Validation succeeded字样表示验证成功。

​	③格式化集群。

```shell
alluxio format
```

​	④启动集群。

```shell
bin/alluxio-start.sh all
```

​		启动之后在主节点上会有AlluxioMaster，AlluxioJobMaster以及AlluxioProxy三个进程；在从节点上会有AlluxioWorker，AlluxioJobWorker以及AlluxioProxy三个进程。

​	⑤验证。

```shell
bin/alluxio runTests
```

​		在部署成功之后，可以通过http://bigdata01:19999 访问Alluxio Master WebUI，通过http://bigdata02:30000 访问Alluxio Worker WebUI



## 三.运维操作

​	①列出所有文件。

```shell
alluxio fs ls /
```

​	②查看文件内容。

```shell
alluxio fs cat /filename
```

​	③从本地复制文件到alluxio。

```shell
alluxio fs copyFromLocal localFilePath alluxioFilePath
```

​	④从alluxio复制文件到本地。

```shell
alluxio fs copyToLocal alluxioFilePath localFilePath
```

​	⑤复制文件。

```shell
alluxio fs cp alluxioFilePath1 alluxioFilePath2
```

​	⑥释放指定路径下已缓存的数据。

```shell
alluxio fs free path
```

​	⑦创建目录。

```shell
alluxio fs mkdir path
```

​	⑧将底层存储系统目录挂载到alluxio。

```shell
alluxio fs mount alluxioPath udfPath
```

​	⑨删除指定路径文件（包括UFS文件）。

```shell
alluxio fs rm path
```

​	更多的运维命令需要参考官方文档。



## 四.案例实战

### 1.集成Spark

​		将$ALLUXIO_HOME/client路径中的alluxio-2.1.2-client.jar分发到Spark所有Driver和Executor上，即$SPARK_HOME/jars目录下，然后就可以通过Spark集群操作alluxio文件系统了。例如上传一份测试数据集test.txt，内容如下：

```tex
hello alluxio
hello spark
```

​		上传至Alluxio，命令如下：

```shell
alluxio fs copyFromLocal test.txt /
```

​		通过$SPARK_HOME/bin/spark-shell 启动交互式命令进行测试，代码如下：

```scala
val data = sc.textFile("alluxio://bigdata01:19998/test.txt")
val wc = data.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)
wc.saveAsTextFile("alluxio://bigdata01:19998/wc")
```

​		查看数据结果：

```shell
alluxio fs cat /wc/part-00000
```

​		结果如下：

```tex
(hello,2)
(alluxio,1)
```



### 2.集成Hive

​		将$ALLUXIO_HOME/client路径中的alluxio-2.1.2-client.jar拷贝到$HIVE_HOME/auxlib中，并在hive-site配置文件中配置需要导入的jar包：

```xml
<property>
  <name>hive.aux.jars.path</name>
  <value>file:///home/zhongtai/opt/apache-hive-2.3.8-bin/auxlib/hudi-hadoop-mr-bundle-0.8.1-SNAPSHOT.jar,file:///home/zhongtai/opt/apache-hive-2.3.8-bin/auxlib/alluxio-2.1.2-client.jar</value>
</property>
```

​		在bigdata01上启动hive元数据：

```shell
nohup hive --service metastore>metastore.log 2>&1 &
```

​		查看该进程是否存活：

```shell
ps -ef | grep java | grep metastore
```

​		在bigdata01上启动hiveserver2服务：

```shell
nohup hive --service hiveserver2>hiveserver2.log 2>&1 &
```

​		查看该进程是否存活：

```shell
ps -ef | grep java | grep hiveserver2
```

​		创建一个数据文件为data.txt，内容如下：

```tex
1,tom
2,jack
```

​		上传至alluxio：

```shell
alluxio fs mkdir /hive/test/person/
alluxio fs copyFromLocal data.txt /hive/test/person/
```

​		通过hive命令进入终端。

```sql
# 创建数据库
create database test;

use test;

# 创建数据表
create table person(id int, name string) row format delimited fields terminated by ',' location 'alluxio://bigdata01:19998/hive/test/person/';

# 查询数据
select * from person;
```

​		能正常查询数据表示集成成功，在查询的时候会将数据加载到内存当中。



### 3.集成presto

​		将$ALLUXIO_HOME/client路径中的alluxio-2.1.2-client.jar拷贝到所有presto节点的$PRESTO_HOME/plugin/hive-hadoop2目录中，并且重启所有coordinator和worker节点。



4.集成测试数据湖

​	这里采用Hudi作为数据湖，我们准备了一份2000w数据集存放在HDFS下/hudi/rdbms/default/etc_project/ods_etc_project_etcobubusiness/clean_tb目录下。然后分别进行测试。

​	①用Spark读Hudi数据，并且Hudi数据目录指向HDFS。

```shell
/home/zhongtai/opt/spark-3.1.1-bin-hadoop2.7/bin/spark-shell \
--driver-class-path /home/zhongtai/opt/apache-hive-2.3.8-bin/conf/ \
--master spark://bigdata01:7077 \
--executor-memory 4g \
--driver-memory 4g \
--num-executors 4 \
--total-executor-cores 8 \
--name TEST \
--jars /home/zhongtai/opt/spark-3.1.1-bin-hadoop2.7/jars/hudi-spark3-bundle_2.12-0.9.0.jar,/home/zhongtai/opt/spark-3.1.1-bin-hadoop2.7/jars/spark-avro_2.12-3.1.1.jar \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
--conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=CORRECTED \
--conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=CORRECTED \
--conf spark.sql.legacy.avro.datetimeRebaseModeInWrite=CORRECTED \
--conf spark.sql.legacy.avro.datetimeRebaseModeInRead=CORRECTED \
--conf spark.sql.legacy.parquet.int96RebaseModeInWrite=CORRECTED \
--conf spark.sql.hive.convertMetastoreParquet=false
```

```scala
spark.read.format("hudi").load("/hudi/rdbms/default/etc_project/ods_etc_project_etcobubusiness/clean_tb").show
```

​		安装自研的hudi同步hive集成工具，解压hiveSync-1.0-SNAPSHOT.tar.gz。

```shell
tar -zxvf hiveSync-1.1-SNAPSHOT.tar.gz
```

​		配置环境变量：

```sh
export HIVE_SYNC_HOME=/home/zhongtai/opt/hiveSync-1.1-SNAPSHOT/hiveSync
export PATH=$PATH:$HIVE_SYNC_HOME/bin
```

​		将数据集元数据同步到Hive中：

```shell
hiveSync --base-path hdfs://bigdata01:9000/hudi/rdbms/default/etc_project/ods_etc_project_etcobubusiness/clean_tb --table-name ods_etc_project_etcobubusiness --table-type 1 --record-key id --precombine-key id
```

​		然后通过Spark去查Hudi表：

```scala
val start = System.currentTimeMillis
spark.sql("select count(1) from data_import.ods_etc_project_etcobubusiness").show
println(System.currentTimeMillis - start)
```

​	②用Spark读Hudi数据，并且Hudi数据目录指向Alluxio。

​		将hdfs路径挂载到alluxio：

```shell
alluxio fs mount /hudi hdfs://bigdata01:9000/hudi
```

​		将hive表中location从hdfs://bigdata01:9000/hudi/rdbms/default/etc_project/ods_etc_project_etcobubusiness/clean_tb修改为alluxio://bigdata01:19998/hudi/rdbms/default/etc_project/ods_etc_project_etcobubusiness/clean_tb。

```sql
alter table ods_etc_project_etcobubusiness 
  set location 'alluxio://bigdata01:19998/hudi/rdbms/default/etc_project/ods_etc_project_etcobubusiness/clean_tb';
  
alter table ods_etc_project_etcobubusiness 
  set serdeproperties( 'path' = 'alluxio://bigdata01:19998/hudi/rdbms/default/etc_project/ods_etc_project_etcobubusiness/clean_tb');
```

​		然后按照步骤测试方案①去进行测试读表数据，注意第一个查询Alluxio会比较慢，因为数据没有缓存到Alluxio，注意多查几次，比对他们的执行时间

### 4.Presto + Alluxio加速湖数据摄取

​		这里主要侧重于Presto，步骤如下：

​	①步骤1：检查每台服务器Presto进程是否存在。

```shell
/home/zhongtai/opt/prestoGuo/presto-server-0.260.2-SNAPSHOT/bin/launcher status
```

​	②步骤2：如果没有开启，在每台服务器上启动Presto进程。

```shell
/home/zhongtai/opt/prestoGuo/presto-server-0.260.2-SNAPSHOT/bin/launcher start
```

​	③步骤3：进入Presto命令行终端。

```shell
/home/zhongtai/opt/prestoGuo/presto-server-0.260.2-SNAPSHOT/bin/presto-cli --server bigdata01:8088 --catalog hive
```

​	④步骤4：输入查询语句进行测试。

​	⑤步骤5：获取查询结果返回的query_id，通过http访问获取每次查询的指标信息。

```shell
curl http://bigdata01:8088/v1/query/$query_id | jq '.queryStats.executionTime'
```

​		我们通过多次查询比对读取2000w的湖数据。大致查询的时间成本如下：

|        | HDFS   | Alluxio |
| ------ | ------ | ------- |
| Spark  | 1082ms | 528ms   |
| Presto | 321ms  | 227ms   |



## 五.常见问题

（1）部署时，执行命令：alluxio-mount.sh Mount workers或者alluxio-mount.sh SudoMount workers失败。可能会出现如下异常：

```shell
[2021-12-01 17:15:48][bigdata02] nohup: ignoring input
[2021-12-01 17:15:48][bigdata03] nohup: ignoring input
[2021-12-01 17:15:50][bigdata02] Unmounting /mnt/ramdisk
[2021-12-01 17:15:50][bigdata02] umount: only root can use "--force" option
[2021-12-01 17:15:50][bigdata02] ERROR: umount RamFS /mnt/ramdisk failed
[2021-12-01 17:15:50][bigdata02] Connection to bigdata02 closed.
[2021-12-01 17:15:51][bigdata03] Unmounting /mnt/ramdisk
[2021-12-01 17:15:51][bigdata03] umount: only root can use "--force" option
[2021-12-01 17:15:51][bigdata03] ERROR: umount RamFS /mnt/ramdisk failed
[2021-12-01 17:15:51][bigdata03] Connection to bigdata03 closed.
```

​	可能的原因有：

​		①对于/mnt/ramdisk目前当前用户没有挂载权限。

​

​		②/mnt/ramdisk因为某些原因（存储不足）需要更换其他路径。



​		解决问题①：执行如下命令：

```shell
mount -t ramfs -o size=10240m ramfs /mnt/ramdisk
```

​		挂载指定存储大小可以通过size参数指定，这里挂载10G，然后这个目录需要针对当前用户富裕读写权限。



​		解决问题②：默认是挂载/mnt/ramdisk目录，如果用户想自定义挂载目录，例如：/ramdisk。同样的需要执行①中的命令：

```shell
mount -t ramfs -o size=10240m ramfs /ramdisk
```

​		然后这个目录需要针对当前用户富裕读写权限。然后还需要修改alluxio-site.properties配置文件。配置如下：

```properties
# alluxio.worker.tieredstore.levels=1
alluxio.worker.tieredstore.level0.alias=MEM
alluxio.worker.tieredstore.level0.dirs.path=/ramdisk
```



（2）Alluxio元数据可以更换存储吗？

​		alluxio会将大部分元数据存储在主节点上，提供了两种存储元数据的方式，分别为ROCKS和HEAP，HEAP是默认的元数据存储方式。



​		ROCKS metastore混合使用内存的磁盘。元数据最终会写入磁盘，但内存中大型缓存会存储最近访问的元数据（LRU）。缓存会在缓冲区中写入，并在缓存填满时异步将其刷洗到RocksDB中。默认的高速缓存大小为1000万个inode，即大约10GB内存。需要alluxio-site.properties文件中进入如下配置：

```properties
# 元数据存储模式，大约32G的master内存可以支持大约4000万个文件
alluxio.master.metastore=ROCKS
# RocksDB写入数据的本地目录
alluxio.master.metastore.dir=/data/alluxio/rocks
# 对inode缓存大小的硬性限制，默认值10000000，每百万个inode大约需要1GB内存
# alluxio.master.metastore.inode.cache.max.size=
# 用于将缓存修改刷新到RocksDB的批处理大小，默认为1000
#alluxio.master.metastore.inode.cache.evict.batch.size=
# 开始移除缓存的最大缓存大小比率，默认为0.85
# alluxio.master.metastore.inode.cache.high.water.mark.ratio=
# 移出的最大告诉缓存大小的比率，默认为0.8
# alluxio.master.metastore.inode.cache.low.water.mark.ratio=
```

​	【注意】切换MetaStore类型需要格式化alluxio日志，移除所有元数据，如果要在切换之后将保存现有数据，则需要执行日志备份和还原。具体步骤如下：

​	①备份日志，会输出日志的存储路径${BACKUP_PATH}。

```shell
alluxio fsadmin backup
```

​		【注意】这里命令返回需要记录返回的hdfs的路径，在步骤⑤中会用到。

​	②停止masters。

```shell
bin/alluxio-stop.sh master
```

​	③修改配置文件中对于存储模式的选项。

​	④格式化masters。

```shell
alluxio formatMaster
```

​	⑤指定步骤①中的路径，重启masters。

```shell
bin/alluxio-start.sh -i ${BACKUP_PATH} master
```



（3）为什么会出现UFS和Alluxio数据不同步呢？

​		这关系到数据同步策略，这里需要用到这个命令：

```shell
alluxio fs ls -R -Dalluxio.user.file.metadata.sync.interval=0
```

​		0：不从UFS获取元数据。



​		1：每次都通过UDF获取元数据。

​

​		【注意】如果这里配置为1，则最后一次同步操作发生在1分钟前。但是如果操作意味强制所有目录上进行递归的同步，会耗费很长时间。



​		当然alluxio支持UFS主动向alluxio同步，alluxio会注册一个监听器，监听HDFS上的任何改动，当HDFS发生改动时，alluxio会进行同步，操作如下：

```shell
alluxio fs startSync /syncedDir
```

​		除了上面的方法，还可以通过参数alluxio.user.file.metadata.load.type来控制。但是这种只会发现新文件，不会重新加载修改或删除的文件，他适用于exists，list和getstatus的RPC调用，参数选项如下：



​		①**ALWAYS**：将始终检查UFS中是否有新的文件。



​		②**ONCE**：将使用默认的行为，即仅仅扫描每个目录一次。



​		③**NEVER**：将阻止alluxio扫描新文件。

​

​	 	或者可以使用这个命令：

```shell
alluxio fs ls -f /path 
```

​		【注意】ls的-f选项的作用将与alluxio.user.file.metadata.load.type设置为ALWAYS相同。他会发现新文件，但不会检测修改或者删除的UFS文件。