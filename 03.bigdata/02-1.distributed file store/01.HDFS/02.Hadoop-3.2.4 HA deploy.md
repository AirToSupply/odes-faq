## 一.节点规划

|                       | NameNode | DataNode | JournalNode | DFSZKFailoverController |
| --------------------- | -------- | -------- | ----------- | ----------------------- |
| 172.16.2.120（2-120） | √        | √        | √           |                         |
| 172.16.2.123（2-123） | √        | √        | √           | √                       |
| 10.232.2.124（2-124） |          | √        | √           |                         |



## 二.安装步骤

###    （1）解压安装包

```shell
cd /workspace/opt
tar -zxvf /tmp/modules/hadoop-3.2.4.tar.gz -C .
cd hadoop-3.2.4
```



### （2）配置Hadoop环境变量

```shell
vim ~/.bashrc
```

​     配置内容如下：

```shell
export HADOOP_HOME=/workspace/opt/hadoop-3.2.4
export HADOOP_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
```

​     生效配置：

```shell
source ~/.bashrc
```



### （3）配置core-site.xml文件

​                在${HADOOP_HOME}/etc/hadoop路径下配置core-site.xml文件，配置项如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <!-- 如果是HA模式则指定hdfs的nameservice为ns 如果是非HA模式，则为hdfs://<host>:<ip>-->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://ns</value>
  </property>

  <!--指定hadoop数据存放目录-->
  <property>
     <name>hadoop.tmp.dir</name>
     <value>/data/hadoop-3.2.4/tmp</value>
  </property>

  <!-- Hadoop的缓冲区大小。默认为4KB，增加为128KB。 -->
  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
  </property>

  <!-- 和系统内核中的backlog相关参数联动，但要小于net.core.somaxconn -->
  <property>
    <name>ipc.server.listen.queue.size</name>
    <value>20480</value>
  </property>

  <property>
    <name>hadoop.proxyuser.zhongtai.hosts</name>
    <value>*</value>
  </property>
  
  <property>
    <name>hadoop.proxyuser.zhongtai.groups</name>
    <value>*</value>
  </property>
    
  <!--HA模式配置-->  
  <!--指定zookeeper地址-->
  <property>
        <name>ha.zookeeper.quorum</name>
        <value>2-120:2181,2-123:2181,2-124:2181</value>
  </property>
    
      <!-- 
        检查点被删除后的分钟数。如果为零，垃圾桶功能将被禁用。该选项可以在服务器和客户端上配置。[单位(minute)]
        如果垃圾箱被禁用服务器端，则检查客户端配置。
        如果在服务器端启用垃圾箱，则会使用服务器上配置的值，并忽略客户端配置值。 
        https://www.jianshu.com/p/1827431d4eb5
      -->
    <property>
      <name>fs.trash.interval</name>
      <value>4320</value>
    </property>

<!-- 
    垃圾检查点之间的分钟数。应该小于或等于fs.trash.interval。[单位(minute)]
    如果为零，则将该值设置为fs.trash.interval的值。每次检查指针运行时，它都会从当前创建一个新的检查点，并删除比fs.trash.interval更早创建的检查点。
    注意：通过回收站恢复误删的数据，要求时间不能超过fs.trash.interval配置的时间。生产中为了防止误删数据，建议开启HDFS的回收站机制。
  -->
    <property>
      <name>fs.trash.checkpoint.interval</name>
      <value>60</value>
    </property>

</configuration>
```



### （4）配置hdfs-site.xml文件

​         在${HADOOP_HOME}/etc/hadoop路径下配置hdfs-site.xml文件，配置项如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <!--HA模式配置-->  
  <!--指定hdfs的nameservice为ns，需要和core-site.xml中的保持一致 -->
  <property>
     <name>dfs.nameservices</name>
     <value>ns</value>
  </property>

  <!-- NameNode 同时和 DataNode 通信的线程数，默认为10，将其增大为40。-->
  <property>
    <name>dfs.namenode.handler.count</name>
    <value>40</value>
  </property>

  <!-- 当 DataNode上面的连接数超过配置中的设置时， DataNode就会拒绝连接。-->
  <property>
    <name>dfs.datanode.max.xcievers</name>
    <value>65536</value>
  </property>

  <!-- 执行 start-balancer.sh 的带宽，默认为1048576(1MB/s)，将其増大到20MB/s -->
  <property>
    <name>dfs.datanode.balance.bandwidthPerSe</name>
    <value>20485760</value>
  </property>
    
  <!-- 
     DataNode 在进行文件传输时最大线程数，最高设置为8192，如果集群中有某台 DataNode 主机的这个值比其他主机的大，
     那么出现的问题是，这台主机上存储的数据相对别的主机比较多，导致数据分布不均匀的问题，即使 balance 仍然会不均匀。 
   -->
  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>8192</value>
  </property>
    
  <!--HA模式配置-->    
  <!-- ns下面有两个NameNode，分别是nn1，nn2 -->
  <property>
     <name>dfs.ha.namenodes.ns</name>
     <value>nn1,nn2</value>
  </property>
    
  <!--HA模式配置-->  
  <!-- nn1的RPC通信地址 -->
  <property>
     <name>dfs.namenode.rpc-address.ns.nn1</name>
     <value>2-120:9000</value>
  </property>
   
  <!--HA模式配置-->    
  <!-- nn1的http通信地址 -->
  <property>
     <name>dfs.namenode.http-address.ns.nn1</name>
     <value>2-120:50070</value>
  </property>
  <!--HA模式配置-->  
  <!-- nn2的RPC通信地址 -->
  <property>
     <name>dfs.namenode.rpc-address.ns.nn2</name>
     <value>2-123:9000</value>
  </property>
  <!--HA模式配置-->  
  <!-- 非HA模式配置如下 -->
  <!--
  <property>
    <name>dfs.namenode.http-address</name>
    <value><hostname>:50070</value>
  </property>
  -->
    
  <!-- nn2的http通信地址 -->
  <property>
     <name>dfs.namenode.http-address.ns.nn2</name>
     <value>2-123:50070</value>
  </property>
  <!--HA模式配置-->  
  <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->
  <property>
     <name>dfs.namenode.shared.edits.dir</name>
     <value>qjournal://2-120:8485;2-123:8485;2-124:8485/ns</value>
  </property>
  <!--HA模式配置-->  
  <!-- 指定JournalNode在本地磁盘存放数据的位置 -->
  <property>
     <name>dfs.journalnode.edits.dir</name>
     <value>/data/hadoop-3.2.4/journal</value>
  </property>

  <!-- 指定NameNode在本地磁盘存放数据的位置 -->
  <property>
     <name>dfs.namenode.name.dir</name>
     <value>file:///data/hadoop-3.2.4/hdfs/name</value>
  </property>
    
  <!-- 指定DataNode在本地磁盘存放数据的位置 -->
  <property>
     <name>dfs.datanode.data.dir</name>
     <value>file:///data/hadoop-3.2.4/hdfs/data</value>
  </property>

  <property>
    <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
    <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
  </property>
  <!--HA模式配置-->  
  <!-- 开启NameNode故障时自动切换 -->
  <property>
     <name>dfs.ha.automatic-failover.enabled</name>
     <value>true</value>
  </property>
  <!--HA模式配置-->  
  <!-- 配置失败自动切换实现方式 -->
  <property>
     <name>dfs.client.failover.proxy.provider.ns</name>
     <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
    
  <property>
     <name>dfs.permissions.enabled</name>
     <value>false</value>
  </property>

  <!-- 注意：如果是伪分布式只能配置为1 -->
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>

  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>
      sshfence
      shell(/bin/true)
    </value>
  </property>
</configuration>
```



### （5）配置hadoop-env.sh文件。

​         在${HADOOP_HOME}/etc/hadoop路径下配置hadoop-env.sh文件，配置项如下：

```sh
export JAVA_HOME=/workspace/opt/jdk1.8.0_201

# hadoop改log、pid、jvm的配置
export HADOOP_PID_DIR=/data/hadoop-3.2.4/pid
export HADOOP_MAPRED_PID_DIR=/data/hadoop-3.2.4/pid
export HADOOP_LOG_DIR=/data/hadoop-3.2.4/log

# JVM
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx4096m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx4096m"
export HDFS_JOURNALNODE_OPTS="-Xmx4096m"

```



### （6）配置mapred-site.xml文件。

​         在${HADOOP_HOME}/etc/hadoop路径下配置mapred-site.xml文件，配置项如下：

```xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```



### （7）配置yarn-site.xml文件

​         在${HADOOP_HOME}/etc/hadoop路径下配置yarn-site.xml文件，配置项如下：

```xml
<configuration>
  <property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
  </property>
  <!--HA模式配置-->
  <!-- 指定zk集群地址 -->
  <property>
	 <name>yarn.resourcemanager.zk-address</name>
	 <value>2-120:2181,2-123:2181,2-124:2181</value>
  </property>

  <!--HA模式配置-->
  <!-- 开启RM高可用 -->
  <property>
     <name>yarn.resourcemanager.ha.enabled</name>
     <value>true</value>
  </property>
  <!--HA模式配置-->
  <!-- 指定RM的cluster id -->
  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>yrc</value>
  </property>
  <!--HA模式配置-->
  <!-- 指定RM的名字 -->
  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
	<value>rm1,rm2</value>
  </property>
  <!--HA模式配置-->
  <!-- 分别指定RM的地址 -->
  <property>
	<name>yarn.resourcemanager.hostname.rm1</name>
	<value>2-120</value>
  </property>
  <!--HA模式配置-->
  <property>
	<name>yarn.resourcemanager.hostname.rm2</name>
	<value>2-123</value>
  </property>
  <!-- 如果非HA 需要如下指定 -->
  <!--
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value><hostname></value>
  </property>
  -->

</configuration>
```



### （8）hadoop改pid、jvm的配置

​    在${HADOOP_HOME}/etc/hadoop路径下配置 yarn-env.sh 文件，配置项如下：

```shell
export YARN_PID_DIR=/data/hadoop-3.2.4/pid

# JVM
export YARN_RESOURCEMANAGER_OPTS="-Drm.audit.logger=INFO,RMAUDIT -Xmx4096m"
export YARN_NODEMANAGER_OPTS="-Dnm.audit.logger=INFO,NMAUDIT -Xmx4096m"
```



​    在${HADOOP_HOME}/etc/hadoop路径下配置 mapred-env.sh 文件，配置项如下：

```shell
export HADOOP_MAPRED_PID_DIR=/data/hadoop-3.2.4/pid
# JVM
export MAPRED_HISTORYSERVER_OPTS="-Xmx4096m"

```



### （9）配置DataNode节点列表

在${HADOOP_HOME}/etc/hadoop/slaves 文件中配置DataNode节点列表，内容如下：

```shell
2-120
2-123
2-124
```



### （10）将Hadoop跟目录分发至各服务器节点

```shell
scp -r /workspace/opt/hadoop-3.2.4  zhongtai@2-123:/workspace/opt/
scp -r /workspace/opt/hadoop-3.2.4  zhongtai@2-124:/workspace/opt/
scp ~/.bashrc zhongtai@2-123:~/
scp ~/.bashrc zhongtai@2-124:~/
```



### （11）启动服务

Hadoop HA模式在首次启动时要保证zk是启动状态，然后一定需要注意顺序，主要步骤如下：

①在某一个namenode节点执行如下命令，创建命名空间，比如这里我们对 2-120 节点进行如下操作。

```shell
hdfs zkfc -formatZK
```

②在每个journalnode节点用如下命令启动journalnode（进程名为JournalNode），我们在 2-120,2-123,2-124 这三个节点进行如下操作。

```shell
${HADOOP_HOME}/sbin/hadoop-daemon.sh start journalnode
```

③在主namenode节点格式化namenode和journalnode目录，这里我们对2-120做操作。

```shell
hdfs namenode -format ns
```

④在主namenode节点启动namenode进程，这里对2-120做操作。

```shell
${HADOOP_HOME}/sbin/hadoop-daemon.sh start namenode
```

⑤在备namenode节点执行如下命令，在bigdata01做操作。

```shell
# 把备namenode节点的目录格式化并把元数据从主namenode节点copy过来，并且这个命令不会把journalnode目录再格式化了！
hdfs namenode -bootstrapStandby
# 启动备namenode进程
${HADOOP_HOME}/sbin/hadoop-daemon.sh start namenode
```

⑥在两个namenode节点都执行以下命令，启动zkfc（进程名为DFSZKFailoverController），在bigdata01,2-120两个节点做操作。

```shell
${HADOOP_HOME}/sbin/hadoop-daemon.sh start zkfc
```

⑦在所有datanode节点都执行以下命令启动datanode。

```shell
${HADOOP_HOME}/sbin/hadoop-daemon.sh start datanode
```

⑧开启数据均衡(非必须执行)。

```shell
${HADOOP_HOME}/sbin/start-balancer.sh –threshold 10
```

这里的10代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

​       ⑨配置完HA之后可以下查看下两台NameNode的状态是否正常，命令如下：

```shell
hdfs haadmin -getServiceState nn1
hdfs haadmin -getServiceState nn2
```

⑩检查各服务器节点进程是否启动。

​         如果是非HA模式下，只需要执行如下命令：

```shell
hdfs namenode -format
```



## 三.运维命令

（1）启停存储管理进程。

```shell
${HADOOP_HOME}/sbin/hadoop-daemon.sh stop datanode
${HADOOP_HOME}/sbin/hadoop-daemon.sh start datanode
```

（2）启停元数据管理进程。

```shell
${HADOOP_HOME}/sbin/hadoop-daemon.sh stop namenode
${HADOOP_HOME}/sbin/hadoop-daemon.sh start namenode
```

（3）启停HDFS集群。

```shell
${HADOOP_HOME}/sbin/stop-dfs.sh
${HADOOP_HOME}/sbin/start-dfs.sh
```

（4）查看HDFS HA NameNode状态。

```shell
hdfs haadmin -getServiceState nn1
hdfs haadmin -getServiceState nn2
```

（5）查看NameNode日志。

```shell
tail -1000f ${HADOOP_HOME}/logs/hadoop-xxx-namenode-xxx.log
```

（6）查看DataNode日志。

```shell
tail -1000f ${HADOOP_HOME}/logs/hadoop-xxx-datanode-xxx.log
```



## 四.问题指南

（1）通过HDFS WebUI浏览HDFS目录树过程中不可访问。

​        此时可能由于某些故障，导致Active NameNode进程退出，如果是HA模式则会发生NameNode切换不影响用户使用，如果是单节点模式需要查看具体错误堆栈。

（2）Namenode设置了HA，但故障时未成功切换。

​        该问题现象是Active NameNode内存故障，主备切换失败。主要原因是dfs.ha.fencing.methods设置为了ssh，但是并不能登录其他namenode执行fence。

​        此时需要生成ssh key免密码登录。或者改成shell(/bin/true)。但是修改fence方式后，需要重启zkfc进程。

（3）【警告】Unable to load native-hadoop library for your platform。

​          上述问题表示系统里没有找到原生的hadoop库libhdfs.so，这个库是C写的，性能比较好。缺少但不影响使用，因为hadoop里有Java实现的客户端库。大多数是因为hadoop安装包里没有自带libhdfs.so。

​           此时去到目录${HADOOP_HOME}/lib/native/看下是否有libhdfs.so,libhdfs.a,libhadoop.so,libhadoop.a。如果没有的话，可以重新下载一个完整的二进制包，把lib/native拷出来用。然后通过如下命令进行检查：

```shell
[zhongtai@bigdata01 bin]$ hadoop checknative
Native library checking:
hadoop:  true /workspace/opt/hadoop-3.2.4/lib/native/libhadoop.so
zlib:    true /lib64/libz.so.1
snappy:  true /lib64/libsnappy.so.1
zstd  :  false
lz4:     true revision:10301
bzip2:   true /lib64/libbz2.so.1
openssl: true /lib64/libcrypto.so
```

​     其他更多的异常问题可以参考：https://zhuanlan.zhihu.com/p/170478965

（4）HDFS回收站功能

​     参考博客：https://www.jianshu.com/p/1827431d4eb5

（5）Hadoop参数优化配置

​            参考博客：http://t.zoukankan.com/peizhe123-p-5540845.html

（6）如何增加NamaNode和DataNode JVM进程内存

​            在hadoop-env.sh配置文件文件中进行配置，

```shell
# Command specific options appended to HADOOP_OPTS when specified
export HADOOP_NAMENODE_OPTS="-Xmx2g -Xms2g -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
export HADOOP_DATANODE_OPTS="-Xmx2g -Xms2g -Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS"
```

（7）hadoop如何支持snappy压缩？

可以通过如下命令来检查hadoop环境支持哪些压缩器：

```shell
hadoop checknative
```

查看snappy选项是否为false，表示当前服务器环境不支持snappy，需要安装依赖库。有时候报错会有如下问题：

```shell
2023-01-30 16:02:43,798 INFO bzip2.Bzip2Factory: Successfully loaded & initialized native-bzip2 library system-native
2023-01-30 16:02:43,801 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2023-01-30 16:02:43,807 ERROR snappy.SnappyCompressor: failed to load SnappyCompressor
java.lang.UnsatisfiedLinkError: Cannot load libsnappy.so.1 (libsnappy.so.1: 无法打开共享对象文件: 没有那个文件或目录)!
        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.initIDs(Native Method)
        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.<clinit>(SnappyCompressor.java:57)
        at org.apache.hadoop.io.compress.SnappyCodec.isNativeCodeLoaded(SnappyCodec.java:82)
        at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:103)
```

则需要安装snappy依赖，如果是Centos系统执行如下命令：

```shell
sudo yum -y install snappy.x86_64
```

如果是Unbutun系统执行如下命令：

```shell
sudo apt-get install -y libsnappy-dev
```

