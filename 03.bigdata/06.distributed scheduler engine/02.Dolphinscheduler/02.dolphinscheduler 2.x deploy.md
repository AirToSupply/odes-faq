## **Dolphinscheduler环境搭建**

### 一.节点规划

|                      | 172.16.10.72（bigdata02） | 172.16.10.205（bigdata03） | 172.16.10.206（bigdata04） |
| :------------------: | :-----------------------: | :------------------------: | :------------------------: |
| ApiApplicationServer |                           |                            |             √              |
|     MasterServer     |                           |                            |             √              |
|     AlertServer      |                           |                            |             √              |
|     WorkerServer     |             √             |             √              |             √              |
|     LoggerServer     |             √             |             √              |             √              |

### 二.安装部署

DS集群搭建方式可以参照如下链接：https://www.apache.org/dyn/closer.lua/dolphinscheduler/2.0.3/apache-dolphinscheduler-2.0.3-bin.tar.gz

前期准备工作

- JDK：下载[JDK](https://www.oracle.com/technetwork/java/javase/downloads/index.html) (1.8+)，并将 JAVA_HOME 配置到以及 PATH 变量中。如果你的环境中已存在，可以跳过这步。
- 二进制包：在[下载页面](https://dolphinscheduler.apache.org/zh-cn/download/download.html)下载 DolphinScheduler 二进制包
- 数据库：[PostgreSQL](https://www.postgresql.org/download/) (8.2.15+) 或者 [MySQL](https://dev.mysql.com/downloads/mysql/) (5.7+)，两者任选其一即可，如 MySQL 则需要 JDBC Driver 8.0.16
- 注册中心：[ZooKeeper](https://zookeeper.apache.org/releases.html) (3.4.6+)，[下载地址](https://zookeeper.apache.org/releases.html)
- ***注意:*** DolphinScheduler 本身不依赖 Hadoop、Hive、Spark，但如果你运行的任务需要依赖他们，就需要有对应的环境支持

（1）解压安装包

```
cd /workspace/opt
tar -zxvf /tmp/apache-dolphinscheduler-1.3.6-bin.tar.gz -C .
```

（2）在所有部署调度的机器上创建部署用户，并且一定要配置 sudo 免密，这里部署用户为zhongtai，由于该用户之前已经创建，所以创建用户步骤忽略。

```
echo 'zhongtai  ALL=(ALL)  NOPASSWD: NOPASSWD: ALL' >> /etc/sudoers
sed -i 's/Defaults    requirett/#Defaults    requirett/g' /etc/sudoers
```

（3）配置hosts映射和ssh打通及修改目录权限。

   (4)  修改一键部署配置文件 conf/config/install_config.conf 中的各参数，特别注意以下参数的配置：

```
vim conf/config/install_config.conf
```

```
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# ---------------------------------------------------------
# INSTALL MACHINE
# ---------------------------------------------------------
# A comma separated list of machine hostname or IP would be installed DolphinScheduler,
# including master, worker, api, alert. If you want to deploy in pseudo-distributed
# mode, just write a pseudo-distributed hostname
# Example for hostnames: ips="ds1,ds2,ds3,ds4,ds5", Example for IPs:ips="192.168.8.1,192.168.8.2,192.168.8.3,192.168.8.4,192.168.8.5"

# 在哪些机器上部署 DS 服务，本机选 localhost
ips="bigdata02,bigdata03,bigdata04"

# Port of SSH protocol, default value is 22. For now we only support same port in all `ips` machine
# modify it if you use different ssh port

# ssh 端口，默认22
sshPort="22"

# A comma separated list of machine hostname or IP would be installed Master server, it
# must be a subset of configuration `ips`.
# Example for hostnames: masters="ds1,ds2", Example for IPs:masters="192.168.8.1,192.168.8.2"

# master 服务部署在哪台机器上
masters="bigdata04"

# A comma separated list of machine <hostname>:<workerGroup> or <IP>:<workerGroup>.All hostname or IP must be a
# subset of configuration `ips`, And workerGroup have default value as `default`, but we recommend you declare behind the hosts
# Example for hostnames: workers="ds1:default,ds2:default,ds3:default", Example for IPs: workers="192.168.8.1:default,192.168.8.2:default,192.168.8.3:default"

# worker 服务部署在哪台机器上，并指定此 worker 属于哪一个 worker 组，下面示例的 default 即为组名
workers="bigdata02:default,bigdata03:default,bigdata04:default"

# A comma separated list of machine hostname or IP would be installed Alert server, it
# must be a subset of configuration `ips`.
# Example for hostname: alertServer="ds3", Example for IP: alertServer="192.168.8.3"

# 报警服务部署在哪台机器上
alertServer="bigdata04"

# A comma separated list of machine hostname or IP would be installed API server, it
# must be a subset of configuration `ips`.
# Example for hostname: apiServers="ds1", Example for IP: apiServers="192.168.8.1"

# 后端 api 服务部署在在哪台机器上
apiServers="bigdata04"

# A comma separated list of machine hostname or IP would be installed Python gateway server, it
# must be a subset of configuration `ips`.
# Example for hostname: pythonGatewayServers="ds1", Example for IP: pythonGatewayServers="192.168.8.1"

# pythonGatewayServers 服务器部署在哪台机器上
pythonGatewayServers="bigdata04"

# The directory to install DolphinScheduler for all machine we config above. It will automatically be created by `install.sh` script if not exists.
# Do not set this configuration same as the current path (pwd)

# dolphinscheduler的安装路径
installPath="/home/workspace/apache-dolphinscheduler-2.0.3-bin-install"

# The user to deploy DolphinScheduler for all machine we config above. For now user must create by yourself before running `install.sh`
# script. The user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled than the root directory needs
# to be created by this user

# 使用哪个用户部署
deployUser="zhongtai"

# The directory to store local data for all machine we config above. Make sure user `deployUser` have permissions to read and write this directory.

# 用于存储本地数据的目录。确保用户"/data/dolphinscheduler-2.0.3"具有读取和写入此目录的权限
dataBasedirPath="/data/dolphinscheduler-2.0.3"

# ---------------------------------------------------------
# DolphinScheduler ENV
# ---------------------------------------------------------
# JAVA_HOME, we recommend use same JAVA_HOME in all machine you going to install DolphinScheduler
# and this configuration only support one parameter so far.

#  JAVA_HOME 的路径，安装的JDK中 JAVA_HOME 所在的位置
javaHome="/home/workspace/jdk1.8.0_201"

# DolphinScheduler API service port, also this is your DolphinScheduler UI component's URL port, default value is 12345

# apiServerPort默认端口 12345
apiServerPort="12345"

# ---------------------------------------------------------
# Database
# NOTICE: If database value has special characters, such as `.*[]^${}\+?|()@#&`, Please add prefix `\` for escaping.
# ---------------------------------------------------------
# The type for the metadata database
# Supported values: ``postgresql``, ``mysql`, `h2``.
#DATABASE_TYPE=${DATABASE_TYPE:-"h2"}

# 选择使用的数据库
DATABASE_TYPE="mysql"

# Spring datasource url, following <HOST>:<PORT>/<database>?<parameter> format, If you using mysql, you could use jdbc
# string jdbc:mysql://127.0.0.1:3306/dolphinscheduler?useUnicode=true&characterEncoding=UTF-8 as example
#SPRING_DATASOURCE_URL=${SPRING_DATASOURCE_URL:-"jdbc:h2:mem:dolphinscheduler;MODE=MySQL;DB_CLOSE_DELAY=-1;DATABASE_TO_LOWER=true"}

# 数据库的相关配置
SPRING_DATASOURCE_URL="jdbc:mysql://172.16.10.72:3306/dolphinscheduler?useUnicode=true&characterEncoding=UTF-8"
# Spring datasource username
#SPRING_DATASOURCE_USERNAME=${SPRING_DATASOURCE_USERNAME:-"sa"}
SPRING_DATASOURCE_USERNAME="root"
# Spring datasource password
#SPRING_DATASOURCE_PASSWORD=${SPRING_DATASOURCE_PASSWORD:-""}
SPRING_DATASOURCE_PASSWORD="123456"

# ---------------------------------------------------------
# Registry Server
# ---------------------------------------------------------
# Registry Server plugin name, should be a substring of `registryPluginDir`, DolphinScheduler use this for verifying configuration consistency

# 注册表服务器插件名称
registryPluginName="zookeeper"

# Registry Server address.

# 注册中心地址，zookeeper服务的地址
registryServers="172.16.10.72:2181,172.16.10.205:2181,172.16.10.206:2181"

# Registry Namespace

# 注册表命名空间
registryNamespace="dolphinscheduler"

# ---------------------------------------------------------
# Worker Task Server
# ---------------------------------------------------------
# Worker Task Server plugin dir. DolphinScheduler will find and load the worker task plugin jar package from this dir.

# 将从此目录找到并加载 worker 任务插件 jar 包。
taskPluginDir="lib/plugin/task"

# resource storage type: HDFS, S3, NONE

# 业务用到的比如 sql 等资源文件上传到哪里，可以设置：HDFS,S3,NONE，单机如果想使用本地文件系统，请配置为 HDFS，因为 HDFS 支持本地文件系统；如果不需要资源上传功能请选择 NONE。强调一点：使用本地文件系统不需要部署 hadoop
resourceStorageType="HDFS"

# resource store on HDFS/S3 path, resource file will store to this hdfs path, self configuration, please make sure the directory exists on hdfs and has read write permissions. "/dolphinscheduler" is recommended

# 资源上传根路径，主持 HDFS 和 S3，由于 hdfs支持本地文件系统，需要确保本地文件夹存在且有读写权限
resourceUploadPath="/dolphinscheduler"

# if resourceStorageType is HDFS，defaultFS write namenode address，HA, you need to put core-site.xml and hdfs-site.xml in the conf directory.
# if S3，write S3 address，HA，for example ：s3a://dolphinscheduler，
# Note，S3 be sure to create the root directory /dolphinscheduler

# 如果上传资源保存想保存在 hadoop 上，hadoop 集群的 NameNode 启用了 HA 的话，需要将 hadoop 的配置文件 core-site.xml 和 hdfs-site.xml 放到安装路径的 conf 目录下，并配置 namenode cluster 名称；如果 NameNode 不是 HA，则只需要将 mycluster 修改为具体的 ip 或者主机名即可
# 这里我们采用的是Hadoop HA模式
defaultFS="hdfs://ns"

# if resourceStorageType is S3, the following three configuration is required, otherwise please ignore
s3Endpoint="http://192.168.xx.xx:9010"
s3AccessKey="xxxxxxxxxx"
s3SecretKey="xxxxxxxxxx"

# resourcemanager port, the default value is 8088 if not specified

resourceManagerHttpAddressPort="8088"

# if resourcemanager HA is enabled, please set the HA IPs; if resourcemanager is single node, keep this value empty
# resourcemanager HA部署在哪台机器上(如果没有HA，则为空)
# yarnHaIps=
yarnHaIps="bigdata02,bigdata03"

# if resourcemanager HA is enabled or not use resourcemanager, please keep the default value; If resourcemanager is single node, you only need to replace 'yarnIp1' to actual resourcemanager hostname
# resourcemanager 部署在哪台机器上
singleYarnIp="bigdata03"

# who has permission to create directory under HDFS/S3 root path
# Note: if kerberos is enabled, please config hdfsRootUser=

# 具备权限创建 resourceUploadPath的用户
hdfsRootUser="zhongtai"

# kerberos config
# whether kerberos starts, if kerberos starts, following four items need to config, otherwise please ignore
kerberosStartUp="false"
# kdc krb5 config file path
krb5ConfPath="$installPath/conf/krb5.conf"
# keytab username,watch out the @ sign should followd by \\
keytabUserName="hdfs-mycluster\\@ESZ.COM"
# username keytab path
keytabPath="$installPath/conf/hdfs.headless.keytab"
# kerberos expire time, the unit is hour
kerberosExpireTime="2"

# use sudo or not
sudoEnable="true"

# worker tenant auto create

# 租户是否自动创建
workerTenantAutoCreate="true"

```

（5）数据库初始化。

进入数据库，默认数据库是 PostgreSQL，如选择 MySQL 的话，后续需要添加 mysql-connector-java 驱动包到 DolphinScheduler 的 lib 目录下。这里我们用的mysql-connect依赖是5.1.47版本。

```
cd /workspace/opt/
tar -zxvf mysql-connector-java-5.1.47.tar.gz
cd mysql-connector-java-5.1.47
cp ./mysql-connector-java-5.1.47.jar /workspace/opt/apache-dolphinscheduler-1.3.6-bin/lib/.
```

进入数据库命令行窗口后，执行数据库初始化命令，设置访问账号和密码。注: {user} 和 {password} 需要替换为具体的数据库用户名和密码。

```
mysql> CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
mysql> GRANT ALL PRIVILEGES ON dolphinscheduler.* TO '{user}'@'%' IDENTIFIED BY '{password}';
mysql> GRANT ALL PRIVILEGES ON dolphinscheduler.* TO '{user}'@'localhost' IDENTIFIED BY '{password}';
mysql> flush privileges;
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| dolphinscheduler   |
+--------------------+
2 rows in set (0.00 sec)
```

修改并保存完后，执行 script 目录下的创建表及导入基础数据脚本。

```
./script/create-dolphinscheduler.sh
```

最后显示如下打印表示数据表创建成功。

```
11:08:48.732 [main] INFO org.apache.dolphinscheduler.dao.upgrade.UpgradeDao - sqlSQLFilePath/workspace/opt/apache-dolphinscheduler-1.3.6-bin/sql/upgrade/1.3.6_schema/mysql/dolphinscheduler_dml.sql
11:08:48.737 [main] INFO org.apache.dolphinscheduler.dao.upgrade.shell.CreateDolphinScheduler - upgrade DolphinScheduler finished
11:08:48.737 [main] INFO org.apache.dolphinscheduler.dao.upgrade.shell.CreateDolphinScheduler - create DolphinScheduler success
```

查看数据表：

```
mysql> use dolphinscheduler
Database changed

mysql> show tables;
+--------------------------------+
| Tables_in_dolphinscheduler     |
+--------------------------------+
| QRTZ_BLOB_TRIGGERS             |
| QRTZ_CALENDARS                 |
| QRTZ_CRON_TRIGGERS             |
| QRTZ_FIRED_TRIGGERS            |
| QRTZ_JOB_DETAILS               |
| QRTZ_LOCKS                     |
| QRTZ_PAUSED_TRIGGER_GRPS       |
| QRTZ_SCHEDULER_STATE           |
| QRTZ_SIMPLE_TRIGGERS           |
| QRTZ_SIMPROP_TRIGGERS          |
| QRTZ_TRIGGERS                  |
| t_ds_access_token              |
| t_ds_alert                     |
| ...                            |
| t_ds_worker_group              |
| t_ds_worker_server             |
+--------------------------------+
38 rows in set (0.00 sec)
```

（6）修改运行参数。

修改 conf/env 目录下的 dolphinscheduler_env.sh 环境变量(以相关用到的软件都安装在 /opt/soft 下为例)。

```
vim conf/env/dolphinscheduler_env.sh
```

```
export HADOOP_HOME=/home/workspace/hadoop-2.10.1
export HADOOP_CONF_DIR=/home/workspace/hadoop-2.10.1/etc/hadoop
export SPARK_HOME1=/opt/soft/spark1
export SPARK_HOME2=/home/workspace/spark-3.1.1-bin-hadoop2.7
export PYTHON_HOME=/opt/soft/python
export JAVA_HOME=/home/workspace/jdk1.8.0_201
export HIVE_HOME=/home/workspace/apache-hive-2.3.8-bin
export FLINK_HOME=/home/workspace/flink-1.13.1
export DATAX_HOME=/opt/soft/datax

export PATH=$HADOOP_HOME/bin:$SPARK_HOME1/bin:$SPARK_HOME2/bin:$PYTHON_HOME/bin:$JAVA_HOME/bin:$HIVE_HOME/bin:$FLINK_HOME/bin:$DATAX_HOME/bin:$PATH

```
（7）拷贝hadoop中的hdfs-site.xml和core-site.xml到conf中

```
cp /home/workspace/hadoop-2.10.1/etc/hadoop/core-site.xml /home/workspace/apache-dolphinscheduler-2.0.3-bin/conf
cp /home/workspace/hadoop-2.10.1/etc/hadoop/hdfs-site.xml /home/workspace/apache-dolphinscheduler-2.0.3-bin/conf
```

（8）一键部署

  执行一键部署脚本。

```
./install.sh
```

因为配置了installPath=/workspace/opt/apache-dolphinscheduler-1.3.6-bin-install。所以需要到这个目录下启动5个服务并且这个目录下的logs目录查看日志，检查各个进程是否启动正常。

### 三.运维操作

（1）一键启停集群所有服务。

```
/workspace/opt/apache-dolphinscheduler-1.3.6-bin-install/bin/start-all.sh
/workspace/opt/apache-dolphinscheduler-1.3.6-bin-install/bin/stop-all.sh
```

（2）启停Master。

```
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh start master-server
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh stop master-server
```

（3）启停 Worker。

```
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh start worker-server
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh stop worker-server
```

（4）启停 Api服务。

```
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh start api-server
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh stop api-server
```

（5）启停 Logger服务。

```
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh start logger-server
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh stop logger-server
```

（6）启停 Alert服务。

```
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh start alert-server
${DOLPHINSCHEDULER_HOME}/bin/dolphinscheduler-daemon.sh stop alert-server
```

（7）各服务日志查看。

```
tail -1000f ${DOLPHINSCHEDULER_HOME}/logs/dolphinscheduler-${server_name}.log
```

（8）访问WebUI

  访问如下地址进行登录：http://10.232.3.217:12345/dolphinscheduler。

  默认DS管理员账号密码分别为：admin和dolphinscheduler123