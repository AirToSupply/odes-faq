# dolphinscheduler安装部署(集成juiceFS)

1.修改 `apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下bin/env/install_env.sh 文件

```shell
# limitations under the License.
#

# ---------------------------------------------------------
# INSTALL MACHINE
# ---------------------------------------------------------
# A comma separated list of machine hostname or IP would be installed DolphinScheduler,
# including master, worker, api, alert. If you want to deploy in pseudo-distributed
# mode, just write a pseudo-distributed hostname
# Example for hostnames: ips="ds1,ds2,ds3,ds4,ds5", Example for IPs: ips="192.168.8.1,192.168.8.2,192.168.8.3,192.168.8.4,192.168.8.5"
# ips=${ips:-"ds1,ds2,ds3,ds4,ds5"}
ips=${ips:-"node111"}   # 部署在哪几个节点

# Port of SSH protocol, default value is 22. For now we only support same port in all `ips` machine
# modify it if you use different ssh port
sshPort=${sshPort:-"22"}   # 端口号

# A comma separated list of machine hostname or IP would be installed Master server, it
# must be a subset of configuration `ips`.
# Example for hostnames: masters="ds1,ds2", Example for IPs: masters="192.168.8.1,192.168.8.2"
# masters=${masters:-"ds1,ds2"}
masters=${masters:-"node111"}    # master所在节点

# A comma separated list of machine <hostname>:<workerGroup> or <IP>:<workerGroup>.All hostname or IP must be a
# subset of configuration `ips`, And workerGroup have default value as `default`, but we recommend you declare behind the hosts
# Example for hostnames: workers="ds1:default,ds2:default,ds3:default", Example for IPs: workers="192.168.8.1:default,192.168.8.2:default,192.168.8.3:default"
# workers=${workers:-"ds1:default,ds2:default,ds3:default,ds4:default,ds5:default"}
workers=${workers:-"node111:default"}    # worker所在节点

# A comma separated list of machine hostname or IP would be installed Alert server, it
# must be a subset of configuration `ips`.
# Example for hostname: alertServer="ds3", Example for IP: alertServer="192.168.8.3"
# alertServer=${alertServer:-"ds3"}
alertServer=${alertServer:-"node111"}    # alert所在节点

# A comma separated list of machine hostname or IP would be installed API server, it
# must be a subset of configuration `ips`.
# Example for hostname: apiServers="ds1", Example for IP: apiServers="192.168.8.1"
# apiServers=${apiServers:-"ds1"}
apiServers=${apiServers:-"node111"}    # api所在节点

# The directory to install DolphinScheduler for all machine we config above. It will automatically be created by `install.sh` script if not exists.
# Do not set this configuration same as the current path (pwd)
# installPath=${installPath:-"/tmp/dolphinscheduler"}
installPath=${installPath:-"/opt/dolphinscheduler-3.1.5"}    # 初始化后的路径

# The user to deploy DolphinScheduler for all machine we config above. For now user must create by yourself before running `install.sh`
# script. The user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled than the root directory needs
# to be created by this user
# deployUser=${deployUser:-"dolphinscheduler"}
deployUser=${deployUser:-"zhongtai"}    # DolphinScheduler的用户，现在用户必须在运行`install.sh`之前自己创建

# The root of zookeeper, for now DolphinScheduler default registry server is zookeeper.
zkRoot=${zkRoot:-"/dolphinscheduler"}   # zookeeper中的路径 
```



2.修改环境变量

`apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下bin/env/dolphinscheduler_env.sh

```shell
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# JAVA_HOME, will use it to start DolphinScheduler server
export JAVA_HOME=${JAVA_HOME:-/opt/jdk1.8.0_201}

# Database related configuration, set database type, username and password
export DATABASE=${DATABASE:-mysql}
export SPRING_PROFILES_ACTIVE=${DATABASE}
export SPRING_DATASOURCE_URL=jdbc:mysql://172.16.3.111:3306/dolphinscheduler
export SPRING_DATASOURCE_USERNAME=dolphinscheduler
export SPRING_DATASOURCE_PASSWORD=dolphinscheduler

# DolphinScheduler server related configuration
export SPRING_CACHE_TYPE=${SPRING_CACHE_TYPE:-none}
export SPRING_JACKSON_TIME_ZONE=${SPRING_JACKSON_TIME_ZONE:-Asia/Shanghai}
export MASTER_FETCH_COMMAND_NUM=${MASTER_FETCH_COMMAND_NUM:-10}

# Registry center configuration, determines the type and link of the registry center
export REGISTRY_TYPE=${REGISTRY_TYPE:-zookeeper}
export REGISTRY_ZOOKEEPER_CONNECT_STRING=${REGISTRY_ZOOKEEPER_CONNECT_STRING:-node111:2181}

# Tasks related configurations, need to change the configuration if you use the related tasks.
export HADOOP_HOME=${HADOOP_HOME:-/opt/hadoop-3.2.4}
export HADOOP_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-$HADOOP_HOME/etc/hadoop}
export SPARK_HOME1=${SPARK_HOME1:-/opt/soft/spark1}
export SPARK_HOME2=${SPARK_HOME2:-/opt/soft/spark2}
export PYTHON_HOME=${PYTHON_HOME:-/opt/soft/python}
export HIVE_HOME=${HIVE_HOME:-/opt/soft/hive}
export FLINK_HOME=${FLINK_HOME:-/opt/flink-1.15.3}
export DATAX_HOME=${DATAX_HOME:-/opt/soft/datax}
export SEATUNNEL_HOME=${SEATUNNEL_HOME:-/opt/apache-seatunnel-incubating-2.3.1}
export CHUNJUN_HOME=${CHUNJUN_HOME:-/opt/soft/chunjun}

export PATH=$HADOOP_HOME/bin:$SPARK_HOME1/bin:$SPARK_HOME2/bin:$PYTHON_HOME/bin:$JAVA_HOME/bin:$HIVE_HOME/bin:$FLINK_HOME/bin:$DATAX_HOME/bin:$SEATUNNEL_HOME/bin:$CHUNJUN_HOME/bin:$PATH
```



3.分别修改

`apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下

alert-server/conf/common.properties、

api-server/conf/common.properties、

master-server/conf/common.properties、

tools/conf/common.properties、

worker-server/conf/common.properties

```shell
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# user data local directory path, please make sure the directory exists and have read write permissions
# data.basedir.path=/tmp/dolphinscheduler
data.basedir.path=/data/dolphinscheduler-3.1.5/data    # 用户数据的本地目录路径

# resource storage type: HDFS, S3, NONE
# resource.storage.type=NONE
resource.storage.type=HDFS    # 指定存储类型

# resource store on HDFS/S3 path, resource file will store to this hadoop hdfs path, self configuration, please make sure the directory exists on hdfs and have read write permissions. "/dolphinscheduler" is recommended
# resource.upload.path=/dolphinscheduler
resource.upload.path=jfs://juice/dolphinscheduler-3.1.5    # JuiceFS资源存储目录

# The AWS access key. if resource.storage.type=S3 or use EMR-Task, This configuration is required
resource.aws.access.key.id=minioadmin
# The AWS secret access key. if resource.storage.type=S3 or use EMR-Task, This configuration is required
resource.aws.secret.access.key=minioadmin
# The AWS Region to use. if resource.storage.type=S3 or use EMR-Task, This configuration is required
resource.aws.region=cn-north-1
# The name of the bucket. You need to create them by yourself. Otherwise, the system cannot start. All buckets in Amazon S3 share a single namespace; ensure the bucket is given a unique name.
resource.aws.s3.bucket.name=dolphinscheduler
# You need to set this parameter when private cloud s3. If S3 uses public cloud, you only need to set resource.aws.region or set to the endpoint of a public cloud such as S3.cn-north-1.amazonaws.com.cn
resource.aws.s3.endpoint=http://localhost:9000

# alibaba cloud access key id, required if you set resource.storage.type=OSS
resource.alibaba.cloud.access.key.id=<your-access-key-id>
# alibaba cloud access key secret, required if you set resource.storage.type=OSS
resource.alibaba.cloud.access.key.secret=<your-access-key-secret>
# alibaba cloud region, required if you set resource.storage.type=OSS
resource.alibaba.cloud.region=cn-hangzhou
# oss bucket name, required if you set resource.storage.type=OSS
resource.alibaba.cloud.oss.bucket.name=dolphinscheduler
# oss bucket endpoint, required if you set resource.storage.type=OSS
resource.alibaba.cloud.oss.endpoint=https://oss-cn-hangzhou.aliyuncs.com

hdfs.root.user=zhongtai    # dolphinscheduler用户

# if resource.storage.type=S3, the value like: s3a://dolphinscheduler; if resource.storage.type=HDFS and namenode HA is enabled, you need to copy core-site.xml and hdfs-site.xml to conf dir
# fs.defaultFS=hdfs://mycluster:8020
fs.defaultFS=jfs://juice    # juiceFS的fs.defaultFS

# whether to startup kerberos
hadoop.security.authentication.startup.state=false

# java.security.krb5.conf path
java.security.krb5.conf.path=/opt/krb5.conf

# login user from keytab username
login.user.keytab.username=hdfs-mycluster@ESZ.COM

# login user from keytab path
login.user.keytab.path=/opt/hdfs.headless.keytab

# kerberos expire time, the unit is hour
kerberos.expire.time=2

# resourcemanager port, the default value is 8088 if not specified
resource.manager.httpaddress.port=8088
# if resourcemanager HA is enabled, please set the HA IPs; if resourcemanager is single, keep this value empty
# yarn.resourcemanager.ha.rm.ids=192.168.xx.xx,192.168.xx.xx
yarn.resourcemanager.ha.rm.ids=    # yarn的resourcemanager节点,如果单节点保持空置
# if resourcemanager HA is enabled or not use resourcemanager, please keep the default value; If resourcemanager is single, you only need to replace ds1 to actual resourcemanager hostname
yarn.application.status.address=http://node111:%s/ws/v1/cluster/apps/%s
# job history status url when application number threshold is reached(default 10000, maybe it was set to 1000)
yarn.job.history.status.address=http://node111:19888/ws/v1/history/mapreduce/jobs/%s  # 历史服务器节点

# datasource encryption enable
datasource.encryption.enable=false

# datasource encryption salt
datasource.encryption.salt=!@#$%^&*

# data quality option
data-quality.jar.name=dolphinscheduler-data-quality-3.1.5.jar    # 指定数据质量jar

#data-quality.error.output.path=/tmp/data-quality-error-data

# Network IP gets priority, default inner outer

# Whether hive SQL is executed in the same session
support.hive.oneSession=false

# use sudo or not, if set true, executing user is tenant user and deploy user needs sudo permissions; if set false, executing user is the deploy user and doesn't need sudo permissions
sudo.enable=true
setTaskDirToTenant.enable=false

# network interface preferred like eth0, default: empty
#dolphin.scheduler.network.interface.preferred=

# network IP gets priority, default: inner outer
#dolphin.scheduler.network.priority.strategy=default

# system env path
#dolphinscheduler.env.path=dolphinscheduler_env.sh

# development state
development.state=false

# rpc port
alert.rpc.port=50052

# set path of conda.sh
conda.path=/opt/anaconda3/etc/profile.d/conda.sh

# Task resource limit state
task.resource.limit.state=false

# mlflow task plugin preset repository
ml.mlflow.preset_repository=https://github.com/apache/dolphinscheduler-mlflow
# mlflow task plugin preset repository version
ml.mlflow.preset_repository_version="main"
```

创建hdfs资源目录

```shell
hdfs dfs -mkdir -p jfs://juice/dolphinscheduler-3.1.5
```



4.分别将core-site.xml和hdfs-site.xml拷贝至

`apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下

alert-server/conf/、

api-server/conf/、

master-server/conf/、

tools/conf/、

worker-server/conf/

```shell
cp $HADOOP_HOME/etc/hadoop/core-site.xml ./
cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml ./
```



5.分别修改

`apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下

alert-server/conf/application.yaml、

api-server/conf/application.yaml、

master-server/conf/application.yaml、

tools/conf/application.yaml、

worker-server/conf/application.yaml

将数据源配置改为我们需要连接的数据源

```shell
  datasource:
    driver-class-name: org.postgresql.Driver
#    url: jdbc:postgresql://127.0.0.1:5432/dolphinscheduler
#    username: root
#    password: root
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://172.16.3.111:3306/dolphinscheduler
    username: dolphinscheduler
    password: dolphinscheduler
    
  registry:
  type: zookeeper
  zookeeper:
    namespace: dolphinscheduler
    connect-string: node111:2181    # 根据zookeeper部署节点配置
    retry-policy:
      base-sleep-time: 60ms
      max-sleep: 300ms
      max-retries: 5
    session-timeout: 30s
    connection-timeout: 9s
    block-until-connected: 600ms
    digest: ~

```



6.添加nacos配置

修改 `apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下api-server/conf/application.yaml

```shell
# nacos服务器地址.
spring.cloud:
  # spring cloud alibaba 适配的是spring boot 2.6.x, dolphinscheduler使用的是2.5.6因此需要关闭兼容性检查.
  compatibility-verifier:
    enabled: false
  nacos:
    discovery:
      server-addr: 172.16.3.111:8848     # nacos所在节点
      # 指定服务名, 不使用spring.application.name的值.
      service: dolphinscheduler-api-server
  # 当前服务器有多个IP,指定nacos使用可被局域网访问的地址,而不是虚拟网卡的地址.
  inetutils.preferred-networks: 172.16.3
```



7.dolphinscheduler集成 JuiceFS需要将 `juicefs-hadoop-1.0.4.jar` 放至

`apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下

alert-server/libs、

api-server/libs、

master-server/libs、

worker-server/libs



8.修改 DS 日志和 pid 位置

`apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下bin/dolphinscheduler-daemon.sh 

```shell
# dolphinscheduler log dir
DOLPHINSCHEDULER_LOG_DIR=/data/dolphinscheduler300/log    # log目录
if [ "x$DOLPHINSCHEDULER_LOG_DIR" = "x" ]; then
  DOLPHINSCHEDULER_LOG_DIR=$DOLPHINSCHEDULER_HOME/logs
fi

DOLPHINSCHEDULER_LOG_DIR=$DOLPHINSCHEDULER_LOG_DIR/$command

if [ ! -d "$DOLPHINSCHEDULER_LOG_DIR" ]; then
  mkdir -p $DOLPHINSCHEDULER_LOG_DIR
fi

# dolphinscheduler pid dir
DOLPHINSCHEDULER_PID_DIR=/data/dolphinscheduler300/pid   # pid目录
if [ "x$DOLPHINSCHEDULER_PID_DIR" = "x" ]; then
  DOLPHINSCHEDULER_PID_DIR=$DOLPHINSCHEDULER_HOME/pid
fi

DOLPHINSCHEDULER_PID_DIR=$DOLPHINSCHEDULER_PID_DIR/$command

if [ ! -d "$DOLPHINSCHEDULER_PID_DIR" ]; then
  mkdir -p $DOLPHINSCHEDULER_PID_DIR
fi

pid=$DOLPHINSCHEDULER_PID_DIR/pid

# dolphinscheduler gc log
DOLPHINSCHEDULER_GC_DIR=/data/dolphinscheduler300/gc   # gc目录
if [ "x$DOLPHINSCHEDULER_GC_DIR" = "x" ]; then
  DOLPHINSCHEDULER_GC_DIR=$DOLPHINSCHEDULER_HOME/gc
fi

DOLPHINSCHEDULER_GC_DIR=$DOLPHINSCHEDULER_GC_DIR/$command

if [ ! -d "$DOLPHINSCHEDULER_GC_DIR" ]; then
  mkdir -p $DOLPHINSCHEDULER_GC_DIR
fi

```

创建日志目录、pid目录、gc目录、用户数据的本地目录路径（所有节点都需创建）

```shell
sudo mkdir -p /data/dolphinscheduler-3.1.5/log 
sudo mkdir -p /data/dolphinscheduler-3.1.5/pid
sudo mkdir -p /data/dolphinscheduler-3.1.5/gc
sudo mkdir -p /data/dolphinscheduler-3.1.5/data
sudo chown -R zhongtai:zhongtai /data/dolphinscheduler-3.1.5
```



9.修改api的初始内存和最大内存

`apache-dolphinscheduler-3.0.1-SNAPSHOT-bin` 目录下api-server/bin/start.sh

```shell
BIN_DIR=$(dirname $0)
DOLPHINSCHEDULER_HOME=${DOLPHINSCHEDULER_HOME:-$(cd $BIN_DIR/..; pwd)}

source "$DOLPHINSCHEDULER_HOME/conf/dolphinscheduler_env.sh"

JAVA_OPTS=${JAVA_OPTS:-"-server -Duser.timezone=${SPRING_JACKSON_TIME_ZONE} -Xms2g -Xmx2g -Xmn512m -XX:+PrintGCDetails -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=dump.hprof"}

if [[ "$DOCKER" == "true" ]]; then
  JAVA_OPTS="${JAVA_OPTS} -XX:-UseContainerSupport"
fi

java $JAVA_OPTS \
  -cp "$DOLPHINSCHEDULER_HOME/conf":"$DOLPHINSCHEDULER_HOME/libs/*" \
  org.apache.dolphinscheduler.api.ApiApplicationServer

```



10.初始化数据库 （数据源：mysql）

登录mysql

```shell
mysql -uroot -p123456
```

```sql
mysql> CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;

# 修改 {user} 和 {password} 为你希望的用户名和密码
mysql> CREATE USER '{user}'@'%' IDENTIFIED BY '{password}';
mysql> GRANT ALL PRIVILEGES ON dolphinscheduler.* TO '{user}'@'%';
mysql> FLUSH PRIVILEGES;
```

初始化脚本

```shell
mysql -udolphinscheduler -pdolphinscheduler -Ddolphinscheduler<初始化sql脚本
```



11.启动dolphinscheduler

在/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/bin目录下执行

```shell
sh ./install.sh
```



13.将平台使用的jar放至资源中心

```
dolphinscheduler web UI：http://ip:12345/dolphinscheduler

用户名：admin
密码：dolphinscheduler123
```

