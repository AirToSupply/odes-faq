## dolphinscheduler-3.X安装部署(greenplum版)

1.修改 /software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/bin/env目录下`install_env.sh` 文件

```shell
# limitations under the License.
#

# ---------------------------------------------------------
# INSTALL MACHINE
# ---------------------------------------------------------
# A comma separated list of machine hostname or IP would be installed DolphinScheduler,
# including master, worker, api, alert. If you want to deploy in pseudo-distributed
# mode, just write a pseudo-distributed hostname
# Example for hostnames: ips="ds1,ds2,ds3,ds4,ds5", Example for IPs: ips="192.168.8.1,192.168.8.2,192.168.8.3,192.168.8.4,192.168.8.5"
# ips=${ips:-"ds1,ds2,ds3,ds4,ds5"}
ips=${ips:-"2-120,2-123,2-124"}   # 部署在哪几个节点

# Port of SSH protocol, default value is 22. For now we only support same port in all `ips` machine
# modify it if you use different ssh port
sshPort=${sshPort:-"22"}   # 端口号

# A comma separated list of machine hostname or IP would be installed Master server, it
# must be a subset of configuration `ips`.
# Example for hostnames: masters="ds1,ds2", Example for IPs: masters="192.168.8.1,192.168.8.2"
# masters=${masters:-"ds1,ds2"}
masters=${masters:-"2-124"}    # master所在节点

# A comma separated list of machine <hostname>:<workerGroup> or <IP>:<workerGroup>.All hostname or IP must be a
# subset of configuration `ips`, And workerGroup have default value as `default`, but we recommend you declare behind the hosts
# Example for hostnames: workers="ds1:default,ds2:default,ds3:default", Example for IPs: workers="192.168.8.1:default,192.168.8.2:default,192.168.8.3:default"
# workers=${workers:-"ds1:default,ds2:default,ds3:default,ds4:default,ds5:default"}
workers=${workers:-"2-120:default,2-123:default,2-124:default"}    # worker所在节点

# A comma separated list of machine hostname or IP would be installed Alert server, it
# must be a subset of configuration `ips`.
# Example for hostname: alertServer="ds3", Example for IP: alertServer="192.168.8.3"
# alertServer=${alertServer:-"ds3"}
alertServer=${alertServer:-"2-124"}    # alert所在节点

# A comma separated list of machine hostname or IP would be installed API server, it
# must be a subset of configuration `ips`.
# Example for hostname: apiServers="ds1", Example for IP: apiServers="192.168.8.1"
# apiServers=${apiServers:-"ds1"}
apiServers=${apiServers:-"2-124"}    # api所在节点

# The directory to install DolphinScheduler for all machine we config above. It will automatically be created by `install.sh` script if not exists.
# Do not set this configuration same as the current path (pwd)
# installPath=${installPath:-"/tmp/dolphinscheduler"}
installPath=${installPath:-"/workspace/apache-dolphinscheduler300"}    # 初始化后的路径

# The user to deploy DolphinScheduler for all machine we config above. For now user must create by yourself before running `install.sh`
# script. The user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled than the root directory needs
# to be created by this user
# deployUser=${deployUser:-"dolphinscheduler"}
deployUser=${deployUser:-"zhongtai"}    # DolphinScheduler的用户，现在用户必须在运行`install.sh`之前自己创建

# The root of zookeeper, for now DolphinScheduler default registry server is zookeeper.
zkRoot=${zkRoot:-"/dolphinscheduler"}    # zookeeper中的路径
```



2.修改环境变量

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/bin/env/dolphinscheduler_env.sh

```shell
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# JAVA_HOME, will use it to start DolphinScheduler server
export JAVA_HOME=${JAVA_HOME:-/home/workspace/jdk1.8.0_201}

# Database related configuration, set database type, username and password
export DATABASE=${DATABASE:-postgresql}
export SPRING_PROFILES_ACTIVE=${DATABASE}
export SPRING_DATASOURCE_DRIVER_CLASS_NAME=org.postgresql.Driver
export SPRING_DATASOURCE_URL=jdbc:postgresql://172.16.10.72:5432/dolphinscheduler300
export SPRING_DATASOURCE_USERNAME=dolphinscheduler
export SPRING_DATASOURCE_PASSWORD=dolphinscheduler

# DolphinScheduler server related configuration
export SPRING_CACHE_TYPE=${SPRING_CACHE_TYPE:-none}
export SPRING_JACKSON_TIME_ZONE=${SPRING_JACKSON_TIME_ZONE:-Asia/Shanghai}
export MASTER_FETCH_COMMAND_NUM=${MASTER_FETCH_COMMAND_NUM:-10}

# Registry center configuration, determines the type and link of the registry center
export REGISTRY_TYPE=${REGISTRY_TYPE:-zookeeper}
export REGISTRY_ZOOKEEPER_CONNECT_STRING=${REGISTRY_ZOOKEEPER_CONNECT_STRING:-bigdata04:2181}

# Tasks related configurations, need to change the configuration if you use the related tasks.
export HADOOP_HOME=${HADOOP_HOME:-/home/workspace/hadoop-2.10.1}
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/home/workspace/hadoop-2.10.1/etc/hadoop}
export HADOOP_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
export SPARK_HOME1=${SPARK_HOME1:-/opt/soft/spark1}
export SPARK_HOME2=${SPARK_HOME2:-/home/workspace/spark-3.1.1-bin-hadoop2.7}
export PYTHON_HOME=${PYTHON_HOME:-/bin/python2.7}
export HIVE_HOME=${HIVE_HOME:-/opt/soft/hive}
export FLINK_HOME=${FLINK_HOME:-/opt/soft/flink}
export DATAX_HOME=${DATAX_HOME:-/home/workspace/datax}

export PATH=$HADOOP_HOME/bin:$SPARK_HOME1/bin:$SPARK_HOME2/bin:$PYTHON_HOME/bin:$JAVA_HOME/bin:$HIVE_HOME/bin:$FLINK_HOME/bin:$DATAX_HOME/bin:$PATH
```



3.分别修改

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/alert-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/api-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/master-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/tools/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/worker-server/conf

目录下`common.properties`文件

```shell
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# user data local directory path, please make sure the directory exists and have read write permissions
# data.basedir.path=/tmp/dolphinscheduler
data.basedir.path=/data/dolphinscheduler300/data    # 用户数据的本地目录路径

# resource storage type: HDFS, S3, NONE
# resource.storage.type=NONE
resource.storage.type=HDFS    # 指定存储类型

# resource store on HDFS/S3 path, resource file will store to this hadoop hdfs path, self configuration, please make sure the directory exists on hdfs and have read write permissions. "/dolphinscheduler" is recommended
# resource.upload.path=/dolphinscheduler
resource.upload.path=file:///data/nfsdata    # 本地资源存储目录

# whether to startup kerberos
hadoop.security.authentication.startup.state=false

# java.security.krb5.conf path
java.security.krb5.conf.path=/opt/krb5.conf

# login user from keytab username
login.user.keytab.username=hdfs-mycluster@ESZ.COM

# login user from keytab path
login.user.keytab.path=/opt/hdfs.headless.keytab

# kerberos expire time, the unit is hour
kerberos.expire.time=2
# resource view suffixs
#resource.view.suffixs=txt,log,sh,bat,conf,cfg,py,java,sql,xml,hql,properties,json,yml,yaml,ini,js
# if resource.storage.type=HDFS, the user must have the permission to create directories under the HDFS root path
# hdfs.root.user=hdfs
hdfs.root.user=zhongtai    # dolphinscheduler用户

# if resource.storage.type=S3, the value like: s3a://dolphinscheduler; if resource.storage.type=HDFS and namenode HA is enabled, you need to copy core-site.xml and hdfs-site.xml to conf dir
# fs.defaultFS=hdfs://mycluster:8020
fs.defaultFS=file:///data/nfsdata    # 如果是本地存储，和资源存储目录一致
aws.access.key.id=minioadmin
aws.secret.access.key=minioadmin
aws.region=us-east-1
aws.endpoint=http://localhost:9000
# resourcemanager port, the default value is 8088 if not specified
resource.manager.httpaddress.port=8088
# if resourcemanager HA is enabled, please set the HA IPs; if resourcemanager is single, keep this value empty
# yarn.resourcemanager.ha.rm.ids=192.168.xx.xx,192.168.xx.xx
yarn.resourcemanager.ha.rm.ids=    # 不使用yarn，保持为空
# if resourcemanager HA is enabled or not use resourcemanager, please keep the default value; If resourcemanager is single, you only need to replace ds1 to actual resourcemanager hostname
yarn.application.status.address=http://ds1:%s/ws/v1/cluster/apps/%s
# job history status url when application number threshold is reached(default 10000, maybe it was set to 1000)
yarn.job.history.status.address=http://ds1:19888/ws/v1/history/mapreduce/jobs/%s

# datasource encryption enable
datasource.encryption.enable=false

# datasource encryption salt
datasource.encryption.salt=!@#$%^&*

# data quality option
data-quality.jar.name=dolphinscheduler-data-quality-3.0.1-SNAPSHOT.jar    # 指定数据质量jar

#data-quality.error.output.path=/tmp/data-quality-error-data

# Network IP gets priority, default inner outer

# Whether hive SQL is executed in the same session
support.hive.oneSession=false

# use sudo or not, if set true, executing user is tenant user and deploy user needs sudo permissions; if set false, executing user is the deploy user and doesn't need sudo permissions
sudo.enable=true

# network interface preferred like eth0, default: empty
#dolphin.scheduler.network.interface.preferred=

# network IP gets priority, default: inner outer
#dolphin.scheduler.network.priority.strategy=default

# system env path
#dolphinscheduler.env.path=dolphinscheduler_env.sh

# development state
development.state=false

# rpc port
alert.rpc.port=50052

# Url endpoint for zeppelin RESTful API
zeppelin.rest.url=http://localhost:8080
```



4.分别将`core-site.xml`和`hdfs-site.xml`拷贝至

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/alert-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/api-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/master-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/worker-server/conf

目录下

```shell
cp $HADOOP_HOME/etc/hadoop/core-site.xml ./
cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml ./
```



5.分别修改

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/alert-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/api-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/master-server/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/tools/conf、

/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/worker-server/conf

目录下`application.yaml`文件，将数据源配置改为我们需要连接的数据源

```shell
  datasource:
    driver-class-name: org.postgresql.Driver
#    url: jdbc:postgresql://127.0.0.1:5432/dolphinscheduler
#    username: root
#    password: root
    url: jdbc:postgresql://2-120:5432/dolphinscheduler300
    username: dolphinscheduler
    password: dolphinscheduler
    
    
  registry:
  type: zookeeper
  zookeeper:
    namespace: dolphinscheduler
    connect-string: localhost:2181    # 根据zookeeper部署节点配置
    retry-policy:
      base-sleep-time: 60ms
      max-sleep: 300ms
      max-retries: 5
    session-timeout: 30s
    connection-timeout: 9s
    block-until-connected: 600ms
    digest: ~

```



6.添加nacos配置

vim /software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/api-server/conf/application.yaml

vim /software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/master-server/conf/application.yaml

vim /software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/worker-server/conf/application.yaml

```shell
# nacos服务器地址.
spring.cloud:
  # spring cloud alibaba 适配的是spring boot 2.6.x, dolphinscheduler使用的是2.5.6因此需要关闭兼容性检查.
  compatibility-verifier:
    enabled: false
  nacos:
    discovery:
      server-addr: 172.16.10.207:8848     # nacos所在节点
      # 指定服务名, 不使用spring.application.name的值.
      service: dolphinscheduler-api-server
  # 当前服务器有多个IP,指定nacos使用可被局域网访问的地址,而不是虚拟网卡的地址.
  inetutils.preferred-networks: 172.16.10
```



7.  修改 DS 日志和 pid 位置

vim /software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/bin/dolphinscheduler-daemon.sh

```shell
# dolphinscheduler log dir
DOLPHINSCHEDULER_LOG_DIR=/data/dolphinscheduler300/log   # log目录
if [ "x$DOLPHINSCHEDULER_LOG_DIR" = "x" ]; then
  DOLPHINSCHEDULER_LOG_DIR=$DOLPHINSCHEDULER_HOME/logs
fi

DOLPHINSCHEDULER_LOG_DIR=$DOLPHINSCHEDULER_LOG_DIR/$command

if [ ! -d "$DOLPHINSCHEDULER_LOG_DIR" ]; then
  mkdir -p $DOLPHINSCHEDULER_LOG_DIR
fi

# dolphinscheduler pid dir
DOLPHINSCHEDULER_PID_DIR=/data/dolphinscheduler300/pid    # pid目录
if [ "x$DOLPHINSCHEDULER_PID_DIR" = "x" ]; then
  DOLPHINSCHEDULER_PID_DIR=$DOLPHINSCHEDULER_HOME/pid
fi

DOLPHINSCHEDULER_PID_DIR=$DOLPHINSCHEDULER_PID_DIR/$command

if [ ! -d "$DOLPHINSCHEDULER_PID_DIR" ]; then
  mkdir -p $DOLPHINSCHEDULER_PID_DIR
fi

pid=$DOLPHINSCHEDULER_PID_DIR/pid

# dolphinscheduler gc log
DOLPHINSCHEDULER_GC_DIR=/data/dolphinscheduler300/gc   # gc目录
if [ "x$DOLPHINSCHEDULER_GC_DIR" = "x" ]; then
  DOLPHINSCHEDULER_GC_DIR=$DOLPHINSCHEDULER_HOME/gc
fi

DOLPHINSCHEDULER_GC_DIR=$DOLPHINSCHEDULER_GC_DIR/$command

if [ ! -d "$DOLPHINSCHEDULER_GC_DIR" ]; then
  mkdir -p $DOLPHINSCHEDULER_GC_DIR
fi

```

创建资源目录、数据目录、日志目录、pid目录、gc目录

```shell
sudo mkdir -p /data/dolphinscheduler300/log 
sudo mkdir -p /data/dolphinscheduler300/pid
sudo mkdir -p /data/dolphinscheduler300/gc
sudo mkdir -p /data/dolphinscheduler300/data
sudo mkdir -p /data/nfsdata/zhongtai/resources/_DATA_IO_
sudo mkdir -p /data/nfsdata/zhongtai/udfs
sudo chown -R zhongtai:zhongtai /data/dolphinscheduler300
sudo chown -R zhongtai:zhongtai /data/nfsdata
```



8.修改api的初始内存和最大内存

vim  /software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/api-server/bin/start.sh

```shell
BIN_DIR=$(dirname $0)
DOLPHINSCHEDULER_HOME=${DOLPHINSCHEDULER_HOME:-$(cd $BIN_DIR/..; pwd)}

source "$DOLPHINSCHEDULER_HOME/conf/dolphinscheduler_env.sh"

JAVA_OPTS=${JAVA_OPTS:-"-server -Duser.timezone=${SPRING_JACKSON_TIME_ZONE} -Xms2g -Xmx2g -Xmn512m -XX:+PrintGCDetails -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=dump.hprof"}

if [[ "$DOCKER" == "true" ]]; then
  JAVA_OPTS="${JAVA_OPTS} -XX:-UseContainerSupport"
fi

java $JAVA_OPTS \
  -cp "$DOLPHINSCHEDULER_HOME/conf":"$DOLPHINSCHEDULER_HOME/libs/*" \
  org.apache.dolphinscheduler.api.ApiApplicationServer

```





9.初始化数据库 （数据源：postgresql）

切换postgres用户

```shell
su - postgres
```

```sql
 psql -h 172.16.2.120 -p 5432 -U postgres -d postgres
 
 登录数据库创建用户及密码
postgres=# create user dolphinscheduler with password 'dolphinscheduler';
创建数据库
postgres=# CREATE DATABASE dolphinscheduler300 TEMPLATE template0 ENCODING UTF8 LC_COLLATE 'zh_CN.utf8' LC_CTYPE 'zh_CN.utf8' owner dolphinscheduler;
赋予全部权限
postgres=# grant all on database dolphinscheduler300 to dolphinscheduler;

postgres=# quit
```

在终端执行如下命令，向配置文件新增登陆权限，并重载 PostgreSQL 配置：

```shell
vim $PGDATA/pg_hba.conf
```

添加如下配置

```shell
host    dolphinscheduler300   dolphinscheduler    0.0.0.0/0     trust 
注： 放置 "host    all    all    0.0.0.0/0    md5" 上
```

执行如下命令,使配置生效：

```shell
pg_ctl reload
```

初始化脚本

```shell
psql -h 172.16.10.59 -U dolphinscheduler -d dolphinscheduler300 -f 初始化sql脚本
```



10.创建用户数据的本地目录路径（common.properties中的data.basedir.path配置,所有worker节点都需创建）

```shell
cd /data
sudo mkdir dolphinscheduler300
sudo chown zhongtai:zhongtai dolphinscheduler300
```



11.启动dolphinscheduler

在/software/dolphinscheduler/apache-dolphinscheduler-3.0.1-SNAPSHOT-bin/bin目录下执行

```shell
sh ./install.sh
```



12.将平台使用的jar放至资源中心

```shell
dolphinscheduler web UI：http://ip:12345/dolphinscheduler

用户名：admin
密码：dolphinscheduler123
```

选择资源中心中`_DATA_IO_`目录,根据平台需求上传jar包

```shell
time_series_influxdb_data_collector.jar            ## 时序数据库influxdb数据采集到内置存储库
flink-psql2kafka.jar                               ## flink任务jar包-数据共享psql2kafka
data_collector_flink_influxdb2tdengine.jar         ## flink实时数据采集influxdb to tdengine
data_collector_flink_mqtt2tdengine.jar             ## flink实时数据采集mqtt to tdengine
data_collector_flink_kafka2tdengine.jar            ## flink实时数据采集kafka to tdengine
```
