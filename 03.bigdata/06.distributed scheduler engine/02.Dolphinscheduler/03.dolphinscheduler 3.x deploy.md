## dolphinscheduler-3.X安装部署

1.修改 /software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/bin/env目录下`install_env.sh` 文件

```
# limitations under the License.
#

# ---------------------------------------------------------
# INSTALL MACHINE
# ---------------------------------------------------------
# A comma separated list of machine hostname or IP would be installed DolphinScheduler,
# including master, worker, api, alert. If you want to deploy in pseudo-distributed
# mode, just write a pseudo-distributed hostname
# Example for hostnames: ips="ds1,ds2,ds3,ds4,ds5", Example for IPs: ips="192.168.8.1,192.168.8.2,192.168.8.3,192.168.8.4,192.168.8.5"
# ips=${ips:-"ds1,ds2,ds3,ds4,ds5"}
ips=${ips:-"2-120,2-123,2-124"}

# Port of SSH protocol, default value is 22. For now we only support same port in all `ips` machine
# modify it if you use different ssh port
sshPort=${sshPort:-"22"}

# A comma separated list of machine hostname or IP would be installed Master server, it
# must be a subset of configuration `ips`.
# Example for hostnames: masters="ds1,ds2", Example for IPs: masters="192.168.8.1,192.168.8.2"
# masters=${masters:-"ds1,ds2"}
masters=${masters:-"2-124"}

# A comma separated list of machine <hostname>:<workerGroup> or <IP>:<workerGroup>.All hostname or IP must be a
# subset of configuration `ips`, And workerGroup have default value as `default`, but we recommend you declare behind the hosts
# Example for hostnames: workers="ds1:default,ds2:default,ds3:default", Example for IPs: workers="192.168.8.1:default,192.168.8.2:default,192.168.8.3:default"
# workers=${workers:-"ds1:default,ds2:default,ds3:default,ds4:default,ds5:default"}
workers=${workers:-"2-120:default,2-123:default,2-124:default"}

# A comma separated list of machine hostname or IP would be installed Alert server, it
# must be a subset of configuration `ips`.
# Example for hostname: alertServer="ds3", Example for IP: alertServer="192.168.8.3"
# alertServer=${alertServer:-"ds3"}
alertServer=${alertServer:-"2-124"}

# A comma separated list of machine hostname or IP would be installed API server, it
# must be a subset of configuration `ips`.
# Example for hostname: apiServers="ds1", Example for IP: apiServers="192.168.8.1"
# apiServers=${apiServers:-"ds1"}
apiServers=${apiServers:-"2-124"}

# The directory to install DolphinScheduler for all machine we config above. It will automatically be created by `install.sh` script if not exists.
# Do not set this configuration same as the current path (pwd)
# installPath=${installPath:-"/tmp/dolphinscheduler"}
installPath=${installPath:-"/workspace/apache-dolphinscheduler-3.0.0-beta-1"}

# The user to deploy DolphinScheduler for all machine we config above. For now user must create by yourself before running `install.sh`
# script. The user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled than the root directory needs
# to be created by this user
# deployUser=${deployUser:-"dolphinscheduler"}
deployUser=${deployUser:-"zhongtai"}

# The root of zookeeper, for now DolphinScheduler default registry server is zookeeper.
zkRoot=${zkRoot:-"/dolphinscheduler"}
```



2.分别修改

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/bin/env

目录下`dolphinscheduler_env.sh`文件 

```
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# JAVA_HOME, will use it to start DolphinScheduler server
# export JAVA_HOME=${JAVA_HOME:-/opt/soft/java}
export JAVA_HOME=/workspace/jdk1.8.0_201

# Database related configuration, set database type, username and password
export DATABASE=${DATABASE:-postgresql}
export SPRING_PROFILES_ACTIVE=${DATABASE}
export SPRING_DATASOURCE_DRIVER_CLASS_NAME=org.postgresql.Driver
export SPRING_DATASOURCE_URL=jdbc:postgresql://2-120:5432/dolphinscheduler
export SPRING_DATASOURCE_USERNAME=postgres
export SPRING_DATASOURCE_PASSWORD=postgres

# DolphinScheduler server related configuration
export SPRING_CACHE_TYPE=${SPRING_CACHE_TYPE:-none}
# export SPRING_JACKSON_TIME_ZONE=${SPRING_JACKSON_TIME_ZONE:-UTC}
export SPRING_JACKSON_TIME_ZONE=${SPRING_JACKSON_TIME_ZONE:-Asia/Shanghai}
export MASTER_FETCH_COMMAND_NUM=${MASTER_FETCH_COMMAND_NUM:-10}

# Registry center configuration, determines the type and link of the registry center
export REGISTRY_TYPE=${REGISTRY_TYPE:-zookeeper}
# export REGISTRY_ZOOKEEPER_CONNECT_STRING=${REGISTRY_ZOOKEEPER_CONNECT_STRING:-localhost:2181}
export REGISTRY_ZOOKEEPER_CONNECT_STRING=${REGISTRY_ZOOKEEPER_CONNECT_STRING:-2-120:2181,2-123:2181,2-124:2181}

# Tasks related configurations, need to change the configuration if you use the related tasks.
# export HADOOP_HOME=${HADOOP_HOME:-/opt/soft/hadoop}
# export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/opt/soft/hadoop/etc/hadoop}
# export SPARK_HOME1=${SPARK_HOME1:-/opt/soft/spark1}
# export SPARK_HOME2=${SPARK_HOME2:-/opt/soft/spark2}
# export PYTHON_HOME=${PYTHON_HOME:-/opt/soft/python}
# export HIVE_HOME=${HIVE_HOME:-/opt/soft/hive}
# export FLINK_HOME=${FLINK_HOME:-/opt/soft/flink}
# export DATAX_HOME=${DATAX_HOME:-/opt/soft/datax}
export HADOOP_HOME=/workspace/hadoop-2.10.1
export HADOOP_CONF_DIR=/workspace/hadoop-2.10.1/etc/hadoop
export SPARK_HOME1=/opt/soft/spark1
export SPARK_HOME2=/workspace/spark-3.1.1-bin-hadoop2.7
export PYTHON_HOME=/opt/soft/python
export JAVA_HOME=/workspace/jdk1.8.0_201
export HIVE_HOME=/workspace/hive-2.3.8-bin
export FLINK_HOME=/workspace/flink-1.13.1
export DATAX_HOME=/opt/soft/datax

export PATH=$HADOOP_HOME/bin:$SPARK_HOME1/bin:$SPARK_HOME2/bin:$PYTHON_HOME/bin:$JAVA_HOME/bin:$HIVE_HOME/bin:$FLINK_HOME/bin:$DATAX_HOME/bin:$PATH
```



3.分别修改

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/alert-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/api-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/master-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/standalone-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/tools/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/worker-server/conf

目录下`common.properties`文件

```
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# user data local directory path, please make sure the directory exists and have read write permissions
# data.basedir.path=/tmp/dolphinscheduler
data.basedir.path=/data/dolphinscheduler

# resource storage type: HDFS, S3, NONE
# resource.storage.type=NONE
resource.storage.type=HDFS

# resource store on HDFS/S3 path, resource file will store to this hadoop hdfs path, self configuration, please make sure the directory exists on hdfs and have read write permissions. "/dolphinscheduler" is recommended
resource.upload.path=/dolphinscheduler

# whether to startup kerberos
hadoop.security.authentication.startup.state=false

# java.security.krb5.conf path
java.security.krb5.conf.path=/opt/krb5.conf

# login user from keytab username
login.user.keytab.username=hdfs-mycluster@ESZ.COM

# login user from keytab path
login.user.keytab.path=/opt/hdfs.headless.keytab

# kerberos expire time, the unit is hour
kerberos.expire.time=2
# resource view suffixs
#resource.view.suffixs=txt,log,sh,bat,conf,cfg,py,java,sql,xml,hql,properties,json,yml,yaml,ini,js
# if resource.storage.type=HDFS, the user must have the permission to create directories under the HDFS root path
# hdfs.root.user=hdfs
hdfs.root.user=zhongtai

# if resource.storage.type=S3, the value like: s3a://dolphinscheduler; if resource.storage.type=HDFS and namenode HA is enabled, you need to copy core-site.xml and hdfs-site.xml to conf dir
# fs.defaultFS=hdfs://mycluster:8020
fs.defaultFS=hdfs://ns
aws.access.key.id=minioadmin
aws.secret.access.key=minioadmin
aws.region=us-east-1
aws.endpoint=http://localhost:9000
# resourcemanager port, the default value is 8088 if not specified
resource.manager.httpaddress.port=8088
# if resourcemanager HA is enabled, please set the HA IPs; if resourcemanager is single, keep this value empty
# yarn.resourcemanager.ha.rm.ids=192.168.xx.xx,192.168.xx.xx
yarn.resourcemanager.ha.rm.ids=2-123,2-124
# if resourcemanager HA is enabled or not use resourcemanager, please keep the default value; If resourcemanager is single, you only need to replace ds1 to actual resourcemanager hostname
yarn.application.status.address=http://ds1:%s/ws/v1/cluster/apps/%s
# job history status url when application number threshold is reached(default 10000, maybe it was set to 1000)
# yarn.job.history.status.address=http://ds1:19888/ws/v1/history/mapreduce/jobs/%s
yarn.job.history.status.address=http://2-124:19888/ws/v1/history/mapreduce/jobs/%s

# datasource encryption enable
datasource.encryption.enable=false

# datasource encryption salt
datasource.encryption.salt=!@#$%^&*

# data quality option
data-quality.jar.name=libs/dolphinscheduler-data-quality-dev-SNAPSHOT.jar

#data-quality.error.output.path=/tmp/data-quality-error-data

# Network IP gets priority, default inner outer

# Whether hive SQL is executed in the same session
support.hive.oneSession=false

# use sudo or not, if set true, executing user is tenant user and deploy user needs sudo permissions; if set false, executing user is the deploy user and doesn't need sudo permissions
sudo.enable=true

# network interface preferred like eth0, default: empty
#dolphin.scheduler.network.interface.preferred=

# network IP gets priority, default: inner outer
#dolphin.scheduler.network.priority.strategy=default

# system env path
#dolphinscheduler.env.path=dolphinscheduler_env.sh

# development state
development.state=false

# rpc port
alert.rpc.port=50052

# Url endpoint for zeppelin RESTful API
zeppelin.rest.url=http://localhost:8080
```



4.分别将`core-site.xml`和`hdfs-site.xml`拷贝至

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/alert-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/api-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/master-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/worker-server/conf

目录下



5.分别修改

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/alert-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/api-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/master-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/standalone-server/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/tools/conf、

/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/worker-server/conf

目录下`application.yaml`文件，将数据源配置改为我们需要连接的数据源

```
  datasource:
    driver-class-name: org.postgresql.Driver
#    url: jdbc:postgresql://127.0.0.1:5432/dolphinscheduler
#    username: root
#    password: root
    url: jdbc:postgresql://2-120:5432/dolphinscheduler
    username: postgres
    password: postgres
```



6.初始化数据库 （数据源：postgresql）

```
create user dolphinscheduler with password 'dolphinscheduler';

CREATE DATABASE dolphinscheduler TEMPLATE template0 ENCODING UTF8 LC_COLLATE 'zh_CN.utf8' LC_CTYPE 'zh_CN.utf8' owner dolphinscheduler;

grant all on database dolphinscheduler to dolphinscheduler;
```

完成上述步骤后，您已经为 DolphinScheduler 创建一个新数据库，现在你可以通过快速的 Shell 脚本来初始化数据库

```shell
sh tools/bin/upgrade-schema.sh
```



7.启动dolphinscheduler

在/software/dolphinscheduler_3.0.0-beta-1/apache-dolphinscheduler-3.0.0-beta-1-bin/bin目录下执行

```
sh ./install.sh
```



## 升级dolphinscheduler

#### 准备工作

##### 备份上一版本文件和数据库

为了防止操作错误导致数据丢失，建议升级之前备份数据，备份方法请结合你数据库的情况来定

```
--备份数据库
/workspace/postgres/bin/pg_dump -h 172.16.2.120 -p 5432 -U postgres runoobdb > /workspace/postgres/runoobdb.sql

--还原首先要准备一个空库，如果想用之前的库名字，需要先删除之前的库，再新建一个空库，但是PG有时候会提示有会话连接，不能删除，这时候断开PG会话，再删除该库，注意需要在别的库设为活动对象的时候，再执行删库语句删除之前的数据库。
--断开pg会话  /*there is 1 other session using the database.*/
 SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='runoobdb';
 -- 删除数据库
DROP DATABASE runoobdb;

--linux还原数据库
/workspace/postgres/bin/psql -h 172.16.2.120 -p 5432 -U postgres -d runoobdb -f /workspace/postgres/runoobdb.sql
```

##### 下载新版本的安装包

在[下载](https://dolphinscheduler.apache.org/zh-cn/download/download.html)页面下载最新版本的二进制安装包，并将二进制包放到与当前 dolphinscheduler 服务不一样的路径中，以下升级操作都需要在新版本的目录进行。

#### 升级步骤

##### 停止 dolphinscheduler 所有服务

根据你部署方式停止 dolphinscheduler 的所有服务，如果你是通过 [集群部署](https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/guide/installation/cluster.html) 来部署你的 dolphinscheduler 的话，可以通过 `bin/stop-all.sh` 停止全部服务。

##### 数据库升级

修改 上面 2 中的所有`dolphinscheduler_env.sh`文件 中的配置（{user}和{password}改成你数据库的用户名和密码），然后运行升级脚本。

```
# Database related configuration, set database type, username and password
export DATABASE=${DATABASE:-postgresql}
export SPRING_PROFILES_ACTIVE=${DATABASE}
export SPRING_DATASOURCE_DRIVER_CLASS_NAME=org.postgresql.Driver
export SPRING_DATASOURCE_URL=jdbc:postgresql://2-120:5432/dolphinscheduler_3_0_0
export SPRING_DATASOURCE_USERNAME=postgres
export SPRING_DATASOURCE_PASSWORD=postgres
```

执行数据库升级脚本：`sh ./tools/bin/upgrade-schema.sh`

#### 服务升级

##### 修改 `bin/env/install_config.conf` 配置内容

##### 其他配置文件等同于上述的2、3、4、5步骤 

##### 然后运行命令 `sh ./install.sh` 重启全部服务。
