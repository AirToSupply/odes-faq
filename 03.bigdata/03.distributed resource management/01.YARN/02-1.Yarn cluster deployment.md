# 一.部署规划

​		理论上Yarn集群可以部署在任意机器上，生产实践中，通常把NodeManager和DataNode部署在同一台机器上。主要是原因是：**有数据的地方就有可能产生计算，移动程序的成本比移动数据的成本低**。

​		我们线下有三台预发布服务器，大致分配如下：

|                  | 172.16.10.72（bigdata02） | 172.16.10.205（bigdata03） | 172.16.10.206（bigdata04） |
| ---------------- | ------------------------- | -------------------------- | -------------------------- |
| ResourceManager  |                           | √                          |                            |
| NodeManager      | √                         | √                          | √                          |
| JobHistoryServer |                           |                            | √                          |



# 二.集群部署

【步骤-1】：修改mapred-site.xml文件。

```xml
<configuration>
  <!-- mr程序默认运行方式:yarn-集群模式,local-本地模式  -->
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

  <!-- jobhistory 服务配置 -->
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>bigdata04:10020</value>
  </property>
  <!-- jobhistory web ui 访问端口 -->
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>bigdata04:19888</value>
  </property>

  <!--MR App Master环境变量 -->
  <property>
    <name>yarn.app.mapreduce.am.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
  <!--MR MapTask环境变量 -->
  <property>
    <name>mapreduce.map.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
  <!--MR ReduceTask环境变量 -->
  <property>
    <name>mapreduce.reduce.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
</configuration>
```

【步骤-2】修改yarn-site.xml文件。

```xml
<configuration>
  <!-- NodeManager上运行的附属服务。 这里需要配置成mapreduce_shuffle才可运行MR程序 -->
  <property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
  </property>

  <!-- yarn集群主角色RM运行机器 -->
  <property>
     <name>yarn.resourcemanager.hostname</name>
     <value>bigdata03</value>
  </property>

  <!-- 容器虚拟内存与物理内存之间的比率，默认：2.1 -->
  <property>
     <name>yarn.nodemanager.vmem-pmem-ratio</name>
     <value>2.1</value>
  </property>

  <!-- 开启yarn日志聚集功能，收集每个容器的日志集中存储在一个地方，默认：false -->
  <property>
     <name>yarn.log-aggregation-enable</name>
     <value>true</value>
  </property>
  <!-- 日志保留时间设置为一个月 -->
  <property>
     <name>yarn.log-aggregation.retain-seconds</name>
     <value>2592000</value>
  </property>
  <property>
     <name>yarn.log.server.url</name>
     <value>http://bigdata04:19888/jobhistory/logs</value>
  </property>
</configuration>
```

【步骤3】启动Yarn集群，检查RM和NM是否启动成功。

```shell
${HADOOP_HOME}/sbin/start-yarn.sh
```

【步骤4】启动JobHistoryServer历史服务器。

```shell
${HADOOP_HOME}/sbin/mr-jobhistory-daemon.sh start historyserver
```

【步骤5】运行测试用例。

​		向Yarn集群提交一个蒙特卡略计算圆周率的MapReducer程序。

```shell
yarn jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar pi 2 2 
```

​		日志如下：

```shell
Number of Maps  = 2
Samples per Map = 2
Wrote input for Map #0
Wrote input for Map #1
Starting Job
22/02/22 16:11:14 INFO client.RMProxy: Connecting to ResourceManager at bigdata03/172.16.10.205:8032
22/02/22 16:11:14 INFO input.FileInputFormat: Total input files to process : 2
22/02/22 16:11:15 INFO mapreduce.JobSubmitter: number of splits:2
22/02/22 16:11:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1645515567477_0001
22/02/22 16:11:15 INFO conf.Configuration: resource-types.xml not found
22/02/22 16:11:15 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
22/02/22 16:11:15 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
22/02/22 16:11:15 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
22/02/22 16:11:15 INFO impl.YarnClientImpl: Submitted application application_1645515567477_0001
22/02/22 16:11:15 INFO mapreduce.Job: The url to track the job: http://bigdata03:8088/proxy/application_1645515567477_0001/
22/02/22 16:11:15 INFO mapreduce.Job: Running job: job_1645515567477_0001
22/02/22 16:11:20 INFO mapreduce.Job: Job job_1645515567477_0001 running in uber mode : false
22/02/22 16:11:20 INFO mapreduce.Job:  map 0% reduce 0%
22/02/22 16:11:23 INFO mapreduce.Job:  map 100% reduce 0%
22/02/22 16:11:28 INFO mapreduce.Job:  map 100% reduce 100%
22/02/22 16:11:29 INFO mapreduce.Job: Job job_1645515567477_0001 completed successfully
22/02/22 16:11:29 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=50
                FILE: Number of bytes written=635598
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=512
                HDFS: Number of bytes written=215
                HDFS: Number of read operations=11
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=3
        Job Counters
                Launched map tasks=2
                Launched reduce tasks=1
                Data-local map tasks=2
                Total time spent by all maps in occupied slots (ms)=2482
                Total time spent by all reduces in occupied slots (ms)=1971
                Total time spent by all map tasks (ms)=2482
                Total time spent by all reduce tasks (ms)=1971
                Total vcore-milliseconds taken by all map tasks=2482
                Total vcore-milliseconds taken by all reduce tasks=1971
                Total megabyte-milliseconds taken by all map tasks=2541568
                Total megabyte-milliseconds taken by all reduce tasks=2018304
        Map-Reduce Framework
                Map input records=2
                Map output records=4
                Map output bytes=36
                Map output materialized bytes=56
                Input split bytes=276
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=56
                Reduce input records=4
                Reduce output records=0
                Spilled Records=8
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=99
                CPU time spent (ms)=1280
                Physical memory (bytes) snapshot=814084096
                Virtual memory (bytes) snapshot=7487442944
                Total committed heap usage (bytes)=548405248
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=236
        File Output Format Counters
                Bytes Written=97
Job Finished in 15.741 seconds
Estimated value of Pi is 4.00000000000000000000
```

​		运行成功！

【步骤6】观察程序。

​		正在运行的应用程序，可以通过Yarn WebUI进行实时观察，服务API为http://172.16.10.205:8088 ，默认端口为8088。

​		运行终止的应用程序，可以通过JobHistoryServer进行查看，服务API为http://172.16.10.206:19888/jobhistory/ ，默认端口为19888。

# 三.重启机制

## 1.配置方式

​		重启机制不是让ResourceManage自动重启的意思，重启机制的场景是：**使得RM在重启时能够使用Yarn集群正常工作，并且出现失败时尽可能让用户无感知**。

​		我们可以考虑这样的场景，Yarn集群可能有正在RUNNING的应用程序，一旦ResourceManage挂掉之后再次重启，之前所有正在RUNNING的应用程序信息将全部丢失，同时这些程序也都会自动失败，甚至其他任何状态的应用程序信息也都会丢失。通过WebUI观察不到任何状态的应用程序。为了让ResourceManage挂掉之后之前RUNNING的应用程序能够继续程序，尽量做到用户无感知，需要重启机制来保障。在Hadoop2.6版本提供了**Work-preserving RM restart**（保存工作的RM重启）机制来进行支持。需要注意的是：**重启机制不可能解决单点故障问题**。

​		该机制中，RM会记录下Container整个生命周期所涉及的数据，包括应用程序运行的相关数据，资源申请状况，队列资源使用状况等。

​		当RM重启之后，会读取之前存储的关于应用程序的运行状态数据，同时发送re-sync命令。NM在接受重新同步命令之后并不会杀死正在运行的Containers，而是继续运行Containers中的任务，同时将Containers的运行状态发送给RM。然后RM根据自己所掌握的数据重构Containers实际和相关应用程序运行状态。

​		RM重启机制的本质是将RM内部状态信息写入外部存储介质中，它的实现抽象是RMStateStore，Yarn对RMStateStore提供几种实例，例如：FileSystemRMStateStore，ZKRMStateStore，MemoryRMStateStore等。

​		一般生产上会使用ZKRMStateStore，RM会将状态信息存储在ZK的/rmstore/ZKRMStateRoot下。主要保存了RM资源预留信息，应用信息，应用的Token信息，RM版本等。

​		需要在yarn-site.xml文件中配置如下参数：

```xml
  <property>
     <name>hadoop.zk.address</name>
     <value>bigdata02:2181,bigdata03:2181,bigdata04:2181</value>
  </property>
  <property>
     <name>yarn.resourcemanager.recovery.enabled</name>
     <value>true</value>
  </property>
  <property>
     <name>yarn.resourcemanager.store.class</name>
     <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>

<!--ZK保存的已完成任务的最大数量-->
<property>
  <name>yarn.resourcemanager.state-store.max-completed-applications</name>
  <value>100</value>
</property>

<!--RM内存中保存的已完成任务的最大数量，调整该参数主要是为了RM内存与ZK中保存的任务信息和数量一致-->
<property>
  <name>yarn.resourcemanager.max-completed-applications</name>
  <value>100</value>
</property>

<!-- 定义了ZK的ZNode节点所能存储的最大数据量，以字节为单位，默认是1048576字节，也就是1MB，这里改成4MB -->
<property>
  <name>yarn.resourcemanager.zk-max-znode-size.bytes</name>
  <value>4194304</value>
</property>
```

​		关于重启机制的采坑可以参照这篇博客：  https://blog.csdn.net/github_32521685/article/details/89953788

​		个人的建议是：如果Yarn不是高可用的话，建议开启重启机制。Yarn如果不是高可用的话，不强制配置。

## 2.疑难问题

（1）ZooKeeper节点数据量限制引起的Hadoop YARN ResourceManager崩溃原因分析。

​         https://cloud.tencent.com/developer/article/1491079

（2）解决任务重试状态数据超过`1M`的问题。

​         https://blog.csdn.net/github_32521685/article/details/89953788

（3）状态如何存储到文件系统？

​          https://blog.csdn.net/wangkai_123456/article/details/88184790

（4）如何配置YARN集群重启时的作业自动恢复？

​          https://blog.csdn.net/nazeniwaresakini/article/details/106712757

（5）Yarn如何增加ResourceManager和NodeManager JVM进程内存大小

​          编辑yarn-env.sh配置文件：

```shell
export YARN_RESOURCEMANAGER_HEAPSIZE=2048
export YARN_NODEMANAGER_HEAPSIZE=2048
```