# Yarn 集成 JuiceFS 单机部署

## 一、部署步骤

注意： 配置Yarn 时，已提交 hadoop 相关环境变量，例如${HADOOP_HOME}等。

【步骤-1】：修改mapred-site.xml文件。

在 ${HADOOP_HOME}/etc/hadoop 路径下配置 mapred-site.xml 文件，配置项如下：

```xml
<configuration>
  <!-- mr程序默认运行方式:yarn-集群模式,local-本地模式  -->
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

  <!-- jobhistory 服务配置 -->
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>node111:10020</value>
  </property>
  <!-- jobhistory web ui 访问端口 -->
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>node111:19888</value>
  </property>

  <property>
    <name>mapreduce.application.classpath</name>
    <value>$HADOOP_HOME/share/hadoop/mapreduce/*,$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>
  </property> 

  <!--MR App Master环境变量 -->
  <property>
    <name>yarn.app.mapreduce.am.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
  <!--MR MapTask环境变量 -->
  <property>
    <name>mapreduce.map.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
  <!--MR ReduceTask环境变量 -->
  <property>
    <name>mapreduce.reduce.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
    
    <!-- 环形缓冲区大小，默认  100m -->
  <property>
    <name>mapreduce.task.io.sort.mb</name>
    <value>200</value>
  </property>
  <!-- merge合并次数，默认 10个   -->
  <property>
    <name>mapreduce.task.io.sort.factor</name>
    <value>20</value>
  </property>
  <!-- maptask 内 存 ， 默 认    1g；        maptask 堆 内 存 大 小 默 认 和 该 值 大 小 一 致 mapreduce.map.java.opts -->
  <property>
    <name>mapreduce.map.memory.mb</name>
    <value>2048</value>
  </property>
  <!-- 每个  Reduce去 Map中拉取数据的并行数。默认值是 5 -->
  <property>
    <name>mapreduce.reduce.shuffle.parallelcopies</name>
    <value>10</value> 
  </property>
  <!-- Buffer大小占  Reduce可用内存的比例，默认值 0.7 -->
  <property>
    <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
    <value>0.80</value>
  </property>
  <!-- Buffer中的数据达到多少比例开始写入磁盘，默认值 0.66。   -->
  <property>
    <name>mapreduce.reduce.shuffle.merge.percent</name>
    <value>0.75</value>
  </property>
  <!-- reducetask 内存，默认   1g；reducetask 堆内存大小默认和该值大小一致 mapreduce.reduce.java.opts -->
  <property>
    <name>mapreduce.reduce.memory.mb</name>
    <value>2048</value>
  </property>
  <!-- 如果程序在规定的默认  10分钟内没有读到数据，将强制超时退出   -->
  <property>
    <name>mapreduce.task.timeout</name>
    <value>600000</value>
  </property>
  <!-- 压缩配置 -->
    <property>
    <name>mapreduce.map.output.compress</name>
    <value>true</value>
  </property>
  <property>
    <name>mapreduce.map.output.compress.codec</name>
    <value>org.apache.hadoop.io.compress.SnappyCodec</value>
  </property>
  <property>
    <name>mapreduce.output.fileoutputformat.compress</name>
    <value>true</value>
  </property>
  <property>
    <name>mapreduce.output.fileoutputformat.compress.codec</name>
    <!--<value>org.apache.hadoop.io.compress.SnappyCodec</value>-->
    <value>org.apache.hadoop.io.compress.GzipCodec</value>
  </property>
  <property>
    <name>mapreduce.output.fileoutputformat.compress.type</name>
    <value>BLOCK</value>
  </property>
  <!-- 小文件配置 -->
  <!-- 开启  uber模式，默认关闭   -->
  <property>
    <name>mapreduce.job.ubertask.enable</name>
    <value>true</value>
  </property> 
</configuration>
```



【步骤-2】修改yarn-site.xml文件。

在 ${HADOOP_HOME}/etc/hadoop 路径下配置 yarn-site.xml 文件，配置项如下：

```xml
<configuration>
<!-- Site specific YARN configuration properties -->

  <!-- NodeManager上运行的附属服务。 这里需要配置成mapreduce_shuffle才可运行MR程序 -->
  <property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
  </property>

  <!-- 如果非HA 需要如下指定 -->
 <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>node111</value>
  </property>

  <!-- 开启自动故障转移 -->
  <property>
     <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
     <value>true</value>
  </property> 
  <!-- zk集群地址 -->
  <property>
     <name>hadoop.zk.address</name>
     <value>node111:2181</value>
  </property>
    
  <!-- 开启RM重启状态恢复机制 -->
  <property>
     <name>yarn.resourcemanager.recovery.enabled</name>
     <value>true</value>
  </property>
  <!-- 状态存储采用ZK -->
  <property>
     <name>yarn.resourcemanager.store.class</name>
     <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>

  <!--ZK保存的已完成任务的最大数量-->
  <property>
    <name>yarn.resourcemanager.state-store.max-completed-applications</name>
    <value>100</value>
  </property>

  <!--RM内存中保存的已完成任务的最大数量，调整该参数主要是为了RM内存与ZK中保存的任务信息和数量一致-->
  <property>
    <name>yarn.resourcemanager.max-completed-applications</name>
    <value>100</value>
  </property>

  <!-- 定义了ZK的ZNode节点所能存储的最大数据量，以字节为单位，默认是1048576字节，也就是1MB，这里改成4MB -->
  <property>
    <name>yarn.resourcemanager.zk-max-znode-size.bytes</name>
    <value>4194304</value>
  </property>

  <!-- cpu -->
  <!-- 确定如何将物理核zhau转换为虚拟核的乘数 -->
  <property>
     <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
     <value>1</value>
  </property>
  <property>
     <name>yarn.nodemanager.resource.cpu-vcores</name>
     <value>40</value>
  </property>
  <property>
     <name>yarn.scheduler.minimum-allocation-vcores</name>
     <value>1</value>
  </property>
  <property>
     <name>yarn.scheduler.maximum-allocation-vcores</name>
     <value>4</value>
  </property>

  <!-- memory -->
  <!-- 容器虚拟内存与物理内存之间的比率，默认：2.1 -->
  <property>
     <name>yarn.nodemanager.vmem-pmem-ratio</name>
     <value>2.1</value>
  </property>
  <!-- 96G -->
  <property>
     <name>yarn.nodemanager.resource.memory-mb</name>
     <value>131072</value>
  </property>
  <property>
     <name>yarn.scheduler.minimum-allocation-mb</name>
     <value>1024</value>
  </property>
  <!-- 12G -->
  <property>
     <name>yarn.scheduler.maximum-allocation-mb</name>
     <value>12288</value>
  </property>
    
  <property>
     <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
     <value>100.0</value>
 </property>   
   <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
 </property>
 <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
  <property>
     <name>yarn.nodemanager.pmem-check-enabled</name>
     <value>false</value>
  </property>
  <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
  <property>
      <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value>
  </property>
    
  <!-- 容器虚拟内存与物理内存之间的比率，默认：2.1 -->
  <property>
     <name>yarn.nodemanager.vmem-pmem-ratio</name>
     <value>2.1</value>
  </property>

  <!-- 开启yarn日志聚集功能，收集每个容器的日志集中存储在一个地方，默认：false -->
  <property>
     <name>yarn.log-aggregation-enable</name>
     <value>true</value>
  </property>
  <!-- 日志保留时间设置为一个月 -->
  <property>
     <name>yarn.log-aggregation.retain-seconds</name>
     <value>2592000</value>
  </property>
  <property>
     <name>yarn.log.server.url</name>
     <value>http://node111:19888/jobhistory/logs</value>
  </property>
</configuration>
```




【步骤-3】:修改capacity-scheduler.xml文件。


在${HADOOP_HOME}/etc/hadoop路径下配置 capacity-scheduler.xml 文件，配置项如下：

```xml
  <property>
    <name>yarn.scheduler.capacity.resource-calculator</name>
    <!-- <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value> -->
    <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
    <description>
      The ResourceCalculator implementation to be used to compare
      Resources in the scheduler.
      The default i.e. DefaultResourceCalculator only uses Memory while
      DominantResourceCalculator uses dominant-resource to compare
      multi-dimensional resources such as Memory, CPU etc.
    </description>
  </property>
```



【步骤-4】：创建目录

创建 Yarn 数据目录并授权 zhongtai 用户

```shell
[zhongtai@node111 ~]$ cd /data
[zhongtai@node111 data]$ sudo mkdir  -p /data/hadoop-3.2.4/pid
[zhongtai@node111 data]$ sudo mkdir  -p /data/hadoop-3.2.4/log
[zhongtai@node111 data]$ sudo mkdir  -p /data/hadoop-3.2.4/yarn/timeline
[zhongtai@node111 data]$ sudo chown -R zhongtai:zhongtai hadoop-3.2.4/
[zhongtai@node111 data]$ ll
total 12
drwxr-xr-x.  2 root     root        6 May  8 09:55 docker
drwxr-xr-x.  5 zhongtai zhongtai   56 May 11 13:57 hadoop-3.2.4
drwxrwxrwx. 25 root     root     8192 May 10 13:49 jenkins_mount
drwxr-xr-x.  5 mysql    mysql      56 May  8 11:56 mysql-8.0.22
drwxr-xr-x.  4 zhongtai zhongtai   41 May  9 14:45 redis-7.0.8
```



【步骤-5】：配置 NodeManager 节点

在${HADOOP_HOME}/etc/hadoop路径下配置 workers 文件，配置项如下：

```sh 
> vim workers

node111
```



【步骤-6】：hadoop改log、pid、jvm的配置

在${HADOOP_HOME}/etc/hadoop路径下配置hadoop-env.sh文件，配置项如下：

```sh
export JAVA_HOME=/opt/jdk1.8.0_201

# hadoop改log、pid、jvm的配置
export HADOOP_PID_DIR=/data/hadoop-3.2.4/pid
export HADOOP_MAPRED_PID_DIR=/data/hadoop-3.2.4/pid
export HADOOP_LOG_DIR=/data/hadoop-3.2.4/log

# JVM
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx4096m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx4096m"
export HDFS_JOURNALNODE_OPTS="-Xmx4096m"
```




在${HADOOP_HOME}/etc/hadoop路径下配置 yarn-env.sh 文件，配置项如下：

```sh
export YARN_PID_DIR=/data/hadoop-3.2.4/pid

# JVM
export YARN_RESOURCEMANAGER_OPTS="-Drm.audit.logger=INFO,RMAUDIT -Xmx4096m"
export YARN_NODEMANAGER_OPTS="-Dnm.audit.logger=INFO,NMAUDIT -Xmx4096m"
```



mapred-env.sh
在${HADOOP_HOME}/etc/hadoop路径下配置 mapred-env.sh 文件，配置项如下：

```sh
export HADOOP_MAPRED_PID_DIR=/data/hadoop-3.2.4/pid
# JVM
export MAPRED_HISTORYSERVER_OPTS="-Xmx4096m"
```







## 二、运维命令


【步骤1】启动Yarn集群，检查RM和NM是否启动成功。

```shell
${HADOOP_HOME}/sbin/start-yarn.sh

${HADOOP_HOME}/sbin/stop-yarn.sh
```



【步骤2】启动JobHistoryServer历史服务器。

```shell
${HADOOP_HOME}/sbin/mr-jobhistory-daemon.sh start historyserver

${HADOOP_HOME}/sbin/mr-jobhistory-daemon.sh stop historyserver
```



【步骤3】运行测试用例。

​		向Yarn集群提交一个蒙特卡略计算圆周率的MapReducer程序。

```shell
yarn jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar pi 2 2 
```

​		日志如下：

```shell
Number of Maps  = 2
Samples per Map = 2
Wrote input for Map #0
Wrote input for Map #1
Starting Job
22/02/22 16:11:14 INFO client.RMProxy: Connecting to ResourceManager at bigdata03/172.16.10.205:8032
22/02/22 16:11:14 INFO input.FileInputFormat: Total input files to process : 2
22/02/22 16:11:15 INFO mapreduce.JobSubmitter: number of splits:2
22/02/22 16:11:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1645515567477_0001
22/02/22 16:11:15 INFO conf.Configuration: resource-types.xml not found
22/02/22 16:11:15 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
22/02/22 16:11:15 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
22/02/22 16:11:15 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
22/02/22 16:11:15 INFO impl.YarnClientImpl: Submitted application application_1645515567477_0001
22/02/22 16:11:15 INFO mapreduce.Job: The url to track the job: http://bigdata03:8088/proxy/application_1645515567477_0001/
22/02/22 16:11:15 INFO mapreduce.Job: Running job: job_1645515567477_0001
22/02/22 16:11:20 INFO mapreduce.Job: Job job_1645515567477_0001 running in uber mode : false
22/02/22 16:11:20 INFO mapreduce.Job:  map 0% reduce 0%
22/02/22 16:11:23 INFO mapreduce.Job:  map 100% reduce 0%
22/02/22 16:11:28 INFO mapreduce.Job:  map 100% reduce 100%
22/02/22 16:11:29 INFO mapreduce.Job: Job job_1645515567477_0001 completed successfully
22/02/22 16:11:29 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=50
                FILE: Number of bytes written=635598
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=512
                HDFS: Number of bytes written=215
                HDFS: Number of read operations=11
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=3
        Job Counters
                Launched map tasks=2
                Launched reduce tasks=1
                Data-local map tasks=2
                Total time spent by all maps in occupied slots (ms)=2482
                Total time spent by all reduces in occupied slots (ms)=1971
                Total time spent by all map tasks (ms)=2482
                Total time spent by all reduce tasks (ms)=1971
                Total vcore-milliseconds taken by all map tasks=2482
                Total vcore-milliseconds taken by all reduce tasks=1971
                Total megabyte-milliseconds taken by all map tasks=2541568
                Total megabyte-milliseconds taken by all reduce tasks=2018304
        Map-Reduce Framework
                Map input records=2
                Map output records=4
                Map output bytes=36
                Map output materialized bytes=56
                Input split bytes=276
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=56
                Reduce input records=4
                Reduce output records=0
                Spilled Records=8
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=99
                CPU time spent (ms)=1280
                Physical memory (bytes) snapshot=814084096
                Virtual memory (bytes) snapshot=7487442944
                Total committed heap usage (bytes)=548405248
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=236
        File Output Format Counters
                Bytes Written=97
Job Finished in 15.741 seconds
Estimated value of Pi is 4.00000000000000000000
```

​		运行成功！

【步骤6】观察程序。

​		正在运行的应用程序，可以通过Yarn WebUI进行实时观察，服务API为http://172.16.3.111:8088 ，默认端口为8088。

​		运行终止的应用程序，可以通过JobHistoryServer进行查看，服务API为http://172.16.3.111:19888/jobhistory/ ，默认端口为19888。



## FAQ:

（1）ZooKeeper节点数据量限制引起的Hadoop YARN ResourceManager崩溃原因分析。

https://cloud.tencent.com/developer/article/1491079



（2）解决任务重试状态数据超过`1M`的问题。

https://blog.csdn.net/github_32521685/article/details/89953788



（3）状态如何存储到文件系统？

https://blog.csdn.net/wangkai_123456/article/details/88184790



（4）如何配置YARN集群重启时的作业自动恢复？

https://blog.csdn.net/nazeniwaresakini/article/details/106712757



（5）Yarn如何增加ResourceManager和NodeManager JVM进程内存大小

编辑yarn-env.sh配置文件：

```shell
export YARN_RESOURCEMANAGER_HEAPSIZE=2048
export YARN_NODEMANAGER_HEAPSIZE=2048
```



（6）Yarn WebUI Application 里面打不开Node和Logs，错误图如下：

![img](https://static.dingtalk.com/media/lQLPJv-oiXTeBiLNAtnNBYaw3QNHbkT2qjQETFJQHsC4AA_1414_729.png)



因为这里用了主机名，所以打开不了，需要在window上面配置ip映射。

解决方法：在C:\Windows\System32\drivers\etc 下面的hosts，添加对应的ip映射，然后保存就可以了。

查看链接：[hadoop Application 里面打不开Node和Logs](https://blog.csdn.net/john_deng2/article/details/103547244)
