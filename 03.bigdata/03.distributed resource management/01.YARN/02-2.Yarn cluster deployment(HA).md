# 一.高可用架构原理

​		在Hadoop2.4之前，ResourceManager是Yarn集群中的SPOF（Single Point of Failure，单点故障），为了解决这个问题，Yarn设计了一套Active/Standby模式的ResourceManager HA架构。

​		Hadoop官方推荐基于Zk集群实现Yarn HA。RM可以选择嵌入基于Zookeeper的ActiveStandbyElector来实现自动故障转移。需要注意的是，Yarn的自动故障转移不需要像HDFS那样运行单独的ZKFC守护进程，因为ActiveStandbyElector是一个嵌入在RM中充当故障检测器和Leader选举的线程。具体的原理如下：

（1）**创建所节点**：在Zookeeper上会创建一个叫做ActiveStandbyElectorLock的锁节点，所有RM在启动时会去竞争写这个临时的Lock节点，而ZK能保证只有一个RM创建成功，创建成功的RM就切换为Active状态，没有成功的RM则切换为Standby状态。

（2）**注册Watcher监听**：Standby状态的RM向ActiveStandbyElectorLock节点注册一个节点变更的Watcher监听，利用临时节点的特性（会话结束节点自动消失），能够快速感知到Active状态的RM运行情况。

（3）**准备切换**：当Active状态的RM出现故障（如宕机或网络中断），其在ZK上创建的Lock节点随之被删除，这时其他各个Standby状态的RM都会受到ZK服务端的Watcher事件通知，然后开始竞争写Lock子节点，创建成功的变成Active状态，其他的则是Standby状态。

（4）**隔离**（Fencing）：在分布式环境中，机器经常出现假死的情况（如GC耗时过长，网络中断或CPU负载过高）而无法正常对外进行及时响应。如果有一个处于Active状态的RM出现假死，其他的RM刚选举出来新的Active状态的RM，这时假死的RM又恢复正常，还认为自己是Active状态，这就是分布式系统的脑裂现象，即存在多个处于Active状态的RM，可以使用隔离机制来解决此类问题。

​		Yarn的Fencing机制是借助ZK数据节点的**ACL权限控制**来实现不同RM之间的隔离。创建的根ZNode必须携带ZK的ACL信息，目的是为了独占该节点，以防止其他RM对该ZNode进行更新。借助这个机制假死之后的RM会视图去更新Zk的相关信息，但发现没有权限去更新节点数据，就把自己切换为Standby状态。

# 二.部署规划

​         线下有三台预发布服务器，大致分配如下：

|                  | 172.16.10.72（bigdata02） | 172.16.10.205（bigdata03） | 172.16.10.206（bigdata04） |
| ---------------- | ------------------------- | -------------------------- | -------------------------- |
| ResourceManager  | √                         | √                          |                            |
| NodeManager      | √                         | √                          | √                          |
| JobHistoryServer |                           |                            | √                          |

# 三.集群部署

​		和非高可用模式配置相比，需要在yarn-site.xml文件中删除如下配置：

```xml
 <property>
     <name>yarn.resourcemanager.hostname</name>
     <value>bigdata03</value>
  </property>
```

​		增加如下配置：

```xml
  <!-- 启用RM HA -->
  <property>
     <name>yarn.resourcemanager.ha.enabled</name>
     <value>true</value>
  </property>
  <!-- RM HA集群标识 -->
  <property>
     <name>yarn.resourcemanager.cluster-id</name>
     <value>cluster</value>
  </property>
  <!-- RM HA集群中各RM的逻辑标识 -->
  <property>
     <name>yarn.resourcemanager.ha.rm-ids</name>
     <value>rm1,rm2</value>
  </property>
  <!-- rm1运行主机 -->
  <property>
     <name>yarn.resourcemanager.hostname.rm1</name>
     <value>bigdata02</value>
  </property>
  <!-- rm2运行主机 -->
  <property>
     <name>yarn.resourcemanager.hostname.rm2</name>
     <value>bigdata03</value>
  </property>
  <!-- rm1 WebUI地址 -->
  <property>
     <name>yarn.resourcemanager.webapp.address.rm1</name>
     <value>bigdata02:8088</value>
  </property>
  <!-- rm2 WebUI地址 -->
  <property>
     <name>yarn.resourcemanager.webapp.address.rm2</name>
     <value>bigdata03:8088</value>
  </property>
  <!-- 开启自动故障转移 -->
  <property>
     <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
     <value>true</value>
  </property>
  <!-- zk集群地址 -->
  <property>
     <name>hadoop.zk.address</name>
     <value>bigdata02:2181,bigdata03:2181,bigdata04:2181</value>
  </property>
  
  <!-- 开启RM重启状态恢复机制 -->
  <property>
     <name>yarn.resourcemanager.recovery.enabled</name>
     <value>true</value>
  </property>
  <!-- 状态存储采用ZK -->
  <property>
     <name>yarn.resourcemanager.store.class</name>
     <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>
  <!--ZK保存的已完成任务的最大数量-->
  <property>
    <name>yarn.resourcemanager.state-store.max-completed-applications</name>
    <value>100</value>
  </property>
  <!--RM内存中保存的已完成任务的最大数量，调整该参数主要是为了RM内存与ZK中保存的任务信息和数量一致-->
  <property>
    <name>yarn.resourcemanager.max-completed-applications</name>
    <value>100</value>
  </property>
  <!-- 定义了ZK的ZNode节点所能存储的最大数据量，以字节为单位，默认是1048576字节，也就是1MB，这里改成4MB -->
  <property>
    <name>yarn.resourcemanager.zk-max-znode-size.bytes</name>
    <value>4194304</value>
  </property>
  
  <!-- NodeManager上运行的附属服务。 这里需要配置成mapreduce_shuffle才可运行MR程序 -->
  <property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
  </property>
  
  <!-- 容器虚拟内存与物理内存之间的比率，默认：2.1 -->
  <property>
     <name>yarn.nodemanager.vmem-pmem-ratio</name>
     <value>2.1</value>
  </property>

  <!-- 开启yarn日志聚集功能，收集每个容器的日志集中存储在一个地方，默认：false -->
  <property>
     <name>yarn.log-aggregation-enable</name>
     <value>true</value>
  </property>
  <!-- 日志保留时间设置为一个月 -->
  <property>
     <name>yarn.log-aggregation.retain-seconds</name>
     <value>2592000</value>
  </property>
  <property>
     <name>yarn.log.server.url</name>
     <value>http://bigdata04:19888/jobhistory/logs</value>
  </property>
```

​		修改完毕之后，通过如下命令启动集群：

```shell
${HADOOP_HOME}/sbin/start-yarn.sh
```

​		启动之后检查两台机器上是否都启动RM，同时可以通过如下命令查看这两机器的active和standby分别是哪个节点。

```shell
yarn rmadmin -getAllServiceState
```

​	     输出结果如下：

```shell
bigdata02:8033                                     standby
bigdata03:8033                                     active
```

​		这里注意的是，active和standby两个节点均可以通过默认的8088端口去访问Yarn WebUI控制台，只不过当访问standby节点的WebUI时会自动跳转active节点的WebUI。