### 1）、Spark Doris Connector

Spark Doris Connector 可以支持通过 Spark 读取 Doris 中存储的数据，也支持通过Spark写入数据到Doris。

#### 1、版本兼容

| Connector | Spark | Doris  | Java | Scala |
| --------- | ----- | ------ | ---- | ----- |
| 1.0.0     | 2.x   | 0.12+  | 8    | 2.11  |
| 1.0.0     | 3.x   | 0.12.+ | 8    | 2.12  |

#### 2、编译与安装

在 `extension/spark-doris-connector/` 源码目录下执行：

注1：这里如果你没有整体编译过 doris 源码，需要首先编译一次 Doris 源码，不然会出现 thrift 命令找不到的情况，需要到 `incubator-doris` 目录下执行 `sh build.sh`。

注2：建议在 doris 的 docker 编译环境 `apache/incubator-doris:build-env-1.2` 下进行编译，因为 1.3 下面的JDK 版本默认是JDK11，会存在编译问题。所以从 build-env-1.3.1 的docker镜像起注意切换JDK版本。

```
sh build.sh 3  ## spark 3.x版本，默认是3.1.2
sh build.sh 2  ## spark 2.x版本，默认是2.3.4
```

注：我们视博环境spark是3.1.1，所以要在`pom_3.0.xml`中修改spark版本。

编译成功后，会在 `output/` 目录下生成文件 `doris-spark-1.0.0-spark-3.1.2_2.12.jar`。将此文件复制到 `Spark` 的 `ClassPath` 中即可使用 `Spark-Doris-Connector`。

#### 3、spark3.x版本编译可能会遇到的问题

sh build.sh 时如果报错

```
[ERROR] /home/deploy/apache-doris-0.15.0-incubating-src/extension/spark-doris-connector/src/main/scala/org/apache/doris/spark/sql/DorisSourceProvider.scala:24: error: object v2 is not a member of package org.apache.spark.sql.sources
[ERROR] import org.apache.spark.sql.sources.v2.writer.streaming.StreamWriter
[ERROR]                                     ^
[ERROR] /home/deploy/apache-doris-0.15.0-incubating-src/extension/spark-doris-connector/src/main/scala/org/apache/doris/spark/sql/DorisSourceProvider.scala:25: error: object v2 is not a member of package org.apache.spark.sql.sources
[ERROR] import org.apache.spark.sql.sources.v2.{DataSourceOptions, StreamWriteSupport}
[ERROR]                                     ^
[ERROR] /home/deploy/apache-doris-0.15.0-incubating-src/extension/spark-doris-connector/src/main/scala/org/apache/doris/spark/sql/DorisSourceProvider.scala:37: error: not found: type StreamWriteSupport
[ERROR] private[sql] class DorisSourceProvider extends DataSourceRegister with RelationProvider with CreatableRelationProvider with StreamWriteSupport with Logging {
......
```

原因：v2 包不在 spark3 中

参考连接：

[1]: SparkDorisConnector构建失败，spark3	"https://github.com/apache/incubator-doris/issues/7363"

问题详解：部分 Spark 连接器代码是使用“Spark DatasourcesV2”API 实现的。但是，“Spark DatasourcesV2”API 仅存在于 spark 2.3.x/2.4.x 中，不存在于 spark 2.1.x/2.2.x/3.x 中。在 spark 3.x 之后，该 API 似乎已被删除。

参考链接：

[2]: Spark连接器支持多个Spark版本：2.1.x/2.3.x/3.x	"https://github.com/apache/incubator-doris/pull/6956"

在Doris版本中0.15.0版本支持spark读写Doris，但在docker编译的时候存在上述问题，因此手动编译了编译最新主干版本代码，解决了此问题

注：在官网编译部分中提到“`apache/incubator-doris:build-env-latest` 用于编译最新主干版本代码，会随主干版本不断更新。可以查看 `docker/README.md` 中的更新时间。”可能可以解决此问题，暂时没有尝试



### 2）、Flink Doris Connector

Flink Doris Connector 可以支持通过 Flink 读写 Doris 中存储的数据。

#### 1、版本兼容

| Connector | Flink           | Doris  | Java | Scala |
| --------- | --------------- | ------ | ---- | ----- |
| 1.0.0     | 1.11.x , 1.12.x | 0.13+  | 8    | 2.12  |
| 1.0.0     | 1.13.x          | 0.13.+ | 8    | 2.12  |

针对Flink1.13.0 版本适配问题

```
 <properties>
        <scala.version>2.12</scala.version>
        <flink.version>1.11.2</flink.version>
        <libthrift.version>0.9.3</libthrift.version>
        <arrow.version>0.15.1</arrow.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <doris.home>${basedir}/../../</doris.home>
        <doris.thirdparty>${basedir}/../../thirdparty</doris.thirdparty>
    </properties>
```

注：我们视博环境flink是1.13.1，所以要在`pom.xml`中修改flink版本。将这里的 `flink.version` 改成和Flink 集群版本一致，编译即可。

#### 2、编译与安装

在 `extension/flink-doris-connector/` 源码目录下执行：

```
sh build.sh
```

注1：这里如果你没有整体编译过 doris 源码，需要首先编译一次 Doris 源码，不然会出现 thrift 命令找不到的情况，需要到 `incubator-doris` 目录下执行 `sh build.sh`。

注2：建议在 doris 的 docker 编译环境 `apache/incubator-doris:build-env-1.2` 下进行编译，因为 1.3 下面的JDK 版本默认是JDK11，会存在编译问题。所以从 build-env-1.3.1 的docker镜像起注意切换JDK版本。

编译成功后，会在 `output/` 目录下生成文件 `doris-flink-1.0.0-flink-1.13.1_2.12.jar` 。

将此文件复制到 `Flink` 的 `ClassPath` 中即可使用 `Flink-Doris-Connector`。
