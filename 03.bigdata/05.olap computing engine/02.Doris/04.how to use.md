### 1）、简单使用

Doris 采用 MySQL 协议进行通信，用户可通过 MySQL client 或者 MySQL JDBC连接到 Doris 集群。选择 MySQL client 版本时建议采用5.1 之后的版本，因为 5.1 之前不能支持长度超过 16 个字符的用户名。

Doris支持支持单分区和复合分区两种建表方式。

#### 1、创表

##### 1.1、创建单分区表

创建一个名字为 table1 的逻辑表。分桶列为 id，桶数为 10。

建表语句如下

```sql
create table table1(
 id INT,
 name VARCHAR(32),
 age INT
)
DISTRIBUTED BY HASH(id) BUCKETS 10
PROPERTIES("replication_num" = "1");
```

##### 1.2、创建复合分区表

建立一个名字为 table2 的逻辑表。使用 event_day 列作为分区列，建立3个分区: p201706, p201707, p201708，每个分区使用 siteid 进行哈希分桶，桶数为10

建表语句如下

```sql
CREATE TABLE table2
(
    event_day DATE,
    siteid INT,
    citycode SMALLINT,
    username VARCHAR(32)
)
PARTITION BY RANGE(event_day)
(
    PARTITION p201706 VALUES LESS THAN ('2017-07-01'),
    PARTITION p201707 VALUES LESS THAN ('2017-08-01'),
    PARTITION p201708 VALUES LESS THAN ('2017-09-01')
)
DISTRIBUTED BY HASH(siteid) BUCKETS 10
PROPERTIES("replication_num" = "1");
```

#### 2、流式导入

流式导入通过 HTTP 协议向 Doris 传输数据，可以不依赖其他系统或组件直接导入本地数据。

##### 2.1、单分区表数据导入

本地文件 `table_data` 以 `,` 作为数据之间的分隔，具体内容如下：

```
1,jim,2
2,grace,4
3,tom,8
4,bush,9
5,helen,16
```

以 "label:135" 为 Label，使用本地文件 table_data 导入 table1 表。

```
curl --location-trusted -u root -H "label:135" -H "column_separator:," -T /home/zhongtai/tmp/table_data http://172.31.41.20:8030/api/ds/table1/_stream_load
```

##### 2.2、复合分区表数据导入

本地文件 `test_table2` 以 `,` 作为数据之间的分隔，具体内容如下：

```
2017-07-03,1,1,jim
2017-07-05,2,1,grace 
2017-07-12,3,2,tom
2017-07-15,4,3,bush
2017-07-12,5,3,helen
```

以 "label:129" 为 Label，使用本地文件 test_table2 导入 table2 表。

```
curl --location-trusted -u root -H "label:129" -H "column_separator:," -T /home/zhongtai/tmp/test_table2 http://172.31.41.20:8030/api/ds/table2/_stream_load
```

注1：FE_HOST 是任一 FE 所在节点 IP，8030 为 fe.conf 中的 http_port

注2：采用流式导入建议文件大小限制在 10GB 以内，过大的文件会导致失败重试代价变大

注3：每一批导入数据都需要取一个 Label，Label 最好是一个和一批数据有关的字符串，方便阅读和管理。Doris 基于 Label 保证在一个Database 内，同一批数据只可导入成功一次。失败任务的 Label 可以重用。

注4、流式导入是同步命令。命令返回成功则表示数据已经导入，返回失败表示这批数据没有导入。



### 2）、离线写入数据到Doris（Spark）

##### 1、通过spark读取hudi数据写入doris

**使用Spark Doris Connector编译的doris-spark-1.0.0-spark-3.1.2_2.12.jar**

1.1、将编译的doris-spark-1.0.0-spark-3.1.2_2.12.jar复制到 `Spark` 的 `ClassPath` 中。

1.2、在doris里创建表

1.3、代码

```scala
package com.hcdsj

import org.apache.spark.{SparkConf}
import org.apache.spark.sql.SparkSession

object DorisTest {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("KafkaToHudi").setMaster("local[4]").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    val spark = SparkSession.builder().config(conf).getOrCreate()

    val frame = spark.read.format("hudi").load("hdfs://localhost:9000/hudi/rdbms/default/db_issue_clear/ods_db_issue_clear_tb_nclear_inside_record_dispute/clean_tb")

    frame.write.format("doris").option("doris.fenodes", "localhost:8030").option("doris.table.identifier", "ds.test1").option("user", "root").option("password","").option("mode","Append").save()

    spark.stop()
  }
}
```

1.4、打包

1.5、启动spark submit运行jar 写入数据

```shell
bin/spark-submit \
    --class com.hcdsj.dorisTest \
    --jars ./jars/hudi-spark3-bundle_2.12-0.9.0.jar,./jars/doris-spark-1.0.0-spark-3.1.1_2.12.jar,./jars/guava-14.0.1.jar,./jars/guava-28.0-jre.jar,./jars/failureaccess-1.0.1.jar \
    --master spark://172.31.41.30:7077 \
    --driver-memory 8g \
    --executor-memory 8g \
    --executor-cores 8 \
    --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
/home/zhongtai/tmp/HdfsToDoris-1.0-SNAPSHOT.jar
```



##### 2、通过spark读取mysql的数据写入doris

**使用jdbc的连接方式读取MySQL数据并且写入doris**

2.1、在Doris里创建表

2.2、代码

```scala
package com.hcdsj

import org.apache.spark.SparkConf
import org.apache.spark.sql.{DataFrame, SparkSession}

object JdbcDoris {
  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("KafkaToHudi").setMaster("local[4]")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")

    val spark = SparkSession.builder().config(conf).getOrCreate()
    val df: DataFrame = spark.read.format("jdbc")
      .option("driver", "com.mysql.jdbc.Driver")
      .option("url", "jdbc:mysql://172.16.2.120:3306")
      .option("dbtable", "db_issue_clear.tb_nclear_dispute_setting")
      .option("user", "root")
      .option("password", "123456")
      .load()
 
    df.write.format("jdbc").option("url","jdbc:mysql://172.31.41.20:9030/ds").option("user","root").option("dbtable","ods_etc_project_etcobubusiness").save()

    spark.stop()

  }
}
```

2.3、打包

2.4启动spark submit运行jar 写入数据

```shell
bin/spark-submit \
    --class com.hcdsj.JdbcDoris \
    --jars ./jars/hudi-spark3-bundle_2.12-0.9.0.jar,./jars/mysql-connector-java-5.1.34.jar,./jars/doris-spark-1.0.0-spark-3.1.1_2.12.jar,./jars/guava-14.0.1.jar,./jars/guava-28.0-jre.jar,./jars/failureaccess-1.0.1.jar \
    --master spark://172.31.41.30:7077 \
    --driver-memory 8g \
    --executor-memory 8g \
    --executor-cores 8 \
    --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
/home/zhongtai/tmp/HdfsToDoris-1.0-SNAPSHOT.jar
```



### 3）、实时写入数据到Doris（Spark）

**使用doris-spark-1.0.0-spark-3.1.2_2.12.jar通过spark实时读取hudi数据写入doris**

代码

```scala
package com.hcdsj

import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

object SparkDorisTest {

  def main(args: Array[String]): Unit = {

    val sc = new SparkConf().setAppName("KafkaToHudi").setMaster("local[4]").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    val spark = SparkSession.builder().config(sc).getOrCreate()

   val frame = spark.readStream.format("hudi").load("/tmp/hudi/my_hudi_table")

   frame.writeStream.format("doris").option("doris.table.identifier", "ds.test1").option("doris.fenodes", "172.31.41.20:8030").option("user", "root").option("password","").option("checkpointLocation", "/tmp/zhongtai/sq/t1_check").option("mode","Append").start().awaitTermination()
    
    spark.stop()

  }
}

```

启动spark submit运行jar 写入数据

```shell
bin/spark-submit \
    --class com.hcdsj.SparkDorisTest \
    --jars ./jars/hudi-spark3-bundle_2.12-0.9.0.jar,./jars/doris-spark-1.0.0-spark-3.1.1_2.12.jar,./jars/guava-14.0.1.jar,./jars/guava-28.0-jre.jar,./jars/failureaccess-1.0.1.jar \
    --master spark://172.31.41.30:7077 \
    --driver-memory 8g \
    --executor-memory 8g \
    --executor-cores 8 \
    --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
/home/zhongtai/tmp/HdfsToDoris-1.0-SNAPSHOT.jar
```

注意：动态获取数据的dataframe中的Schame信息，通过jdbc的方式在doris中建表，则无需手动在doris中提前建表，

```scala
package com.hcdsj

import java.sql.DriverManager
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession


object SparkDorisTest {


  def main(args: Array[String]): Unit = {
    val USER_DATABASES = "ds"
    val USER_TABLE = "test1"

    val sc = new SparkConf().setAppName("KafkaToHudi").setMaster("local[4]").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    val spark = SparkSession.builder().config(sc).getOrCreate()

    Class.forName("com.mysql.jdbc.Driver")
    val connection = DriverManager.getConnection("jdbc:mysql://172.31.41.20:9030/ds", "root", "")

    val frame = spark.readStream.format("hudi").load("/tmp/hudi/my_hudi_table")

    /**
     * 1.提取数据
     */
    val (fieldName,fieldType,pl) = frame.schema.fields.foldLeft("", "", "")(
      (z, f) => {
        if (z._1.nonEmpty && z._2.nonEmpty && z._3.nonEmpty) {
          //非空即表示不是第一次的时候进行拼接
          (z._1 + "," + f.name, z._2 + "," + f.name + " " + f.dataType.simpleString, z._3 + ",?")
        } else {
          (f.name, f.name + " " + f.dataType.simpleString, "?")
        }
      }
    )

    /**
     * 2.将spark的表达式转换为doris的表达式
     */
    val chCol: String = dfTypeName2CH(fieldType)

    /**
     * 3.建库
     */
    val createDatabaseSql =
      s"create database if not exists ${USER_DATABASES}";

    /**
     * 4.在doris中建表
     */
    val chTableSql =
      s"""
         |create table if not exists ${USER_DATABASES}.${USER_TABLE}(${chCol})
         |ENGINE=olap
         |DISTRIBUTED BY HASH(id) BUCKETS 10
         |PROPERTIES("replication_num" = "1")
         |""".stripMargin

    val i = connection.createStatement().executeUpdate(createDatabaseSql)
    val y = connection.createStatement().executeUpdate(chTableSql)
    if(y != -1){
      frame.writeStream.format("doris").option("doris.table.identifier", "${USER_DATABASES}.${USER_TABLE}").option("doris.fenodes", "172.31.41.20:8030").option("user", "root").option("password","").option("checkpointLocation", "/tmp/zhongtai/sq/t1_check").option("mode","Append").start().awaitTermination()

    }
    spark.stop()

  }

  /**
   * df
   * uid string gender string,...
   * ch
   * uid String gender String,...
   */
  def dfTypeName2CH(dfCol:String) = {
    dfCol.split(",").map(line => {
      val fields: Array[String] = line.split(" ")
      val fType: String = dfType2CHType(fields(1))
      val fName: String = fields(0)
      fName + " " + fType
    }).mkString(",")
  }

  /**
   * 将df的type转为ch的type
   */
  def dfType2CHType(fieldType: String):String = {
    fieldType.toLowerCase() match {
      case "string" => "VARCHAR(255)"
      case "integer" => "INT"
      case "long" => "BIGINT"
      case "float" => "FLOAT"
      case "double" => "DOUBLE"
      case "date" => "VARCHAR(255)"
      case "timestamp" => "VARCHAR(255)"
      case _ => "VARCHAR(255)"
    }
  }

}
```

### 4）、离线写入数据到Doris（flink）

##### 1、通过flink读取mysql数据写入doris

**使用Flink Doris Connector编译的doris-flink-1.0.0-flink-1.13.1_2.12.jar**

1.1、将编译的doris-flink-1.0.0-flink-1.13.1_2.12.jar复制到 `flink 的 `ClassPath` 中。

1.2、在doris里创建表

1.3、代码

```scala
package com.hcdsj

import org.apache.flink.table.api.EnvironmentSettings
import org.apache.flink.table.api.TableEnvironment

object DorisSinkMysql {
  def main(args: Array[String]): Unit = {
    val settings = EnvironmentSettings.newInstance().build()
    val tenv = TableEnvironment.create(settings)
    val sql1 =
      """
        |CREATE TABLE tb1  (
        |  id INT NOT NULL,
        |  rule_name STRING,
        |  free_time_begin timestamp(3),
        |  free_time_end timestamp(3),
        |  vehicle_types STRING,
        |  processing_mode STRING,
        |  rule_status INT,
        |  create_time timestamp(3),
        |  update_time timestamp(3),
        |  timee timestamp(3),
        |  primary key(id) not enforced
        |) WITH (
        | 'connector' = 'mysql-cdc',
        | 'hostname' = '172.16.2.120',
        | 'port' = '3306',
        | 'username' = 'root',
        | 'password' = '123456',
        | 'database-name' = 'db_issue_clear',
        | 'table-name' = 'tb_nclear_dispute_setting'
        |)
        |""".stripMargin
      tenv.executeSql(sql1)

    val sql2 =
      """
        |CREATE TABLE flinks (
        |  id BIGINT ,
        |  rule_name STRING,
        |  free_time_begin timestamp(3),
        |  free_time_end timestamp(3),
        |  vehicle_types STRING,
        |  processing_mode STRING,
        |  rule_status INT,
        |  create_time timestamp(3),
        |  update_time timestamp(3),
        |  timee timestamp(3)
        |) WITH (
        | 'connector' = 'doris',
        | 'fenodes' = '172.16.2.123:8030',
        | 'table.identifier' = 'ds.mqflink1',
        | 'sink.batch.size' = '2',
        | 'sink.batch.interval'='1',
        | 'username' = 'root',
        | 'password'=''
        |)
        |""".stripMargin
    tenv.executeSql(sql2)

    tenv.executeSql("INSERT INTO flinks SELECT id,rule_name,free_time_begin,free_time_end,vehicle_types,processing_mode,rule_status,create_time,update_time,timee FROM tb1")

  }
}
```

1.4、打包

1.5、启动flink运行jar 写入数据

```shell
 ./bin/flink run -p 2 -c com.hcdsj.DorisSinkMysql -d /home/cxy/FlinkMysqlDoris-1.0-SNAPSHOT.jar
```

1.6、遇到的问题

```shell
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unable to instantiate java compiler
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
        at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.lang.IllegalStateException: Unable to instantiate java compiler
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.compile(JaninoRelMetadataProvider.java:428)
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.load3(JaninoRelMetadataProvider.java:374)

```

解决方法：

修改$FLINK_HOME/conf/flink-conf.yaml中

classloader.resolve-order: parent-first

参考链接：https://blog.csdn.net/appleyuchi/article/details/111597050

### 5）、实时写入数据到Doris（flink）

**使用Flink Doris Connector编译的doris-flink-1.0.0-flink-1.13.1_2.12.jar**

1.1、代码

```scala
package com.hcdsj

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment

object FlinkMysqlD {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val tenv = StreamTableEnvironment.create(env);
    val sql1 =
      """
        |CREATE TABLE tb1  (
        |  id INT NOT NULL,
        |  rule_name STRING,
        |  free_time_begin timestamp(3),
        |  free_time_end timestamp(3),
        |  vehicle_types STRING,
        |  processing_mode STRING,
        |  rule_status INT,
        |  create_time timestamp(3),
        |  update_time timestamp(3),
        |  timee timestamp(3),
        |  primary key(id) not enforced
        |) WITH (
        | 'connector' = 'mysql-cdc',
        | 'hostname' = '172.16.2.120',
        | 'port' = '3306',
        | 'username' = 'root',
        | 'password' = '123456',
        | 'database-name' = 'db_issue_clear',
        | 'table-name' = 'tb_nclear_dispute_setting'
        |)
        |""".stripMargin
    tenv.executeSql(sql1)

    val sql2 =
      """
        |CREATE TABLE flinks (
        |  id BIGINT ,
        |  rule_name STRING,
        |  free_time_begin timestamp(3),
        |  free_time_end timestamp(3),
        |  vehicle_types STRING,
        |  processing_mode STRING,
        |  rule_status INT,
        |  create_time timestamp(3),
        |  update_time timestamp(3),
        |  timee timestamp(3)
        |) WITH (
        | 'connector' = 'doris',
        | 'fenodes' = '172.16.2.123:8030',
        | 'table.identifier' = 'ds.mqflink1',
        | 'sink.batch.size' = '2',
        | 'sink.batch.interval'='1',
        | 'username' = 'root',
        | 'password'=''
        |)
        |""".stripMargin
    tenv.executeSql(sql2)
    tenv.executeSql("INSERT INTO flinks SELECT id,rule_name,free_time_begin,free_time_end,vehicle_types,processing_mode,rule_status,create_time,update_time,timee FROM tb1")
  }
}
```

启动flink 运行jar

```shell
./bin/flink run -c com.hcdsj.FlinkMysqlD -d /home/cxy/FlinkMysqlDoris-1.0-SNAPSHOT.jar
```
