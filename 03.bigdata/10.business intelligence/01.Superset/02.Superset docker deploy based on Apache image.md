## Docker化部署

前置条件必须确认已经Anaconda环境。然后执行如下几步：

（1）获取密钥

```shell
openssl rand -base64 42
```

（2）启动Superset容器：

```shell
docker run -d -p 8787:8088 -e "SUPERSET_SECRET_KEY=<SECRET_KEY>" --name superset apache/superset
```

​	<SECRET_KEY>处填写第一步中获取的密钥

（3）初始化用户名和密码：

```shell
docker exec -it superset superset fab create-admin \
              --username admin \
              --firstname Superset \
              --lastname Admin \
              --email admin@superset.com \
              --password admin
```

（4）初始化数据库：

```shell
docker exec -it superset superset db upgrade
```

（5）初始化权限：

```shell
docker exec -it superset superset init
```

然后访问：http://[ip]:8787 即可。如果需要DIY则继续进行后续的步骤，否则安装完毕。

（6）更改图标：
替换容器 /app/superset/static/assets/images中的这些文件:
`favicon.png`  `superset.png` `superset-logo-horiz.png`

```shell
# 1. 主机上准备好图标文件
# 2. 进入容器
docker exec --user root -it superset /bin/bash
# 3. 备份原文件后,把图标都拷贝进容器
docker cp ~/favicon.png <容器ID>:/app/superset/static/assets/images
docker cp ~/superset.png <容器ID>:/app/superset/static/assets/images
docker cp ~/superset-logo-horiz.png <容器ID>:/app/superset/static/assets/images
```
(7) 汉化
```shell
docker exec -it -u root <容器ID> /bin/bash
sed -i "s/BABEL_DEFAULT_LOCALE = \"en/BABEL_DEFAULT_LOCALE = \"zh/g" superset/config.py
sed -i "s/LANGUAGES = {}/# LANGUAGES = {}/g" superset/config.py
apt-get update
apt-get install python3-babel
cd superset
pybabel compile -d translations
docker restart ff63a001a3d3 
```
> 汉化中其他语言文件异常可忽略。

(8) 导出定制化后的镜像备用

TODO

## FAQ

（1）Superset如何连接Hive？

​		点击【Data】->【DataBases】-> 【+DataBases】，选择【SUPPORTED DATABASES】为Apache Hive。填写Hiveserver2的地址信息即可，形如：hive://<ip>:<port>/<db>

（2）Superset如何连接Presto？

​		点击【Data】->【DataBases】-> 【+DataBases】，选择【SUPPORTED DATABASES】为Presto。填写presto coordinator descovery uri的地址信息即可，形如：presto://<ip>:<port>/<scatalog>

（3）Superset如何连接Spark SQL？

​		连接Spark SQL需要优先启动Spark thriftserver服务，启动方式如下：

```shell
$SPARK_HOME/sbin/start-thriftserver.sh \
--master yarn \
--deploy-mode client \
--executor-memory 1g \
--driver-memory 2g \
--num-executors 2 \
--executor-cores 2 \
--jars hdfs://ns/test_jars/hudi-spark3.1-bundle_2.12-0.11.0.jar \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
--conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension \
--conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=CORRECTED \
--conf spark.sql.legacy.avro.datetimeRebaseModeInWrite=CORRECTED \
--conf spark.sql.hive.convertMetastoreParquet=false
```

​		点击【Data】->【DataBases】-> 【+DataBases】，选择【SUPPORTED DATABASES】为Apache Spark SQL。填写hiveserver2i的地址信息即可，形如：hive://<ip>:<port>/<db>
